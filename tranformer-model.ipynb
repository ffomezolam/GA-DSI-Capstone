{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e776957-a9e8-4efb-8f76-2958ff1fd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of the below is taken pretty verbatim from the gpt2 example notebook:\n",
    "# https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80abc1e6-89cc-4232-a4a6-ef638177b316",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 10:19:29.582853: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-06 10:19:29.924642: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-06 10:19:29.924664: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-06 10:19:29.966672: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-06 10:19:30.841892: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-06 10:19:30.841955: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-06 10:19:30.841961: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "from transformers import create_optimizer, AdamWeightDecay\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset, DatasetDict\n",
    "import re\n",
    "from utilities import extract_sentences, extract_words\n",
    "from itertools import chain\n",
    "import math\n",
    "import random\n",
    "\n",
    "model_type = 'distilgpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94df510-c6bf-4ed9-aff7-2c22639ead19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From fairest creatures we desire increase,\\n  That thereby beauty’s rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou, contracted to thine own bright eyes,\\n  Feed’st thy light’s flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thyself thy foe, to thy sweet self too cruel:\\n  Thou that art now the world’s fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak’st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world’s due, by the grave and thee.\\n\\nWhen forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty’s field,\\n  Thy youth’s proud livery so gazed on now,\\n  Will be a tatter’d weed of small worth held:\\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say, within thine own deep sunken eyes,\\n  Were an all-eating shame, and thriftless'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get sample text (shakespeare's sonnets)\n",
    "with open('./shakespeare-sonnets.stripped.split.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80a61884-7769-4297-8c1e-99562b37eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because I think the model doesn't like lots of odd apostraphes, I'm\n",
    "# subbing the obvious shakespearean contractions with an 'e'\n",
    "RE_CONTRACT = re.compile(r'(r|d|k|l)’(d|st)')\n",
    "RE_QUOTE = re.compile(r'(“|”|‘|’)')\n",
    "\n",
    "text = RE_CONTRACT.sub(r'\\1e\\2', text)\n",
    "text = RE_QUOTE.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb773b99-8325-43e8-9a4f-7fb20329fe08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From fairest creatures we desire increase,\\n  That thereby beautys rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou, contracted to thine own bright eyes,\\n  Feedest thy lights flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thyself thy foe, to thy sweet self too cruel:\\n  Thou that art now the worlds fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl makest waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the worlds due, by the grave and thee.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split into separate poems (separated by full blank line)\n",
    "RE_BLANKLINE = re.compile(r'\\n\\s*?\\n')\n",
    "\n",
    "poems = RE_BLANKLINE.split(text)\n",
    "poems[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a46c78-a1bc-4d4f-9705-66d6756a9452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poems) # 154 sonnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fcd1a31-f659-4477-95c4-3eb225f391b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Two loves I have of comfort and despair,\\n  Which like two spirits do suggest me still:\\n  The better angel is a man right fair,\\n  The worser spirit a woman coloured ill.\\n  To win me soon to hell, my female evil,\\n  Tempteth my better angel from my side,\\n  And would corrupt my saint to be a devil,\\n  Wooing his purity with her foul pride.\\n  And whether that my angel be turnd fiend,\\n  Suspect I may, yet not directly tell;\\n  But being both from me, both to each friend,\\n  I guess one angel in anothers hell:\\n    Yet this shall I neer know, but live in doubt,\\n    Till my bad angel fire my good one out.',\n",
       " 'Those lips that Loves own hand did make,\\n  Breathed forth the sound that said ‘I hate,\\n  To me that languishd for her sake:\\n  But when she saw my woeful state,\\n  Straight in her heart did mercy come,\\n  Chiding that tongue that ever sweet\\n  Was usd in giving gentle doom;\\n  And taught it thus anew to greet;\\n  ‘I hate she altered with an end,\\n  That followed it as gentle day,\\n  Doth follow night, who like a fiend\\n  From heaven to hell is flown away.\\n    ‘I hate, from hate away she threw,\\n    And savd my life, saying ‘not you.')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use last 10 as test set\n",
    "train = poems[:-10]\n",
    "test = poems[-10:]\n",
    "train[-1], test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69bc6d46-e09d-4927-ae28-d28e8bdaa767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'From fairest creatures we desire increase,\\n  That thereby beautys rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou, contracted to thine own bright eyes,\\n  Feedest thy lights flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thyself thy foe, to thy sweet self too cruel:\\n  Thou that art now the worlds fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl makest waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the worlds due, by the grave and thee.'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_src = Dataset.from_list([{'text': p} for p in train])\n",
    "test_src = Dataset.from_list([{'text': p} for p in test])\n",
    "#datasets = DatasetDict({\"train\": train_src, \"validation\": test_src})\n",
    "datasets = DatasetDict({\"train\": Dataset.from_list([{'text': p} for p in poems])})\n",
    "datasets['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3853c6fe-9d1c-43fb-b91b-7c019b812597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c459d4dbc8374f22a216044053f1ba13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/154 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "def tokenize(src):\n",
    "    return tokenizer(src['text'])\n",
    "\n",
    "#train_enc = tokenizer(train_src['text'])\n",
    "#test_enc = tokenizer(test_src['text'])\n",
    "#train_enc.keys()\n",
    "tokened_data = datasets.map(tokenize, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f538e0b6-4460-4ba0-94a9-00d3001aa69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 154\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9dd09ef-f8cb-48ab-a4be-1033001c3388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# size of a block of encodings\n",
    "block_size = 128\n",
    "\n",
    "# function to group encoded texts into a single long encoding per set\n",
    "def group_texts(encoded_texts):\n",
    "    #TODO: I think I need to rewrite this. It isn't working\n",
    "    concat_texts = {k: sum(encoded_texts[k],[]) for k in encoded_texts.keys()}\n",
    "    total_len = len(concat_texts[list(encoded_texts.keys())[0]])\n",
    "    total_len = (total_len // block_size) * block_size\n",
    "    \n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_len, block_size)]\n",
    "        for k, t in concat_texts.items()\n",
    "    }\n",
    "    result['labels'] = result['input_ids'].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba9c7f96-8ea6-4c41-83a2-6c6a64d02594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b69f8970264433b463c5d1108d515b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'  And tender churl makest waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the worlds due, by the grave and thee.When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beautys field,\\n  Thy youths proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:\\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say, within thine own deep sunken'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combo_data = tokened_data.map(group_texts, batched=True)\n",
    "tokenizer.decode(combo_data['train'][1]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba474106-16ae-4e34-819a-11d26fa6a769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 10:19:34.348851: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-06 10:19:34.349028: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-06 10:19:34.349049: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (archzolam): /proc/driver/nvidia/version does not exist\n",
      "2022-10-06 10:19:34.349494: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForCausalLM.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a383d959-b779-4674-b525-e5f8776c6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05f42041-00b1-47fc-97a7-5f5fee704365",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, jit_compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a483f1b9-8ca0-4d52-a487-377bcead6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = model.prepare_tf_dataset(combo_data['train'],\n",
    "                                     shuffle=True,\n",
    "                                     batch_size=32)\n",
    "#validation_set = model.prepare_tf_dataset(combo_data['validation'],\n",
    "#                                          shuffle=True,\n",
    "#                                          batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a8d6f3-6d9f-45ea-b164-a26f58d7fa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-06 10:23:57.560105: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator tfgpt2lm_head_model/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_set, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e387fe21-e7b6-4607-862e-8ad52f3159b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1858  318  257  845 1402 5434   11  475  340  318  845 1593  284  760\n",
      "   326  262 5434  318  407 5969   13  198  198  198  198  198  198  198\n",
      "   198  198  198  198  198  198  198  198  198  198  198  198  198  198\n",
      "   198  198  198  198  198  198  198  198]], shape=(1, 50), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "test = \"There is a very small bug\"\n",
    "\n",
    "tokenized = tokenizer(test, return_tensors=\"np\")\n",
    "outputs = model.generate(**tokenized, max_length=50)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9b52ab8-4be1-46b6-920b-9ce7a1e88f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There is a very small bug, but it is very important to know that the bug is not fixed.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(outputs[0]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd83e10a-74ee-4ff6-8470-32192bfaf53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: My tongue-tied Muse in manners holds her still,\n",
      "out: My tongue-tied Muse in manners holds her still, and she is not a man, but a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and a woman, and\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: While comments of your praise richly compild,\n",
      "out: While comments of your praise richly compild, and I will not be able to do so.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: Reserve their character with golden quill,\n",
      "out: Reserve their character with golden quill, and they will be the same.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: And precious phrase by all the Muses fild.\n",
      "out: And precious phrase by all the Muses fild.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: I think good thoughts, whilst others write good words,\n",
      "out: I think good thoughts, whilst others write good words, and I think that I am not a good man, but I am a good man, and I am a good man, and I am a good man, and I am a good man, and I am a good man, and I am a good man, and I am a good man, and I am a good man, and I am a good man, and I am a good man, and I am a good man, and\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: And like unlettered clerk still cry ‘Amen\n",
      "out: And like unlettered clerk still cry ‘Amen!’\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: To every hymn that able spirit affords,\n",
      "out: To every hymn that able spirit affords, and that the spirit of the world, which is the spirit of the world, which is the spirit of the world, which is the spirit of the world, which is the spirit of the world, which is the spirit of the world, which is the spirit of the world, which is the spirit of the world, which is the spirit of the world, which is the spirit of the world, which is the spirit of the world, which is\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: In polishd form of well-refined pen.\n",
      "out: In polishd form of well-refined pen.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: Hearing you praised, I say ‘tis so, tis true,\n",
      "out: Hearing you praised, I say ‘tis so, tis true,’\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: And to the most of praise add something more;\n",
      "out: And to the most of praise add something more;\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: But that is in my thought, whose love to you,\n",
      "out: But that is in my thought, whose love to you, and my love to you, and my love to you, and my love to you, and my love to you, and my love to you, and my love to you, and my love to you, and my love to you, and my love to you, and my love to you, and my love to you, and my love to you, and my love to you, and my love to you, and my love to\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: Though words come hindmost, holds his rank before.\n",
      "out: Though words come hindmost, holds his rank before.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in: Then others, for the breath of words respect,\n",
      "out: Then others, for the breath of words respect, and for the love of the Lord, and for the love of the Lord, and for the love of the Lord, and for the love of the Lord, and for the love of the Lord, and for the love of the Lord, and for the love of the Lord, and for the love of the Lord, and for the love of the Lord, and for the love of the Lord, and for the love of the Lord, and for\n",
      "\n",
      "in: Me for my dumb thoughts, speaking in effect.\n",
      "out: Me for my dumb thoughts, speaking in effect.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test on lines of poem\n",
    "num = random.randrange(len(poems))\n",
    "poem = poems[num]\n",
    "lines = [line.strip() for line in poem.split('\\n')]\n",
    "for line in lines:\n",
    "    t = tokenizer(line, return_tensors=\"np\")\n",
    "    out = model.generate(**t, max_length=100)\n",
    "    print(f'in: {line}\\nout: {tokenizer.decode(out[0]).strip()}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c138150-ddd8-402b-89c1-a0a6138387ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
