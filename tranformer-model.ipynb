{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e247aac4-af16-4b6e-be7c-8a32403263d8",
   "metadata": {},
   "source": [
    "Test notebook to get the hang of using OpenAI's Transformers package, including GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e776957-a9e8-4efb-8f76-2958ff1fd574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A lot of the below is adapted from the gpt2 tutorial:\n",
    "# https://huggingface.co/docs/transformers/v4.22.2/en/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ebac-93be-488f-b5b8-3fc324c36fe7",
   "metadata": {},
   "source": [
    "## Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3c7b112-6339-422c-8b13-6beae6a1ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting for model\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# lm collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# model and support\n",
    "from transformers import TFAutoModelForCausalLM, create_optimizer, AdamWeightDecay\n",
    "\n",
    "# other utilities\n",
    "from itertools import chain\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40b0184-61c8-4b8c-9be3-bca0f00cac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model we are using\n",
    "MODELS = [\n",
    "    'gpt',\n",
    "    'gpt2',\n",
    "    'ctrl',\n",
    "    'transformerxl',\n",
    "    'reformer',\n",
    "    'xlnet'\n",
    "]\n",
    "    \n",
    "model_type = 'gpt2-medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3af3b72-26ba-46b1-9e03-40a695527906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "MODEL_DIR = './models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959970d-dfcd-4339-bdac-9877ac3f3607",
   "metadata": {},
   "source": [
    "## Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1c7d8c-8aa3-4227-ab44-40d079392282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of lines: 2155\n",
      "First 5: ['From fairest creatures we desire increase,', 'That thereby beautys rose might never die,', 'But as the riper should by time decease,', 'His tender heir might bear his memory:', 'But thou, contracted to thine own bright eyes,']\n",
      "# of lines: 617\n",
      "First 5: ['\\ufeff', 'I thought once how Theocritus had sung', 'Of the sweet years, the dear and wished-for years,', 'Who each one in a gracious hand appears', 'To bear a gift for mortals, old or young:']\n",
      "# of lines: 2046\n",
      "First 5: ['\\ufeff    Wonder of these, glory of other times,', 'O thou whom envy evn is forced tadmire!', 'Great Patroness of these my humble rhymes,', 'Which thou from out thy greatness dost inspire!', 'Since only thou has deigned to raise them higher,']\n",
      "# of lines: 2524\n",
      "First 5: ['\\ufeff', 'Into these loves who but for passion looks,', 'At this first sight here let him lay them by,', 'And seek elsewhere in turning other books,', 'Which better may his labour satisfy.']\n",
      "# of lines: 2579\n",
      "First 5: ['\\ufeff', 'Dear are the forceful energies of Song,', 'For they do swell the spring-tide of the heart', 'With rosier currents, and impel along', 'The life-blood freely:--O! they can impart']\n",
      "TOTAL LINES: 9921\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "paths = [\n",
    "    './shakespeare-sonnets.clean.txt',\n",
    "    './browning-sonnets.clean.txt',\n",
    "    './daniel-constable-sonnets.clean.txt',\n",
    "    './drayton-griffin-smith-sonnet-cycles.clean.txt',\n",
    "    './seward-sonnets-and-odes.stripped.split.txt'\n",
    "]\n",
    "data = []\n",
    "for path in paths:\n",
    "    with open(path, 'r') as f:\n",
    "        data.append([line.strip() for line in f.readlines() if line.strip()])\n",
    "        print(f'# of lines: {len(data[-1])}\\nFirst 5: {data[-1][:5]}')\n",
    "\n",
    "lines = list(chain(*data))\n",
    "print(f'TOTAL LINES: {len(lines)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74281949-b8a1-4eca-9c5b-7be3e8c9669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# various functions to help with preprocessing\n",
    "\n",
    "# combine 2 lines into 1\n",
    "def double_lines(lines):\n",
    "    return ['\\n'.join(double) for double in zip(lines[0::2], lines[1::2])]\n",
    "\n",
    "# return all lines concatenated\n",
    "def full_text(lines):\n",
    "    return '\\n'.join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01b72ce-07e3-4575-8b73-750b339c6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9424, 497)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test\n",
    "lines_train, lines_test = train_test_split(lines, test_size=0.05)\n",
    "len(lines_train), len(lines_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437b48-0310-4f4e-ba9f-fc222c3f84e4",
   "metadata": {},
   "source": [
    "## Cleaning and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bc6d46-e09d-4927-ae28-d28e8bdaa767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 9424\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 497\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset and DatasetDict instances - I think this is needed for model\n",
    "train_dataset = Dataset.from_dict({'text': lines_train})\n",
    "test_dataset = Dataset.from_dict({'text': lines_test})\n",
    "datasets = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3853c6fe-9d1c-43fb-b91b-7c019b812597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c54bbcd4-81ee-42c6-83d3-428829fa00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for tokenizer to use with map() method of datasetdict\n",
    "def token_preproc(data):\n",
    "    return tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f538e0b6-4460-4ba0-94a9-00d3001aa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efbdbd86f0c486f8a5654c835c5244e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf9f65cc81444acad35e2abff4e7482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d542c336c034f639934c63f475b6acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5062674e8404ddfa257546a2b38d994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f715b9287d4a48529219ea8dc1364293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d6f87df33247ed8e61606057a51d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e326acddc514135a6e91450f50d6a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c2de890f8b47449f8d54bab6875d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 9424\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 497\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokened_data = datasets.map(token_preproc, batched=True, num_proc=4, remove_columns=['text'])\n",
    "tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a746d71-3b09-40b8-95ae-b5f3a71d44d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='gpt2-medium', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='tf')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad encodings and prep for modeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors='tf')\n",
    "collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10ee63-a50a-42da-a49f-59f015e2814e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f45202c2-393d-4e35-9ffd-fea2adf70329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2-medium.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "model = TFAutoModelForCausalLM.from_pretrained(model_type, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec648afe-a7b2-464e-b805-ca0ffce0e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(16, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(16, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(16, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to special format for tf model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tf_train_set = model.prepare_tf_dataset(tokened_data['train'], shuffle=True, batch_size=16, collate_fn=collator)\n",
    "tf_test_set = model.prepare_tf_dataset(tokened_data['test'], shuffle=False, batch_size=16, collate_fn=collator)\n",
    "tf_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6abd4074-b0d6-4058-b017-67d196514c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6d0663a-d3bc-4c8d-b713-bb892ec5dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "589/589 [==============================] - 1860s 3s/step - loss: 5.0656 - val_loss: 4.8011\n",
      "Epoch 2/4\n",
      "589/589 [==============================] - 1774s 3s/step - loss: 4.6181 - val_loss: 4.7670\n",
      "Epoch 3/4\n",
      "589/589 [==============================] - 1720s 3s/step - loss: 4.3258 - val_loss: 4.7816\n",
      "Epoch 4/4\n",
      "589/589 [==============================] - 1731s 3s/step - loss: 4.0471 - val_loss: 4.8507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1699c5cc0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(tf_train_set, validation_data=tf_test_set, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2b5ee69e-b6b2-4f6d-822d-e42a0a75eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if not os.path.exists(MODEL_DIR + model_type):\n",
    "    os.makedirs(MODEL_DIR + model_type)\n",
    "\n",
    "model.save_pretrained(MODEL_DIR + model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9205b01-beaf-4603-9836-01cfcefe172e",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7018e21-169f-4268-bf96-20b11eae6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get predicted text\n",
    "def test(text, max_length=50, temp=1, top_k=100):\n",
    "    tokened = tokenizer(text, return_tensors='np')\n",
    "    output = model.generate(**tokened, max_length=max_length, temperature=temp, top_k=top_k, repetition_penalty=1.5)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc98e32b-111c-4dc0-843c-252cab0de139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Tomorrow I will\n",
      "Output: Tomorrow I will not sleep, but must die.  And yet my love is so strong;--and still it is! O well then: thou art a saint!—O beauteous one!--I sing,—Ah me! how oft\n",
      "\n",
      "Original: Yesterday we were\n",
      "Output: Yesterday we were both young, and yet so old.  What a difference! I am thine; you are my love:--and then the story begins!--I was born in Fidessa's landscape Gardens Conservatory Gymnasium Building\n",
      "\n",
      "Original: For naught I may\n",
      "Output: For naught I may love thee, but that thou mightst know.  For this reason my heart is in strife; and yet I am not free:--and thus it lies. O! how much more doth she please me than you\n",
      "\n",
      "Original: My love has been\n",
      "Output: My love has been a fire, and my flame hath not died.  And now I know it is cold; yea colder than snow!  Then did I feel the chill of night: then was mine eye dry—now doth seem white.—\n",
      "\n",
      "Original: I am\n",
      "Output: I am not a man, but an angel.  And yet I love thee with such zeal;--and so do you: and thus we both!—so it is said,—but thou art the true one!--O Love me, my Bel\n",
      "\n",
      "Original: Thou art\n",
      "Output: Thou art the true and only god, who dost know all.  For thou hast no name but that, which is his;--god!—and I am not: so be it known to thee alone!--O love me with a\n",
      "\n",
      "Original: The little love-god lying once asleep\n",
      "Output: The little love-god lying once asleep, doth wake;--and the sun shines bright.  The world is not so dark, nor yet as it used be: nay! fairer days are come!--more fair than now?—\n",
      "\n",
      "Original: In loving thee thou\n",
      "Output: In loving thee thou hast no right to complain.  For I am not a woman, nor born in the time;--nor can be so!—but you are. You have my heart and soul: mine alone is your name? Then tell\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lines = [\n",
    "    'Tomorrow I will',\n",
    "    'Yesterday we were',\n",
    "    'For naught I may',\n",
    "    'My love has been',\n",
    "    'I am',\n",
    "    'Thou art',\n",
    "    'The little love-god lying once asleep',\n",
    "    'In loving thee thou'\n",
    "]\n",
    "\n",
    "for line in test_lines:\n",
    "    print(f'Original: {line}\\nOutput: {test(line)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b278a-3993-4ec1-a3fb-6e2fc33e768b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
