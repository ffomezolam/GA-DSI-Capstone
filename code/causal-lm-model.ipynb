{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e247aac4-af16-4b6e-be7c-8a32403263d8",
   "metadata": {},
   "source": [
    "# GA Capstone\n",
    "## Causal Language Transformer Modeling with GPT2\n",
    "\n",
    "Creating a model to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2444a-4ed5-4266-b23f-6e4cafcfd25d",
   "metadata": {},
   "source": [
    "A lot of the below is adapted from the gpt2 tutorial at https://huggingface.co/docs/transformers/v4.22.2/en/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ebac-93be-488f-b5b8-3fc324c36fe7",
   "metadata": {},
   "source": [
    "### Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c7b112-6339-422c-8b13-6beae6a1ebc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 21:45:43.317722: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-15 21:45:43.429322: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-10-15 21:45:43.429341: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-15 21:45:43.454626: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-15 21:45:44.082009: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-15 21:45:44.082064: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-15 21:45:44.082071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# data formatting for model\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# lm collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# model and support\n",
    "from transformers import TFAutoModelForCausalLM, create_optimizer, AdamWeightDecay\n",
    "\n",
    "# support\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "\n",
    "# custom utilities\n",
    "from utilities.utilities import load_config, get_dataset_from_config\n",
    "from utilities.utilities import split_text_and_labels\n",
    "from utilities.utilities import generate_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a405336-299b-433e-8bc3-670f9e5652ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RANDOM_SEED': 1,\n",
       " 'MODEL_DIR': '../models/',\n",
       " 'DATA_DIR': '../data/',\n",
       " 'CAUSAL_N_EPOCHS': 8,\n",
       " 'CLASS_N_EPOCHS': 2,\n",
       " 'BATCH_SIZE': 16,\n",
       " 'CAUSAL_MODEL': 'distilgpt2',\n",
       " 'CLASS_MODEL': 'distilbert-base-uncased',\n",
       " 'MODEL_NAME': 'shakespeare',\n",
       " 'DATA_SHAKESPEARE': ['shakespeare-sonnets.clean.txt', 'shakespeareplays.txt'],\n",
       " 'DATA_OTHER': ['belloc_hilaire-sonnets_and_verse.clean.txt',\n",
       "  'blake_william-poems.clean.txt',\n",
       "  'browning_elizabeth-sonnets_from_the_portuguese.clean.txt',\n",
       "  'daniel_samuel_and_constable_henry-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'donne_john-poetry_vol_1.clean.txt',\n",
       "  'drayton_michael_et_al-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'farjeon_eleanor-sonnets_and_poems.clean.txt',\n",
       "  'keats_john-poems_1820.clean.txt',\n",
       "  'lodge_thomas_and_fletcher_giles-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'lovell_robert_and_southey_robert-poems.clean.txt',\n",
       "  'milton_john-poetical_works.clean.txt',\n",
       "  'seward_anna-sonnets-and-odes.clean.txt',\n",
       "  'shelley_percy-complete_poetic_works.clean.txt',\n",
       "  'wilde_oscar-poems.clean.txt',\n",
       "  'wilde_oscar-selected_prose.txt',\n",
       "  'gpt2-text.txt'],\n",
       " 'N_SAMPLES': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_FILE = 'config.json'\n",
    "\n",
    "config_vars = load_config(CONFIG_FILE)\n",
    "config_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40b0184-61c8-4b8c-9be3-bca0f00cac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = config_vars['RANDOM_SEED'] if 'RANDOM_SEED' in config_vars else 1\n",
    "\n",
    "# pretrained model designator\n",
    "MODEL_TYPE = config_vars['CAUSAL_MODEL'] if 'CAUSAL_MODEL' in config_vars else 'distilgpt2'\n",
    "\n",
    "# model batch size\n",
    "BATCH_SIZE = config_vars['BATCH_SIZE'] if 'BATCH_SIZE' in config_vars else 16\n",
    "\n",
    "# model num epochs\n",
    "N_EPOCHS = config_vars['CAUSAL_N_EPOCHS'] if 'CAUSAL_N_EPOCHS' in config_vars else 8\n",
    "\n",
    "# whether to downsample\n",
    "SAMPLE = config_vars['N_SAMPLES'] if 'N_SAMPLES' in config_vars else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3af3b72-26ba-46b1-9e03-40a695527906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories and other constants, from config.json\n",
    "\n",
    "# model name for saving\n",
    "MODEL_NAME = config_vars['MODEL_NAME'] if 'MODEL_NAME' in config_vars else 'shakespeare'\n",
    "\n",
    "# directory for saved models\n",
    "MODEL_DIR = config_vars['MODEL_DIR'] if 'MODEL_DIR' in config_vars else '../models/'\n",
    "\n",
    "# full model save path\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, f'{MODEL_NAME}.{MODEL_TYPE}.{str(N_EPOCHS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959970d-dfcd-4339-bdac-9877ac3f3607",
   "metadata": {},
   "source": [
    "### Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f247c9d-a576-4d1a-83a7-508ce3252b18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76578,\n",
       " [('From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decease, His tender heir might bear his memory:',\n",
       "   1),\n",
       "  ('But thou, contracted to thine own bright eyes, Feed’st thy light’s flame with self-substantial fuel, Making a famine where abundance lies, Thyself thy foe, to thy sweet self too cruel:',\n",
       "   1)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data - just load the shakespeare stuff\n",
    "data = get_dataset_from_config(config_vars, limit=SAMPLE)[1]\n",
    "len(data), data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01b72ce-07e3-4575-8b73-750b339c6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75812, 766)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test - we don't need a test set here\n",
    "data_train, data_val = train_test_split(data, test_size=0.01, random_state=SEED)\n",
    "\n",
    "# this is the labeled dataset - split into text and label lists\n",
    "data_train = split_text_and_labels(data_train)\n",
    "data_val = split_text_and_labels(data_val)\n",
    "\n",
    "# we don't need labels for causal LM\n",
    "data_train = data_train['text']\n",
    "data_val = data_val['text']\n",
    "\n",
    "len(data_train), len(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437b48-0310-4f4e-ba9f-fc222c3f84e4",
   "metadata": {},
   "source": [
    "### Cleaning and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bc6d46-e09d-4927-ae28-d28e8bdaa767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 75812\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset and DatasetDict instances - I think this is needed for model\n",
    "train_dataset = Dataset.from_dict({'text': data_train})\n",
    "val_dataset = Dataset.from_dict({'text': data_val})\n",
    "datasets = DatasetDict({'train': train_dataset, 'val': val_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3853c6fe-9d1c-43fb-b91b-7c019b812597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c54bbcd4-81ee-42c6-83d3-428829fa00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for tokenizer to use with map() method of datasetdict\n",
    "def token_preproc(data):\n",
    "    return tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f538e0b6-4460-4ba0-94a9-00d3001aa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17fcccf3b4149f496eb251169b9f133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b3d363fb0e4d6fa7a46bbcf3050d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744e86211a824a0e8792dc5dc122fec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d93d41fbd1e64ab8bd72a3a52ccea404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17451d4ddb746c1982e4dbb78c7992b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c8660153a34fa48df998be632dd41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d33c8132f74c43ba3cb22fb03295bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75663c8bd452499f9cd35cf5a32a72b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 75812\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokened_data = datasets.map(token_preproc, batched=True, num_proc=4, remove_columns=['text'])\n",
    "tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a746d71-3b09-40b8-95ae-b5f3a71d44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad encodings and prep for modeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10ee63-a50a-42da-a49f-59f015e2814e",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f45202c2-393d-4e35-9ffd-fea2adf70329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 21:45:48.588365: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-15 21:45:48.588389: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-15 21:45:48.588408: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (archzolam): /proc/driver/nvidia/version does not exist\n",
      "2022-10-15 21:45:48.588625: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ../models/shakespeare.distilgpt2.8.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(MODEL_TYPE, pad_token_id = tokenizer.eos_token_id)\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "    \n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec648afe-a7b2-464e-b805-ca0ffce0e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(32, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(32, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(32, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to special format for tf model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tf_train_set = model.prepare_tf_dataset(tokened_data['train'], shuffle=True, batch_size=32, collate_fn=collator)\n",
    "tf_val_set = model.prepare_tf_dataset(tokened_data['val'], shuffle=False, batch_size=32, collate_fn=collator)\n",
    "tf_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6abd4074-b0d6-4058-b017-67d196514c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6d0663a-d3bc-4c8d-b713-bb892ec5dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model (if pretrained does not exist)\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    model.fit(tf_train_set, validation_data=tf_val_set, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b5ee69e-b6b2-4f6d-822d-e42a0a75eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    model.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9205b01-beaf-4603-9836-01cfcefe172e",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc98e32b-111c-4dc0-843c-252cab0de139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, Done!\n",
      "\n",
      "\n",
      "Original: Fire and\n",
      "Generated: Fire and brimstone are all gone. A hundred; two hundred at once or night. My face is cold in a fiery pit of blood, And I have slept with the dead men for some time to see ’em. The man that had been their enemy Is now taken from them both. They must die before my sight shall come forth To show how they have fell through this unseasoned storm. One man hath borne these crowns on his head But never was so well recovered From his body he\n",
      "\n",
      "\n",
      "Original: Let’s go\n",
      "Generated: Let’s go, then. Go in with him that doth hold this business true And say no more words of it Than I will make thee good to you as best I can. That my life must be an answer for all his cares and pain For yours itself. But yet another time shall give us satisfaction ere we have done ‘But thy great heart has a mighty service As full even as is mine; To save himself from the wrongs of oppression. A gentlewoman could not live by me till\n",
      "\n",
      "\n",
      "Original: His breeding, sir, hath been\n",
      "Generated: His breeding, sir, hath been well assured Of all the rest. I have no less patience to speak; in my opinion he speaks a very good speech most intelligently. And of your own person nothing can move A more modest ear when you are his subject-servant. Unless by heaven or through him be brought back again To make it known directly what thou art accused and done here. How many times do thy true name take some comfort from thee With that which was condemned for these same treasonous deeds? For so I should\n",
      "\n",
      "\n",
      "Original: But in my heart was Palamon, and there, Lord,\n",
      "Generated: But in my heart was Palamon, and there, Lord, I must say. No more; nor that your tongue is full of words That you’ll write it like an arrant gentleman— For though aught can utter ‘Tis thought to speak with malice And so your good will are no more Than makes this his truth the greater? To thee he should know himself! But these vile deeds do so little effect What may be read upon me as thou makest some offense By such a deed which stands unto thy face As hath made thee\n",
      "\n",
      "\n",
      "Original: Who hath\n",
      "Generated: Who hath sent you word that this gentleman of York is at hand? Who makes a peace with the French and all, Alluding directly to his good will in France? If not, I warrant him. He’s gone out into the streets; but never by man That would have brought me forth But for my name! To th  earth whence this traitor comes From Warwick dead himself? The crown he held till thus much lost on thee Unto him, what part thou dost take from thy\n",
      "\n",
      "\n",
      "Original: The younger brother, Cadwal, Once Arviragus, in as like a figure Strikes life into my speech\n",
      "Generated: The younger brother, Cadwal, Once Arviragus, in as like a figure Strikes life into my speech. I know him better than Achilles; To be thus bold and quick With a sudden and swift mind—that hath by his own power Made himself the dearer. His heart goes to earth more valiant for such high hopes Than with a fiery heart made up of rage, Even from his heart’s edge against death? As much can ever do but hold so fast In a weak and most stubborn body. And that he was bound, whose strength it shall stand against From our arms and we�\n",
      "\n",
      "\n",
      "Original: Signior Iachimo will not\n",
      "Generated: Signior Iachimo will not speak to her. But she must be contented more than you do know, Than by the love of my master is yours. The only thing they should say unto this: Dear mother, good Marina— How come we no less? What matters so? And what concerns me now in your behalf To leave from us and to call them husbands or wives as we can be? Wherefore shall any man keep true a friend with his wife’s affection Unto the state that he sends him?\n",
      "\n",
      "\n",
      "Original: But is there any else longs to see this\n",
      "Generated: But is there any else longs to see this? How soon will you come unto the door of heaven And put your head in his pocket window To give witness, and show him how good a man he was! Or shall you be pleased by some grace too. Then may I say: No more are these things well fitted than such As that we have seen here already? Let God’s blessings fall on thee; for so thou know‘st all about us Is one of thine own fair birth born— A thing which would never\n",
      "\n",
      "\n",
      "Original: Of this make no conclusion, lest you say Your\n",
      "Generated: Of this make no conclusion, lest you say Your master is a good soldier. There’s some very truth in having fought before him that might serve as his means to keep himself company and not to be turned To the mickle hand of hell When he hath been banished by God Himself! The King has sent me forth from prison And there with us unto our cell Where I must wait for Lord Parson tomorrow morning? But one thing further shall follow: Upon my father—s commandment thou shalt hear Of all these matters now performed thus\n",
      "\n",
      "\n",
      "Original: You owe money here besides, Sir John, for your diet and by-drinkings\n",
      "Generated: You owe money here besides, Sir John, for your diet and by-drinkings Have ta’en me up; but I promise you there shall be no more. That a good cause may come from the King in payment Of two hours of rest when we have seen him meet again with Alanson? To buy his company at Dover to save my life If he please. For what hath struck down this fellow is not much known But that both will lie upon him And do him justice without pay His ransom doth amount. Wherein they all live! My brother Thomas Will serve\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lines = [data_train[random.randint(0,len(data) - 1)] for _ in range(10)]\n",
    "fragment_ratio = 0.7\n",
    "\n",
    "generated = list()\n",
    "for line in test_lines:\n",
    "    words = line.split()\n",
    "    fragment_end_ix = int(len(words) * fragment_ratio) or 1\n",
    "    line = ' '.join(words[:fragment_end_ix])\n",
    "    generated.append((line, generate_from(line, model, tokenizer)))\n",
    "    print(len(generated), end=', ')\n",
    "    \n",
    "print('Done!\\n\\n')\n",
    "\n",
    "for line in generated:\n",
    "    print(f'Original: {line[0]}\\nGenerated: {line[1]}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2dc1d-4f27-47f5-9a2f-a3940d67014d",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "There is a perplexity metric which, in my understanding, provides a score for how confused the model was in generating next words. Due to dependency issues in using [Hugging Face's Evaluate library](https://huggingface.co/docs/evaluate/index) I have not used that here. I also feel that the perplexity metric is maybe too generalized for what I'm looking for. See the classification notebook for my alternative metric for determining accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf742070-67be-4913-8246-0eacf1f8bc47",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The fine-tuned GPT2 model generates text that, to me, seems reasonably Shakespearean. The real determination will be pitting this against other period works, which often sound similar. The classification notebook will explore a classification model that seeks to classify Shakespearean vs. Non-Shakespearean text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
