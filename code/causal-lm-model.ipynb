{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e247aac4-af16-4b6e-be7c-8a32403263d8",
   "metadata": {},
   "source": [
    "# GA Capstone\n",
    "## Causal Language Transformer Modeling with GPT2\n",
    "\n",
    "Creating a model to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2444a-4ed5-4266-b23f-6e4cafcfd25d",
   "metadata": {},
   "source": [
    "A lot of the below is adapted from the gpt2 tutorial at https://huggingface.co/docs/transformers/v4.22.2/en/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ebac-93be-488f-b5b8-3fc324c36fe7",
   "metadata": {},
   "source": [
    "### Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c7b112-6339-422c-8b13-6beae6a1ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting for model\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# lm collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# model and support\n",
    "from transformers import TFAutoModelForCausalLM, create_optimizer, AdamWeightDecay\n",
    "\n",
    "# support\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "\n",
    "# custom utilities\n",
    "from utilities.utilities import load_config, get_dataset_from_config\n",
    "from utilities.utilities import split_text_and_labels\n",
    "from utilities.utilities import generate_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a405336-299b-433e-8bc3-670f9e5652ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RANDOM_SEED': 1,\n",
       " 'MODEL_DIR': '../models/',\n",
       " 'DATA_DIR': '../data/',\n",
       " 'CAUSAL_N_EPOCHS': 8,\n",
       " 'CLASS_N_EPOCHS': 2,\n",
       " 'BATCH_SIZE': 16,\n",
       " 'CAUSAL_MODEL': 'distilgpt2',\n",
       " 'CLASS_MODEL': 'distilbert-base-uncased',\n",
       " 'MODEL_NAME': 'shakespeare',\n",
       " 'DATA_SHAKESPEARE': ['shakespeare-sonnets.clean.txt', 'shakespeareplays.txt'],\n",
       " 'DATA_OTHER': ['belloc_hilaire-sonnets_and_verse.clean.txt',\n",
       "  'blake_william-poems.clean.txt',\n",
       "  'browning_elizabeth-sonnets_from_the_portuguese.clean.txt',\n",
       "  'daniel_samuel_and_constable_henry-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'donne_john-poetry_vol_1.clean.txt',\n",
       "  'drayton_michael_et_al-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'farjeon_eleanor-sonnets_and_poems.clean.txt',\n",
       "  'keats_john-poems_1820.clean.txt',\n",
       "  'lodge_thomas_and_fletcher_giles-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'lovell_robert_and_southey_robert-poems.clean.txt',\n",
       "  'milton_john-poetical_works.clean.txt',\n",
       "  'seward_anna-sonnets-and-odes.clean.txt',\n",
       "  'shelley_percy-complete_poetic_works.clean.txt',\n",
       "  'wilde_oscar-poems.clean.txt',\n",
       "  'wilde_oscar-selected_prose.txt',\n",
       "  'gpt2-text.txt'],\n",
       " 'N_SAMPLES': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_FILE = 'config.json'\n",
    "\n",
    "config_vars = load_config(CONFIG_FILE)\n",
    "config_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40b0184-61c8-4b8c-9be3-bca0f00cac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = config_vars['RANDOM_SEED'] if 'RANDOM_SEED' in config_vars else 1\n",
    "\n",
    "# pretrained model designator\n",
    "MODEL_TYPE = config_vars['CAUSAL_MODEL'] if 'CAUSAL_MODEL' in config_vars else 'distilgpt2'\n",
    "\n",
    "# model batch size\n",
    "BATCH_SIZE = config_vars['BATCH_SIZE'] if 'BATCH_SIZE' in config_vars else 16\n",
    "\n",
    "# model num epochs\n",
    "N_EPOCHS = config_vars['CAUSAL_N_EPOCHS'] if 'CAUSAL_N_EPOCHS' in config_vars else 8\n",
    "\n",
    "# whether to downsample\n",
    "SAMPLE = config_vars['N_SAMPLES'] if 'N_SAMPLES' in config_vars else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3af3b72-26ba-46b1-9e03-40a695527906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories and other constants, from config.json\n",
    "\n",
    "# model name for saving\n",
    "MODEL_NAME = config_vars['MODEL_NAME'] if 'MODEL_NAME' in config_vars else 'shakespeare'\n",
    "\n",
    "# directory for saved models\n",
    "MODEL_DIR = config_vars['MODEL_DIR'] if 'MODEL_DIR' in config_vars else '../models/'\n",
    "\n",
    "# full model save path\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, f'{MODEL_NAME}.{MODEL_TYPE}.{str(N_EPOCHS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959970d-dfcd-4339-bdac-9877ac3f3607",
   "metadata": {},
   "source": [
    "### Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f247c9d-a576-4d1a-83a7-508ce3252b18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76578,\n",
       " [('From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decease, His tender heir might bear his memory:',\n",
       "   1),\n",
       "  ('But thou, contracted to thine own bright eyes, Feed’st thy light’s flame with self-substantial fuel, Making a famine where abundance lies, Thyself thy foe, to thy sweet self too cruel:',\n",
       "   1)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data - just load the shakespeare stuff\n",
    "data = get_dataset_from_config(config_vars, limit=SAMPLE)[1]\n",
    "len(data), data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01b72ce-07e3-4575-8b73-750b339c6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75812, 766)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test - we don't need a test set here\n",
    "data_train, data_val = train_test_split(data, test_size=0.01, random_state=SEED)\n",
    "\n",
    "# this is the labeled dataset - split into text and label lists\n",
    "data_train = split_text_and_labels(data_train)\n",
    "data_val = split_text_and_labels(data_val)\n",
    "\n",
    "# we don't need labels for causal LM\n",
    "data_train = data_train['text']\n",
    "data_val = data_val['text']\n",
    "\n",
    "len(data_train), len(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437b48-0310-4f4e-ba9f-fc222c3f84e4",
   "metadata": {},
   "source": [
    "### Cleaning and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bc6d46-e09d-4927-ae28-d28e8bdaa767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 75812\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset and DatasetDict instances - I think this is needed for model\n",
    "train_dataset = Dataset.from_dict({'text': data_train})\n",
    "val_dataset = Dataset.from_dict({'text': data_val})\n",
    "datasets = DatasetDict({'train': train_dataset, 'val': val_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3853c6fe-9d1c-43fb-b91b-7c019b812597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c54bbcd4-81ee-42c6-83d3-428829fa00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for tokenizer to use with map() method of datasetdict\n",
    "def token_preproc(data):\n",
    "    return tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f538e0b6-4460-4ba0-94a9-00d3001aa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aee35d0bdd674eb7b143de92a811fc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca359cacbb34c789ee0acc2493135eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde3decaf1ce4b079d36801362c174ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85853507fd24207a5afb9d4bb2188e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec7e6764d4534272a9de1b4a804a89e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ac93f2df08406e95b7a97fb9e28c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdde34fbafa34b4cb7879317d0367818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973ad04c6d424014a351d97d3d8f487e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 75812\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokened_data = datasets.map(token_preproc, batched=True, num_proc=4, remove_columns=['text'])\n",
    "tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a746d71-3b09-40b8-95ae-b5f3a71d44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad encodings and prep for modeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10ee63-a50a-42da-a49f-59f015e2814e",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f45202c2-393d-4e35-9ffd-fea2adf70329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(MODEL_TYPE, pad_token_id = tokenizer.eos_token_id)\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "    \n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec648afe-a7b2-464e-b805-ca0ffce0e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(32, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(32, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(32, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to special format for tf model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tf_train_set = model.prepare_tf_dataset(tokened_data['train'], shuffle=True, batch_size=32, collate_fn=collator)\n",
    "tf_val_set = model.prepare_tf_dataset(tokened_data['val'], shuffle=False, batch_size=32, collate_fn=collator)\n",
    "tf_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6abd4074-b0d6-4058-b017-67d196514c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6d0663a-d3bc-4c8d-b713-bb892ec5dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "2369/2369 [==============================] - 7908s 3s/step - loss: 4.7381 - val_loss: 4.4661\n",
      "Epoch 2/8\n",
      "2369/2369 [==============================] - 7645s 3s/step - loss: 4.4453 - val_loss: 4.3511\n",
      "Epoch 3/8\n",
      "2369/2369 [==============================] - 7717s 3s/step - loss: 4.3184 - val_loss: 4.2878\n",
      "Epoch 4/8\n",
      "2369/2369 [==============================] - 7678s 3s/step - loss: 4.2248 - val_loss: 4.2451\n",
      "Epoch 5/8\n",
      "2369/2369 [==============================] - 7681s 3s/step - loss: 4.1501 - val_loss: 4.2162\n",
      "Epoch 6/8\n",
      "2369/2369 [==============================] - 7679s 3s/step - loss: 4.0842 - val_loss: 4.1992\n",
      "Epoch 7/8\n",
      "2369/2369 [==============================] - 7708s 3s/step - loss: 4.0278 - val_loss: 4.1856\n",
      "Epoch 8/8\n",
      "2369/2369 [==============================] - 7686s 3s/step - loss: 3.9731 - val_loss: 4.1771\n"
     ]
    }
   ],
   "source": [
    "# fit model (if pretrained does not exist)\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    model.fit(tf_train_set, validation_data=tf_val_set, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b5ee69e-b6b2-4f6d-822d-e42a0a75eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    model.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9205b01-beaf-4603-9836-01cfcefe172e",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc98e32b-111c-4dc0-843c-252cab0de139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, Done!\n",
      "\n",
      "\n",
      "Original: Have I once lived to see\n",
      "Generated: Have I once lived to see in this a man? Would he had not been born of such a burden as thee, for that fortune hath been in him since my father died. For one thing: ’tis the least false which may be true; It hath in one thing only known to me. In another it is the very privilege That thou comest with mine eye When thou hast first received her fair queen and her child So well deserved when she died— But there must at least be more from me than they show From\n",
      "\n",
      "\n",
      "Original: Live And deal with\n",
      "Generated: Live And deal with all our fortunes. This is the Duke of Norfolk that brings it forward, Or any of the country’s great armies come To fight against this sight and love, That no man in France but himself Will be consul to you if I do not make his Grace a queen? If my true heart so true As he hath done her wrongings (Who bears here by false title), Make him peace for herself! The King shall live well enough ere long For Henry to die on thy head\n",
      "\n",
      "\n",
      "Original: God witness with me, I have\n",
      "Generated: God witness with me, I have a king of kings and ministers at hand. If that my name be true, The people will speak as truth but fear what God hath wrought for them all. With good excuse tell you this vile curse is laid upon both men and children; And not in our hearts. Let’s see the guiltless time pass By shame or war which may hide it from your eye To find some cause to blame whilst on our souls. Before him be found guilty Of slanderous sins committed by one man more\n",
      "\n",
      "\n",
      "Original: For this pains, Caesar\n",
      "Generated: For this pains, Caesar and the rest—and most bitter words. But what good reason is That I have so many mouths against you? Or rather with one that stands in contempt Of all honorable people, than that I am not A man but myself; sir, why does he keep me there till my death Befall’n him or should break My will by means I may wish his love To mine own comfort from henceforth? Which no other name can call shame ‘fore my departure When none shall be proud\n",
      "\n",
      "\n",
      "Original: Ah, my back,\n",
      "Generated: Ah, my back, and poor hand! With mine own hands? By what a woman were this tender girl. That I am yours if not dead And yet the world may remember you all of these tears When thou hast spoke too true now: Thou hadst but thy tongue as far from me As was from our love as to thee From that which we made by your Grace. For so I did see The sight and smell here of heaven in thee; I hope for God’s sake let it be forgot To give\n",
      "\n",
      "\n",
      "Original: And ne’er was Agamemnon’s brother wronged By that false woman\n",
      "Generated: And ne’er was Agamemnon’s brother wronged By that false woman in the town, I doubt now my heart Were at a point of grief more than mine. Since his birth so late In all parts o’erswatched there, I do think He had some malice to be revenged on thee by thine alone To make her husband mad for it That this did make him an instrument of nature And turn me from herself ere thou couldst have heard—to know Afeard how much she loves! How often She would love thy master when he\n",
      "\n",
      "\n",
      "Original: That ring’s a\n",
      "Generated: That ring’s a thousand times more precious Than one mine, that I have in the world bought for my sister. To borrow and to leave her not with money would make another wife; As if it were well spent on his behalf by all the world— But you are as welcome! She shall marry me tonight without fear of death And no matter your title or countenance which way she may choose This present day before our coming forth Unto her husband. So we will follow this marriage out the front door With many other\n",
      "\n",
      "\n",
      "Original: I am your\n",
      "Generated: I am your good lord, And you must take my father’s leave Of that which he hath denied. I do give The name of his will to both him and me; To none else but myself alone. For all men here know not what is in heaven When there they are bound together, But for their best sake be at stake Or die on those charges wherein That hold up so precious life There is no honor ‘gainst the shame of such a king As yet shall yield it out Against\n",
      "\n",
      "\n",
      "Original: The dew is raw\n",
      "Generated: The dew is raw and raw; and my soul cannot tell if it had been a hair. To get the rest out of him makes me mad, And I have seen in mine eyes that this world’s as foul As doth your father to thee: In love thou shalt show thyself but in her own bosom Shall evermore wear a head than thy uncle hath. More wretched still than thy father, who, being so dear, Is bound thus by a dowry made Of many men. A poor\n",
      "\n",
      "\n",
      "Original: So, here comes\n",
      "Generated: So, here comes a stranger. I must leave him in his cell at sea alone; he may not speak to me till the Duke of Norfolk come forth or no afterwards that is past twenty year old. And as my father told them with oaths and solemnity, So let it go! As thou art now born before thee, there is something more noble To know how this was made up so: Some fair-faced wench from York will say What shall be spoke by thy royal handmaid—a very rare\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lines = [data_train[random.randint(0,len(data) - 1)] for _ in range(10)]\n",
    "fragment_ratio = 0.7\n",
    "\n",
    "generated = list()\n",
    "for line in test_lines:\n",
    "    words = line.split()\n",
    "    fragment_end_ix = int(len(words) * fragment_ratio) or 1\n",
    "    line = ' '.join(words[:fragment_end_ix])\n",
    "    generated.append((line, generate_from(line, model, tokenizer)))\n",
    "    print(len(generated), end=', ')\n",
    "    \n",
    "print('Done!\\n\\n')\n",
    "\n",
    "for line in generated:\n",
    "    print(f'Original: {line[0]}\\nGenerated: {line[1]}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2dc1d-4f27-47f5-9a2f-a3940d67014d",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "There is a perplexity metric which, in my understanding, provides a score for how confused the model was in generating next words. Due to dependency issues in using [Hugging Face's Evaluate library](https://huggingface.co/docs/evaluate/index) I have not used that here. I also feel that the perplexity metric is maybe too generalized for what I'm looking for. See the classification notebook for my alternative metric for determining accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf742070-67be-4913-8246-0eacf1f8bc47",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The fine-tuned GPT2 model generates text that, to me, seems reasonably Shakespearean. The real determination will be pitting this against other period works, which often sound similar. The classification notebook will explore a classification model that seeks to classify Shakespearean vs. Non-Shakespearean text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
