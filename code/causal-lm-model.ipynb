{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e247aac4-af16-4b6e-be7c-8a32403263d8",
   "metadata": {},
   "source": [
    "# GA Capstone\n",
    "## Causal Language Transformer Modeling with GPT2\n",
    "\n",
    "Creating a model to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2444a-4ed5-4266-b23f-6e4cafcfd25d",
   "metadata": {},
   "source": [
    "A lot of the below is adapted from the gpt2 tutorial at https://huggingface.co/docs/transformers/v4.22.2/en/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ebac-93be-488f-b5b8-3fc324c36fe7",
   "metadata": {},
   "source": [
    "### Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c7b112-6339-422c-8b13-6beae6a1ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting for model\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# lm collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# model and support\n",
    "from transformers import TFAutoModelForCausalLM, create_optimizer, AdamWeightDecay\n",
    "\n",
    "# support\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "\n",
    "# custom utilities\n",
    "from utilities.utilities import load_config, get_dataset_from_config\n",
    "from utilities.utilities import split_text_and_labels\n",
    "from utilities.utilities import generate_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a405336-299b-433e-8bc3-670f9e5652ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RANDOM_SEED': 1,\n",
       " 'MODEL_DIR': '../models/',\n",
       " 'DATA_DIR': '../data/',\n",
       " 'CAUSAL_N_EPOCHS': 8,\n",
       " 'CLASS_N_EPOCHS': 2,\n",
       " 'BATCH_SIZE': 16,\n",
       " 'CAUSAL_MODEL': 'distilgpt2',\n",
       " 'CLASS_MODEL': 'distilbert-base-uncased',\n",
       " 'MODEL_NAME': 'shakespeare',\n",
       " 'DATA_SHAKESPEARE': ['shakespeare-sonnets.clean.txt', 'shakespeareplays.txt'],\n",
       " 'DATA_OTHER': ['belloc_hilaire-sonnets_and_verse.clean.txt',\n",
       "  'blake_william-poems.clean.txt',\n",
       "  'browning_elizabeth-sonnets_from_the_portuguese.clean.txt',\n",
       "  'daniel_samuel_and_constable_henry-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'donne_john-poetry_vol_1.clean.txt',\n",
       "  'drayton_michael_et_al-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'farjeon_eleanor-sonnets_and_poems.clean.txt',\n",
       "  'keats_john-poems_1820.clean.txt',\n",
       "  'lodge_thomas_and_fletcher_giles-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'lovell_robert_and_southey_robert-poems.clean.txt',\n",
       "  'milton_john-poetical_works.clean.txt',\n",
       "  'seward_anna-sonnets-and-odes.clean.txt',\n",
       "  'shelley_percy-complete_poetic_works.clean.txt',\n",
       "  'wilde_oscar-poems.clean.txt',\n",
       "  'wilde_oscar-selected_prose.txt',\n",
       "  'gpt2-text.txt'],\n",
       " 'N_SAMPLES': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_FILE = 'config.json'\n",
    "\n",
    "config_vars = load_config(CONFIG_FILE)\n",
    "config_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40b0184-61c8-4b8c-9be3-bca0f00cac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = config_vars['RANDOM_SEED'] if 'RANDOM_SEED' in config_vars else 1\n",
    "\n",
    "# pretrained model designator\n",
    "MODEL_TYPE = config_vars['CAUSAL_MODEL'] if 'CAUSAL_MODEL' in config_vars else 'distilgpt2'\n",
    "\n",
    "# model batch size\n",
    "BATCH_SIZE = config_vars['BATCH_SIZE'] if 'BATCH_SIZE' in config_vars else 16\n",
    "\n",
    "# model num epochs\n",
    "N_EPOCHS = config_vars['CAUSAL_N_EPOCHS'] if 'CAUSAL_N_EPOCHS' in config_vars else 8\n",
    "\n",
    "# whether to downsample\n",
    "SAMPLE = config_vars['N_SAMPLES'] if 'N_SAMPLES' in config_vars else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3af3b72-26ba-46b1-9e03-40a695527906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories and other constants, from config.json\n",
    "\n",
    "# model name for saving\n",
    "MODEL_NAME = config_vars['MODEL_NAME'] if 'MODEL_NAME' in config_vars else 'shakespeare'\n",
    "\n",
    "# directory for saved models\n",
    "MODEL_DIR = config_vars['MODEL_DIR'] if 'MODEL_DIR' in config_vars else '../models/'\n",
    "\n",
    "# full model save path\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, f'{MODEL_NAME}.{MODEL_TYPE}.{str(N_EPOCHS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959970d-dfcd-4339-bdac-9877ac3f3607",
   "metadata": {},
   "source": [
    "### Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f247c9d-a576-4d1a-83a7-508ce3252b18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76578,\n",
       " [('From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decease, His tender heir might bear his memory:',\n",
       "   1),\n",
       "  ('But thou, contracted to thine own bright eyes, Feed’st thy light’s flame with self-substantial fuel, Making a famine where abundance lies, Thyself thy foe, to thy sweet self too cruel:',\n",
       "   1)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data - just load the shakespeare stuff\n",
    "data = get_dataset_from_config(config_vars, limit=SAMPLE)[1]\n",
    "len(data), data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01b72ce-07e3-4575-8b73-750b339c6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75812, 766)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test - we don't need a test set here\n",
    "data_train, data_val = train_test_split(data, test_size=0.01, random_state=SEED)\n",
    "\n",
    "# this is the labeled dataset - split into text and label lists\n",
    "data_train = split_text_and_labels(data_train)\n",
    "data_val = split_text_and_labels(data_val)\n",
    "\n",
    "# we don't need labels for causal LM\n",
    "data_train = data_train['text']\n",
    "data_val = data_val['text']\n",
    "\n",
    "len(data_train), len(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437b48-0310-4f4e-ba9f-fc222c3f84e4",
   "metadata": {},
   "source": [
    "### Cleaning and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69bc6d46-e09d-4927-ae28-d28e8bdaa767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 75812\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset and DatasetDict instances - I think this is needed for model\n",
    "train_dataset = Dataset.from_dict({'text': data_train})\n",
    "val_dataset = Dataset.from_dict({'text': data_val})\n",
    "datasets = DatasetDict({'train': train_dataset, 'val': val_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3853c6fe-9d1c-43fb-b91b-7c019b812597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c54bbcd4-81ee-42c6-83d3-428829fa00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for tokenizer to use with map() method of datasetdict\n",
    "def token_preproc(data):\n",
    "    return tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f538e0b6-4460-4ba0-94a9-00d3001aa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d422ccc8f1942daa9244ac6f4fd609a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e126e6c4a38a4621baa5958d21f0498e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728a62bbe5324591963fc4718617de9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c69047a01345dbbea68d5bd0a7d900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b275b555914894be4d552baefd87d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0db5c1706974222bf930c5d5b1db8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78755d0cb6644269bca674a0bfeb8ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "851ed96bc1d0400290d42da3d31f368d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 75812\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokened_data = datasets.map(token_preproc, batched=True, num_proc=4, remove_columns=['text'])\n",
    "tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a746d71-3b09-40b8-95ae-b5f3a71d44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad encodings and prep for modeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10ee63-a50a-42da-a49f-59f015e2814e",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f45202c2-393d-4e35-9ffd-fea2adf70329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ../models/shakespeare.distilgpt2.8.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(MODEL_TYPE, pad_token_id = tokenizer.eos_token_id)\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "    \n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec648afe-a7b2-464e-b805-ca0ffce0e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(32, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(32, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(32, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to special format for tf model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tf_train_set = model.prepare_tf_dataset(tokened_data['train'], shuffle=True, batch_size=32, collate_fn=collator)\n",
    "tf_val_set = model.prepare_tf_dataset(tokened_data['val'], shuffle=False, batch_size=32, collate_fn=collator)\n",
    "tf_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6abd4074-b0d6-4058-b017-67d196514c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6d0663a-d3bc-4c8d-b713-bb892ec5dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model (if pretrained does not exist)\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    model.fit(tf_train_set, validation_data=tf_val_set, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b5ee69e-b6b2-4f6d-822d-e42a0a75eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    model.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9205b01-beaf-4603-9836-01cfcefe172e",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc98e32b-111c-4dc0-843c-252cab0de139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5, 6, 7, 8, 9, 10, Done!\n",
      "\n",
      "\n",
      "Original: That’s Hector, that, that,\n",
      "Generated: That’s Hector, that, that, and I am thy father. The King That hath given you my charge is not afraid to hear of these accidents And knows no other course beyond the realm Can be stopped by a law Which God hath made me call safe for: Not knowing what place there’ll or what conditions Our state shall bear it home forever Unto mine own safety. Should he dare? No cause would hold forth My good opinion more than thee if thou dost believe him alive; Or else ‘Forgive well his\n",
      "\n",
      "\n",
      "Original: What is\n",
      "Generated: What is the matter, lady? Wherefore doth it find me That he hath a name of Henry’s being called So rare indeed as thy father—? And by the presence that I see thee in? The face and complexion Of such an unspeakable majesty there be in thee? Which, my Lord Protector unto his lord? Who didst thou bring all to light Till then time had made him tremble When he perceived this strange apparition? From thy very lips it spurnè\n",
      "\n",
      "\n",
      "Original: Like a demigod here sit I in the sky, And\n",
      "Generated: Like a demigod here sit I in the sky, And all night stands my dear lord and father’s; He is come hither. We like the stars of heaven as we did by Earth, And with the first moon on ‘twixt us— This fair orb will be full bright Even at this sunning place of hell. They shall shine Like one another there o’eral To look upon them beholding forth And make themselves think they have died for him, Being now brought to light From whence he comes his daughter so dear\n",
      "\n",
      "\n",
      "Original: Out on\n",
      "Generated: Out on! With all my heart, here to stay. For I am but of that word and tongue. As they say at the table do! Wherein there is a queen’s love gone; That she may be feared or punished by danger But for her ill-conceived eye shall die In his stead when he turns away And with her duke leaves no room left in these wars A widow still alive not only dead as men? To have this woman died before us Or else give him leave\n",
      "\n",
      "\n",
      "Original: He may,\n",
      "Generated: He may, but not so far. A lady that was at the stake of all this will be well placed for it and would have her turn in a most reverend manner after any other day’s work; there is no better way to do more than she should by these circumstances go along alone; I must therefore put you my thoughts into your prayers—that some friends might show their hearts toward me with as keen looks on her behalf; what doth she think? By whom or where they know Her\n",
      "\n",
      "\n",
      "Original: Speak,\n",
      "Generated: Speak, be bold with me. By the way! Speak thus aloud of what I have heard: When Cassius and Rome all that know you truly are That fear their country more than Caesar. But think how quickly in heart they fly—and as swift As flies on fiery meteors They say nothing when they fly How low to fall at sea; yet let them stay but Forsooth. Let his passion speak for itself The which might move him toward death And show thee a true grace in this case.\n",
      "\n",
      "\n",
      "Original: Why, and I trust I may go\n",
      "Generated: Why, and I trust I may go so far. Though she be out yet now there alone? She would not wish me the benefit that might turn my face again! All this is for nothing to do with her pleasure; It shall be better than death or madness should make thee happy To look upon myself in spite of all reason: Why think they here on earth Have no love but lust more than men’s? When their souls are in vain? Let it rest behind your father And give ‘all reasons That thou hast been\n",
      "\n",
      "\n",
      "Original: Not I, for this\n",
      "Generated: Not I, for this is my death. But not mine: When you have power to rule upon me— Unless in the hand Of Lord Mortimer—you must take My head off The hands of men like thee Unto the sovereign of them all! It hath been one time before that Thisbe broke with Warwick till now; That England did never so secure your life By being against it from hence Until late afterwards. Yet come thou out and tell us what’s passing ‘tis And why we are bound unto\n",
      "\n",
      "\n",
      "Original: Your master, Pindarus, In his own change or by ill officers, Hath given me some worthy cause to wish Things done undone,\n",
      "Generated: Your master, Pindarus, In his own change or by ill officers, Hath given me some worthy cause to wish Things done undone, that he may have him come To help you. I could make thee part. But this is not the case again; Your Highness’ good care Did well in my behalf give time To answer of your request and counsel As thou art presently acquainted with The course of our journey And what offense we shall undertake With ease than did last night Thy royal father, Sir Antony—who lost most Honor! My hand was wont so far from the ground That Hector stood before me At Ephesus,\n",
      "\n",
      "\n",
      "Original: I think Hector was\n",
      "Generated: I think Hector was dead. And he is come to know what you are not of; I have no choice but to speak with him before the court: To take away your father’s grace from King Antony, who shall enjoy his Highness through a most holy and virtuous life. Who hath done so well since last March A poor peasant that died as king in Cyprus May bear it much note by ‘t again! Wherefore should they complain? My lord Must be my subject now o’ th\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lines = [data_train[random.randint(0,len(data) - 1)] for _ in range(10)]\n",
    "fragment_ratio = 0.7\n",
    "\n",
    "generated = list()\n",
    "for line in test_lines:\n",
    "    words = line.split()\n",
    "    fragment_end_ix = int(len(words) * fragment_ratio) or 1\n",
    "    line = ' '.join(words[:fragment_end_ix])\n",
    "    generated.append((line, generate_from(line, model, tokenizer)))\n",
    "    print(len(generated), end=', ')\n",
    "    \n",
    "print('Done!\\n\\n')\n",
    "\n",
    "for line in generated:\n",
    "    print(f'Original: {line[0]}\\nGenerated: {line[1]}\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a2dc1d-4f27-47f5-9a2f-a3940d67014d",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "There is a perplexity metric which, in my understanding, provides a score for how confused the model was in generating next words. Due to dependency issues in using [Hugging Face's Evaluate library](https://huggingface.co/docs/evaluate/index) I have not used that here. I also feel that the perplexity metric is maybe too generalized for what I'm looking for. See the `classification-model` notebook for my alternative metric for determining accuracy, and the `causal-lm-evaluation` notebook for my application of that metric to generated text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf742070-67be-4913-8246-0eacf1f8bc47",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The fine-tuned GPT2 model generates text that, to me, seems reasonably Shakespearean. The real determination will be pitting this against other period works, which often sound similar. The classification notebook will explore a classification model that seeks to classify Shakespearean vs. Non-Shakespearean text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
