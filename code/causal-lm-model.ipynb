{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e247aac4-af16-4b6e-be7c-8a32403263d8",
   "metadata": {},
   "source": [
    "# GA Capstone\n",
    "## Causal Language Transformer Modeling with GPT2\n",
    "\n",
    "Creating a model to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2444a-4ed5-4266-b23f-6e4cafcfd25d",
   "metadata": {},
   "source": [
    "A lot of the below is adapted from the gpt2 tutorial at https://huggingface.co/docs/transformers/v4.22.2/en/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ebac-93be-488f-b5b8-3fc324c36fe7",
   "metadata": {},
   "source": [
    "### Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c7b112-6339-422c-8b13-6beae6a1ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting for model\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# lm collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# model and support\n",
    "from transformers import TFAutoModelForCausalLM, create_optimizer, AdamWeightDecay\n",
    "\n",
    "# other utilities\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "from utilities.utilities import load_config, get_dataset_from_config\n",
    "from utilities.utilities import split_text_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a405336-299b-433e-8bc3-670f9e5652ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL_DIR': '../models/',\n",
       " 'DATA_DIR': '../data/',\n",
       " 'CAUSAL_N_EPOCHS': 4,\n",
       " 'CLASS_N_EPOCHS': 8,\n",
       " 'BATCH_SIZE': 16,\n",
       " 'CAUSAL_MODEL': 'distilgpt2',\n",
       " 'CLASS_MODEL': 'distilbert-base-uncased',\n",
       " 'MODEL_NAME': 'shakespeare',\n",
       " 'DATA_SHAKESPEARE': ['shakespeare-sonnets.clean.txt', 'shakespeareplays.txt'],\n",
       " 'DATA_OTHER': ['belloc_hilaire-sonnets_and_verse.clean.txt',\n",
       "  'blake_william-poems.clean.txt',\n",
       "  'browning_elizabeth-sonnets_from_the_portuguese.clean.txt',\n",
       "  'daniel_samuel_and_constable_henry-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'donne_john-poetry_vol_1.clean.txt',\n",
       "  'drayton_michael_et_al-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'farjeon_eleanor-sonnets_and_poems.clean.txt',\n",
       "  'keats_john-poems_1820.clean.txt',\n",
       "  'lodge_thomas_and_fletcher_giles-elizabethan_sonnet_cycles.clean.txt',\n",
       "  'lovell_robert_and_southey_robert-poems.clean.txt',\n",
       "  'milton_john-poetical_works.clean.txt',\n",
       "  'seward_anna-sonnets-and-odes.clean.txt',\n",
       "  'shelley_percy-complete_poetic_works.clean.txt',\n",
       "  'wilde_oscar-poems.clean.txt',\n",
       "  'wilde_oscar-selected_prose.txt',\n",
       "  'gpt2-text.txt'],\n",
       " 'N_SAMPLES': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_FILE = 'config.json'\n",
    "\n",
    "config_vars = load_config(CONFIG_FILE)\n",
    "config_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40b0184-61c8-4b8c-9be3-bca0f00cac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model designator\n",
    "MODEL_TYPE = config_vars['CAUSAL_MODEL'] if 'CAUSAL_MODEL' in config_vars else 'distilgpt2'\n",
    "\n",
    "# model batch size\n",
    "BATCH_SIZE = config_vars['BATCH_SIZE'] if 'BATCH_SIZE' in config_vars else 16\n",
    "\n",
    "# model num epochs\n",
    "N_EPOCHS = config_vars['CAUSAL_N_EPOCHS'] if 'CAUSAL_N_EPOCHS' in config_vars else 8\n",
    "\n",
    "# whether to downsample\n",
    "SAMPLE = config_vars['N_SAMPLES'] if 'N_SAMPLES' in config_vars else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3af3b72-26ba-46b1-9e03-40a695527906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories and other constants, from config.json\n",
    "\n",
    "# model name for saving\n",
    "MODEL_NAME = config_vars['MODEL_NAME'] if 'MODEL_NAME' in config_vars else 'shakespeare'\n",
    "\n",
    "# directory for saved models\n",
    "MODEL_DIR = config_vars['MODEL_DIR'] if 'MODEL_DIR' in config_vars else '../models/'\n",
    "\n",
    "# full model save path\n",
    "MODEL_PATH = os.path.join(MODEL_DIR, f'{MODEL_NAME}.{MODEL_TYPE}.{str(N_EPOCHS)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959970d-dfcd-4339-bdac-9877ac3f3607",
   "metadata": {},
   "source": [
    "### Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f247c9d-a576-4d1a-83a7-508ce3252b18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76578,\n",
       " [('From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decease, His tender heir might bear his memory:',\n",
       "   1),\n",
       "  ('But thou, contracted to thine own bright eyes, Feed’st thy light’s flame with self-substantial fuel, Making a famine where abundance lies, Thyself thy foe, to thy sweet self too cruel:',\n",
       "   1)])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data - just load the shakespeare stuff\n",
    "data = get_dataset_from_config(config_vars, limit=SAMPLE)[1]\n",
    "len(data), data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c01b72ce-07e3-4575-8b73-750b339c6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75812, 766)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test - we don't need a test set here\n",
    "data_train, data_val = train_test_split(data, test_size=0.01)\n",
    "\n",
    "# this is the labeled dataset - split into text and label lists\n",
    "data_train = split_text_and_labels(data_train)\n",
    "data_val = split_text_and_labels(data_val)\n",
    "\n",
    "# we don't need labels for causal LM\n",
    "data_train = data_train['text']\n",
    "data_val = data_val['text']\n",
    "\n",
    "len(data_train), len(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437b48-0310-4f4e-ba9f-fc222c3f84e4",
   "metadata": {},
   "source": [
    "### Cleaning and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69bc6d46-e09d-4927-ae28-d28e8bdaa767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 75812\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset and DatasetDict instances - I think this is needed for model\n",
    "train_dataset = Dataset.from_dict({'text': data_train})\n",
    "val_dataset = Dataset.from_dict({'text': data_val})\n",
    "datasets = DatasetDict({'train': train_dataset, 'val': val_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3853c6fe-9d1c-43fb-b91b-7c019b812597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c54bbcd4-81ee-42c6-83d3-428829fa00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for tokenizer to use with map() method of datasetdict\n",
    "def token_preproc(data):\n",
    "    return tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f538e0b6-4460-4ba0-94a9-00d3001aa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d19defdebc49d0bc86e4294b845c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea8f16ca67e4103b3ec7addcbcd33a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa7aa10e336a4504b36e08a88738b2fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70751fa9309f452da2b10378a4952603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704ad1f2bb194af28b9907bac62bf259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a794c5e4085e4b36b2fec12bc18b8a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa46a206f0d46d0951ffbfe6d4a5aa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133ff78a4f944cee9a646c5bae62a50a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 75812\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 766\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokened_data = datasets.map(token_preproc, batched=True, num_proc=4, remove_columns=['text'])\n",
    "tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a746d71-3b09-40b8-95ae-b5f3a71d44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad encodings and prep for modeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10ee63-a50a-42da-a49f-59f015e2814e",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45202c2-393d-4e35-9ffd-fea2adf70329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(MODEL_TYPE, pad_token_id = tokenizer.eos_token_id)\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(MODEL_PATH)\n",
    "    \n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec648afe-a7b2-464e-b805-ca0ffce0e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(32, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(32, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(32, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to special format for tf model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tf_train_set = model.prepare_tf_dataset(tokened_data['train'], shuffle=True, batch_size=32, collate_fn=collator)\n",
    "tf_val_set = model.prepare_tf_dataset(tokened_data['val'], shuffle=False, batch_size=32, collate_fn=collator)\n",
    "tf_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6abd4074-b0d6-4058-b017-67d196514c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a6d0663a-d3bc-4c8d-b713-bb892ec5dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2369/2369 [==============================] - 7501s 3s/step - loss: 4.7405 - val_loss: 4.4643\n",
      "Epoch 2/4\n",
      "2369/2369 [==============================] - 7419s 3s/step - loss: 4.4440 - val_loss: 4.3458\n",
      "Epoch 3/4\n",
      "2369/2369 [==============================] - 7353s 3s/step - loss: 4.3189 - val_loss: 4.2735\n",
      "Epoch 4/4\n",
      "2369/2369 [==============================] - 7369s 3s/step - loss: 4.2261 - val_loss: 4.2278\n"
     ]
    }
   ],
   "source": [
    "# fit model (if pretrained does not exist)\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    model.fit(tf_train_set, validation_data=tf_val_set, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d52f1bb4-92fa-4a4e-ac0d-fcb230031be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2369/2369 [==============================] - 7396s 3s/step - loss: 4.1500 - val_loss: 4.1956\n",
      "Epoch 2/4\n",
      "2369/2369 [==============================] - 7489s 3s/step - loss: 4.0854 - val_loss: 4.1700\n",
      "Epoch 3/4\n",
      "2369/2369 [==============================] - 7496s 3s/step - loss: 4.0285 - val_loss: 4.1532\n",
      "Epoch 4/4\n",
      "2369/2369 [==============================] - 7572s 3s/step - loss: 3.9753 - val_loss: 4.1442\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x175374b80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_set, validation_data=tf_val_set, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b5ee69e-b6b2-4f6d-822d-e42a0a75eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    model.save_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9205b01-beaf-4603-9836-01cfcefe172e",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7018e21-169f-4268-bf96-20b11eae6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get predicted text\n",
    "def test(text, max_new=50, temp=1, top_k=50, rep_penalty=1.5, len_penalty=0.75, n_seq=1):\n",
    "    tokened = tokenizer(text, return_tensors='tf')\n",
    "    output = model.generate(**tokened,\n",
    "                            do_sample=True,\n",
    "                            max_new_tokens=max_new, \n",
    "                            temperature=temp, \n",
    "                            top_k=top_k, \n",
    "                            repetition_penalty=rep_penalty,\n",
    "                            length_penalty=len_penalty,\n",
    "                            num_return_sequences=n_seq)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc98e32b-111c-4dc0-843c-252cab0de139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The pale moon shines by night, And when I wander here and there, I then do most go right.', 'Methinks these peers of France should smile at that.', 'So, love, be thou, although to-day thou fill Thy hungry eyes, even till they wink with fulness, To-morrow see again, and do not kill The spirit of love, with a perpetual dulness.', 'Believe me, I speak as my understanding instructs me and as mine honesty puts it to utterance.', 'But thou, ’gainst all proportion, didst bring in Wonder to wait on treason and on murder, And whatsoever cunning fiend it was That wrought upon thee so preposterously Hath got the voice in hell for excellence.', 'I’ll question her.', 'Then meet me forthwith at the notary’s.', 'Better Macbeth Than such an one to reign.', 'One scene of it comes near the circumstance Which I have told thee of my father’s death.', 'My lord, I warrant you we will play our part As he shall think by our true diligence He is no less than what we say he is.']\n",
      "Original: The pale moon shines by night, And\n",
      "Output: The pale moon shines by night, And the golden moon on Earth doth shine. But yet she is not seen By the world’s eyes nor eye of heaven; For her life and beauty‘s sake, let it be known That I am no man! As thou art as fair and beautiful As he that’s born to me. Whom my youth and mine own merit may live in, So long a life and beauty shall never meet again Till at thy hands I have made thee proud for all. If ever\n",
      "\n",
      "Original: Methinks these peers\n",
      "Output: Methinks these peers are here, And I’ll make them known to you. If there be no more but this, they shall have a thousand crowns To give me to wear in my arms again Than when the time of day once was called; When the time had been called—that it was still called— That men were so fond Of their own that should not know The name and honor thereof. Who knows what? Why then we must send for him thence with us As many as may be desired By\n",
      "\n",
      "Original: So, love, be thou, although to-day thou fill Thy hungry eyes, even\n",
      "Output: So, love, be thou, although to-day thou fill Thy hungry eyes, even if thou wilt not. But when thou wilt know’st how much thou dost love me, Thou shalt see them all in one smile; And I will make thee proud of it by the way: To have no more of that than they are. Even so! When thou art dead and sad? As well as men do. With their hearts, though I thank God for this, The world is full o’ercharged with sorrows. So go you on till death\n",
      "\n",
      "Original: Believe me, I speak as my\n",
      "Output: Believe me, I speak as my love. That’s the way to heaven; The way that nature should set it apart! And yet ‘tis a thing of great import To lay on him and make his power known— As in this world he is so strong: A man with nothing but only himself Will keep us from our wronged vows. But we must for sure be true men Even now they may be false-believing fools by their own hands. If you saw them fall out against your will, You would\n",
      "\n",
      "Original: But thou, ’gainst all proportion, didst bring in Wonder to wait on treason and\n",
      "Output: But thou, ’gainst all proportion, didst bring in Wonder to wait on treason and make it known. If thy Grace— be not so great, Thou art a man of my birth—yet I am sorry That this is the way To whom that thou hast done me wrong; Yet, as you see withal: The fear of death hath made thee rich And makes thee rich again. Although I have been here for three thousand years, I never saw more shame Than when I was born there. When I will live till God give thee peace! A king but born now shall\n",
      "\n",
      "Original: I’ll\n",
      "Output: I’ll have no more to do with you, my lord. But I will be sworn it is not so; but rather stay in your company for all the rest. To make this business a better one than we did before— It shall please thee as well. And that thou dost keep me from doing any harm That ever I may speak of! As much as thy love hath done unto her and myself Shall ne—t again offend him too. Unless by some means or other which hastened Thy\n",
      "\n",
      "Original: Then meet\n",
      "Output: Then meet him, then. And when he is come to this place, let us speak with the Duke of York Before his coming hither again; we’ll be there a little while longer together To make sure all our business are well received By that which will bring you home tomorrow morning With your best preparation and more than one can think upon it. A word or two in friendship shall go anon Till death do grace thee from thy evil reigns. That thou mayst not believe me! I am\n",
      "\n",
      "Original: Better Macbeth Than\n",
      "Output: Better Macbeth Than a man, the better. But I have no more to say than they are Of their own worthiness and of what is in them. The worse that true men’s souls Are at once by virtue. Most goodly creatures—for whom we call ourselves! All things not worth your help Must be done for us; but you shall find Our justice do us good service And make it ours too. By him, my lord: For he hath made me mad To think this was mine\n",
      "\n",
      "Original: One scene of it comes near\n",
      "Output: One scene of it comes near the Duke. The very second part is a tale told, and all but one; One which would be known to be true or false— But not so much as an article in truth. A word more than another: For he hath no other name for that title! From whence you come? And here tell him what I have done To give my master his daughter this ring with her hand Against your royal father’s head if she bear ‘her tongue at home Or else refuse to take\n",
      "\n",
      "Original: My lord, I warrant you we will play our part As\n",
      "Output: My lord, I warrant you we will play our part As if the world were in love. But to be true and to know it is not so; Ourself are as free from this strife as ever was before When all our friends did stand upon the Earth. We have sworn no war unless your Grace’s death or His pardon come near us. That he may live a good life with him And let his father make peace for himself! To that end shall never be my wish— Unless heaven give me leave of thee forever For what thou hast done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lines = [data_train[random.randint(0,len(data) - 1)] for _ in range(10)]\n",
    "fragment_ratio = 0.4\n",
    "print(test_lines)\n",
    "\n",
    "for line in test_lines:\n",
    "    words = line.split()\n",
    "    stop_ix = int(len(words) * fragment_ratio) or 1\n",
    "    fragment = ' '.join(words[0:stop_ix])\n",
    "    print(f'Original: {fragment}')\n",
    "    output = test(fragment,\n",
    "                  temp=0.5,\n",
    "                  max_new=100,\n",
    "                  top_k=200,\n",
    "                  rep_penalty=1.5,\n",
    "                  len_penalty=0.75,\n",
    "                  n_seq=1)\n",
    "    print(f'Output: {output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf742070-67be-4913-8246-0eacf1f8bc47",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Text is generated that at many times sounds reasonably Shakespearian. Soon to be combined with classification model to test this out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
