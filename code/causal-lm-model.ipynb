{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e247aac4-af16-4b6e-be7c-8a32403263d8",
   "metadata": {},
   "source": [
    "# GA Capstone\n",
    "## Causal Language Transformer Modeling with GPT2\n",
    "\n",
    "Creating a model to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2444a-4ed5-4266-b23f-6e4cafcfd25d",
   "metadata": {},
   "source": [
    "A lot of the below is adapted from the gpt2 tutorial at https://huggingface.co/docs/transformers/v4.22.2/en/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ebac-93be-488f-b5b8-3fc324c36fe7",
   "metadata": {},
   "source": [
    "### Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c7b112-6339-422c-8b13-6beae6a1ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting for model\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# lm collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# model and support\n",
    "from transformers import TFAutoModelForCausalLM, create_optimizer, AdamWeightDecay\n",
    "\n",
    "# other utilities\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import json\n",
    "from utilities.utilities import load_config, get_dataset_from_config\n",
    "from utilities.utilities import split_text_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a405336-299b-433e-8bc3-670f9e5652ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = 'config.json'\n",
    "\n",
    "config_vars = load_config(CONFIG_FILE)\n",
    "config_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40b0184-61c8-4b8c-9be3-bca0f00cac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model designator\n",
    "MODEL_TYPE = config_vars['CAUSAL_MODEL'] if 'CAUSAL_MODEL' in config_vars else 'distilgpt2'\n",
    "\n",
    "# model batch size\n",
    "BATCH_SIZE = config_vars['BATCH_SIZE'] if 'BATCH_SIZE' in config_vars else 16\n",
    "\n",
    "# model num epochs\n",
    "N_EPOCHS = config_vars['CAUSAL_N_EPOCHS'] if 'CAUSAL_N_EPOCHS' in config_vars else 8\n",
    "\n",
    "# whether to downsample\n",
    "SAMPLE = config_vars['N_SAMPLES'] if 'N_SAMPLES' in config_vars else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3af3b72-26ba-46b1-9e03-40a695527906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories and other constants, from config.json\n",
    "\n",
    "# model name for saving\n",
    "MODEL_NAME = config_vars['MODEL_NAME'] if 'MODEL_NAME' in config_vars else 'shakespeare'\n",
    "\n",
    "# directory for saved models\n",
    "DIR_MODEL = config_vars['MODEL_DIR'] if 'MODEL_DIR' in config_vars else '../models/'\n",
    "\n",
    "# directory for text data\n",
    "DIR_DATA = config_vars['DATA_DIR'] if 'DATA_DIR' in config_vars else '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959970d-dfcd-4339-bdac-9877ac3f3607",
   "metadata": {},
   "source": [
    "### Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f247c9d-a576-4d1a-83a7-508ce3252b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data - just load the shakespeare stuff\n",
    "data = get_dataset_from_config(config_vars, limit=SAMPLE)[1]\n",
    "len(data), data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c01b72ce-07e3-4575-8b73-750b339c6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74663, 1915)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test - we don't need a test set here\n",
    "data_train, data_val = train_test_split(data, test_size=0.01)\n",
    "\n",
    "# this is the labeled dataset - split into text and label lists\n",
    "data_train = split_text_and_labels(data_train)\n",
    "data_val = split_text_and_labels(data_val)\n",
    "\n",
    "# we don't need labels for causal LM\n",
    "data_train = data_train['text']\n",
    "data_val = data_val['test']\n",
    "\n",
    "len(data_train), len(data_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437b48-0310-4f4e-ba9f-fc222c3f84e4",
   "metadata": {},
   "source": [
    "### Cleaning and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69bc6d46-e09d-4927-ae28-d28e8bdaa767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 74663\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1915\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset and DatasetDict instances - I think this is needed for model\n",
    "train_dataset = Dataset.from_dict({'text': data_train})\n",
    "val_dataset = Dataset.from_dict({'text': data_val})\n",
    "datasets = DatasetDict({'train': train_dataset, 'val': val_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3853c6fe-9d1c-43fb-b91b-7c019b812597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54bbcd4-81ee-42c6-83d3-428829fa00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for tokenizer to use with map() method of datasetdict\n",
    "def token_preproc(data):\n",
    "    return tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f538e0b6-4460-4ba0-94a9-00d3001aa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b942fcfcdd574af29cce1c1f00c361c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40be30e466ce4ca983ed5bfc4774d0e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05c2645caa8429c9c38010e081ecbc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f09d79c16948fcb35ca2338eea3a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/19 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4147b73c0464c94b37567e13b44e5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c27e300e5a4634b18515db96388ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a6eb25cc314a2f9af692d44386737f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f51dedcee84c09bff48a308230122b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 74663\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1915\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokened_data = datasets.map(token_preproc, batched=True, num_proc=4, remove_columns=['text'])\n",
    "tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a746d71-3b09-40b8-95ae-b5f3a71d44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad encodings and prep for modeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10ee63-a50a-42da-a49f-59f015e2814e",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f45202c2-393d-4e35-9ffd-fea2adf70329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "model_path = os.path.join(DIR_MODEL, f'{model_type}.{MODEL_NAME}.{str(N_EPOCHS)}')\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_type, pad_token_id = tokenizer.eos_token_id)\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_path)\n",
    "    \n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec648afe-a7b2-464e-b805-ca0ffce0e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(32, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(32, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(32, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to special format for tf model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tf_train_set = model.prepare_tf_dataset(tokened_data['train'], shuffle=True, batch_size=32, collate_fn=collator)\n",
    "tf_val_set = model.prepare_tf_dataset(tokened_data['val'], shuffle=False, batch_size=32, collate_fn=collator)\n",
    "tf_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6abd4074-b0d6-4058-b017-67d196514c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a6d0663a-d3bc-4c8d-b713-bb892ec5dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "2333/2333 [==============================] - 7869s 3s/step - loss: 4.7409 - val_loss: 4.4987\n",
      "Epoch 2/4\n",
      "2333/2333 [==============================] - 7239s 3s/step - loss: 4.4469 - val_loss: 4.4121\n",
      "Epoch 3/4\n",
      "2333/2333 [==============================] - 7211s 3s/step - loss: 4.3173 - val_loss: 4.3728\n",
      "Epoch 4/4\n",
      "2333/2333 [==============================] - 7193s 3s/step - loss: 4.2262 - val_loss: 4.3394\n"
     ]
    }
   ],
   "source": [
    "# fit model (if pretrained does not exist)\n",
    "if not os.path.exists(model_path):\n",
    "    model.fit(tf_train_set, validation_data=tf_val_set, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b5ee69e-b6b2-4f6d-822d-e42a0a75eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9205b01-beaf-4603-9836-01cfcefe172e",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7018e21-169f-4268-bf96-20b11eae6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get predicted text\n",
    "def test(text, max_new=50, temp=1, top_k=50, rep_penalty=1.5, len_penalty=0.75, n_seq=1):\n",
    "    tokened = tokenizer(text, return_tensors='tf')\n",
    "    output = model.generate(**tokened,\n",
    "                            do_sample=True,\n",
    "                            max_new_tokens=max_new, \n",
    "                            temperature=temp, \n",
    "                            top_k=top_k, \n",
    "                            repetition_penalty=rep_penalty,\n",
    "                            length_penalty=len_penalty,\n",
    "                            num_return_sequences=n_seq)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc98e32b-111c-4dc0-843c-252cab0de139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: I will\n",
      "Output: I will not. But I know my love is dead, And that it shall be with her; and do so by the time of death: For if she sleep tonight or tomorrow night, She’ll die for me in a coffin where no man can touch him With this dearest handkerchief to his head—a story Which must have been more true than truth! That he would draw upon your face To make you laugh at what thou hast said thus far As never before done here on Earth?\n",
      "\n",
      "Original: I do\n",
      "Output: I do not understand this man. But I know him by his name, and will tell you what he is; And to my knowledge the King of England hath sent for me— For here we are at odds with our countrymen! He’s a traitor that would have had us dead in arms upon any other occasion That might have been so grievously wronged as it is against your Majesty To make such an act himself As may be proved guilty or given up again Against all those which did bear them\n",
      "\n",
      "Original: Sir\n",
      "Output: Sir, you are a very poor gentleman. I’ll tell thee what is true and false; And so will my wife as well I can say to her that she has made me mad at him for going with the Duke of York in this case against his brother Richard—and would he have died if we had been married? He might not be dead! That your father was too virtuous by being such an honest man As all our enemies were but one-half fools or beggars Of any kind\n",
      "\n",
      "Original: Where\n",
      "Output: Where’s the boy? That that is not in this world, and all of his will Is forsworn. And ‘tis a shame to be here! How many times hath he been abused by an heirless king Than if you have made him so poor; or else your children are too rich To live with nothing but our own wealth at home When we shall see how much more than ever I do know Wherein my heart doth trust me when it hurts The better part ope\n",
      "\n",
      "Original: Fore God,\n",
      "Output: Fore God, I’ll have you there. And keep it as a secret from the world! Save for your good safety and my love to heaven; For all that is of me will be well served To put on another day more than this time Of five years in peace with thee: A happy marriage between thy husband and mine—forsooth, sir? The King shall help us both by our side By giving him leave to do so (Which he hath done), but we must go find out what\n",
      "\n",
      "Original: Two.\n",
      "Output: Two. The crown of the King! And all that I have in my heart, But for this noble lord and his brother; My father is dead—he’s gone to prison Before you can tell me more what he hath done To make him free from your tyranny: Avenge himself by any means That may be so true as mine own? Or else do not know but fear no further than death When thou hast made it known How much offense will come between yourself and thy friend This great man or\n",
      "\n",
      "Original: And in this state she gallops night\n",
      "Output: And in this state she gallops night and I have not dined. But when you do see her, be it to my sweet love’s sake. And then by the way of that will cure me for your grief: You shall find a goodly maid; but with no more than five hundred crowning jewels—that is almost as much money As gold or silver can buy from an honest man would make him rich again! So happy are we all together at once That so many three thousand knaves may come hither one day\n",
      "\n",
      "Original: I better brook the loss of\n",
      "Output: I better brook the loss of my husband, for I am a traitor. But that’s not so true to him As he shall be revenged on me if thou wilt keep it at home; And yet ‘tis such an affliction That no more can come with thee than by giving up thy life For his love and mine own sake To let this man die in honor: By any means which may deserve thine as well as himself— My heart is full ope-ho! Unless your father give you\n",
      "\n",
      "Original: Would you\n",
      "Output: Would you were the King? And would not come to know a man. So I must be your lord, sir. That is my heart and will make me glad of it; for this hour’s sake let him go away! My dear queen shall do no more than she can with such an honorable deed To keep her true love from being false-hearted men—but that was nothing in ‘t But women are as soft as thieves Are their prey when they steal money From poor souls whose husbands\n",
      "\n",
      "Original: Flibbertigibbet, of mopping and\n",
      "Output: Flibbertigibbet, of mopping and plucking. I’ll do it all to thine own honor; And that much more shall be done for thee than ever before by this gentleman! For thy sake my love is with you: If thou canst not think what a goodly fortune doth appear To me in the face Of such an eye as mine? A man would have made so great any woman— That she may take her hand if he were honest enough ‘I am sure no one else hath been deceived With\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lines = [sentences[random.randint(0,len(sentences) - 1)] for _ in range(10)]\n",
    "fragment_ratio = 0.4\n",
    "\n",
    "for line in test_lines:\n",
    "    words = line.split()\n",
    "    stop_ix = int(len(words) * fragment_ratio) or 1\n",
    "    fragment = ' '.join(words[0:stop_ix])\n",
    "    print(f'Original: {fragment}')\n",
    "    output = test(fragment,\n",
    "                  temp=0.5,\n",
    "                  max_new=100,\n",
    "                  top_k=200,\n",
    "                  rep_penalty=1.5,\n",
    "                  len_penalty=0.75,\n",
    "                  n_seq=1)\n",
    "    print(f'Output: {output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf742070-67be-4913-8246-0eacf1f8bc47",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Text is generated that at many times sounds reasonably Shakespearian. Soon to be combined with classification model to test this out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
