{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0a3b80-58dc-45dc-9cec-909d09eb0a8e",
   "metadata": {},
   "source": [
    "# GA Capstone\n",
    "## Classification Modeling\n",
    "\n",
    "The goal here is to create a binary classification model that will classify text as being Shakespearian or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cde711-25f0-452d-99ce-43c7c0532dc1",
   "metadata": {},
   "source": [
    "Much of the below is adapted from the [Hugging Face Text Classification Tutorial](https://huggingface.co/docs/transformers/tasks/sequence_classification) and the notebook linked therein."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcca06d-df6f-4773-a33c-042a392b3fd8",
   "metadata": {},
   "source": [
    "### Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb5a0184-f650-42cd-a299-38973b083610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "# Datasets for dataset formatting\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# tokenizer and collator\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "# model and optimizer\n",
    "from transformers import TFAutoModelForSequenceClassification, create_optimizer\n",
    "\n",
    "# support\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4bf935c-63d8-4ec5-9d45-91c87c625d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MODEL_DIR': '../models/',\n",
       " 'DATA_DIR': '../data/',\n",
       " 'DEFAULT_N_EPOCHS': 4,\n",
       " 'DEFAULT_CAUSAL_MODEL': 'distilgpt2',\n",
       " 'DEFAULT_CLASS_MODEL': 'distilbert-base-uncased',\n",
       " 'DEFAULT_MODEL_NAME': 'shakespeare',\n",
       " 'DATA_SHAKESPEARE': ['shakespeare-sonnets.clean.txt', 'shakespeareplays.txt'],\n",
       " 'DATA_NOT_SHAKESPEARE': {'elizabethan': ['daniel_samuel_and_constable_henry-elizabethan_sonnet_cycles.clean.txt',\n",
       "   'drayton_michael_et_al-elizabethan_sonnet_cycles.clean.txt',\n",
       "   'lodge_thomas_and_fletcher_giles-elizabethan_sonnet_cycles.clean.txt'],\n",
       "  'other': ['milton_john-poetical_works.clean.txt',\n",
       "   'belloc_hilaire-sonnets_and_verse.clean.txt',\n",
       "   'browning_elizabeth-sonnets_from_the_portuguese.clean.txt',\n",
       "   'farjeon_eleanor-sonnets_and_poems.clean.txt',\n",
       "   'lovell_robert_and_southey_robert-poems.clean.txt',\n",
       "   'seward_anna-sonnets-and-odes.clean.txt']},\n",
       " 'N_SAMPLES': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load config from json\n",
    "with open('config.json', 'r') as jsf:\n",
    "    config_vars = json.load(jsf)\n",
    "    \n",
    "config_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51254765-56de-4d56-a708-89874691e0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained model designator\n",
    "MODEL_TYPE = config_vars['DEFAULT_CLASS_MODEL']\n",
    "\n",
    "# model batch size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# model num epochs\n",
    "N_EPOCHS = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9c3d135-14a9-4d16-a834-e4d1808e3e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories, etc.\n",
    "MODEL_DIR = config_vars['MODEL_DIR']\n",
    "MODEL_NAME = config_vars['DEFAULT_MODEL_NAME']\n",
    "MODEL_FULL_PATH = os.path.join(MODEL_DIR, f'{MODEL_NAME}.{MODEL_TYPE}.{N_EPOCHS}')\n",
    "\n",
    "DATA_DIR = config_vars['DATA_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1e51b2a-4dac-4fc6-90ea-eee2a1479da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpful regexes\n",
    "RE_SENTENCE = re.compile('\\w.*?[.!?:;]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e47bfd-9402-4cea-a98e-19a48ddfc1f9",
   "metadata": {},
   "source": [
    "### Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e609c19f-90f0-4cda-9fea-bdd9b481768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4417284 ﻿From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decease, His tender heir might bear his memory: But thou, contracted to thine own bright eyes, Feed’st thy light’s flame with self\n",
      "1393736 Wonder of these, glory of other times, O thou whom envy evn is forced tadmire! Great Patroness of these my humble rhymes, Which thou from out thy greatness dost inspire! Since only thou has deigned to raise them higher, Vouchsafe now to accept them a\n"
     ]
    }
   ],
   "source": [
    "# load shakespeare data\n",
    "shakespeare = list()\n",
    "for fn in config_vars['DATA_SHAKESPEARE']:\n",
    "    path = os.path.join(DATA_DIR, fn)\n",
    "    with open(path, 'r') as f:\n",
    "        shakespeare.append(' '.join([line.strip() for line in f.readlines() if line.strip()]))\n",
    "\n",
    "shakespeare = '\\n'.join(shakespeare)\n",
    "\n",
    "other = list()\n",
    "for cat in config_vars['DATA_NOT_SHAKESPEARE'].values():\n",
    "    for fn in cat:\n",
    "        path = os.path.join(DATA_DIR, fn)\n",
    "        with open(path, 'r') as f:\n",
    "            other.append(' '.join([line.strip() for line in f.readlines() if line.strip()]))\n",
    "            \n",
    "other = '\\n'.join(other)\n",
    "\n",
    "print(len(shakespeare), shakespeare[:250])\n",
    "print(len(other), other[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0196be9e-039f-4f15-af38-09f75d466f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decease, His tender heir might bear his memory:\n",
      "Man cannot be a sophist to his heart, He must look nakedly on his intent, Expose it of all shreds of argument, And strip it like a slave-girl in the mart.\n"
     ]
    }
   ],
   "source": [
    "# split into sentences\n",
    "sentences = {\n",
    "    'shakespeare': RE_SENTENCE.findall(shakespeare),\n",
    "    'farjeon': RE_SENTENCE.findall(farjeon)\n",
    "}\n",
    "print(sentences['shakespeare'][0])\n",
    "print(sentences['farjeon'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "030a4574-c947-4adb-a838-5e870a50a593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'text': 'From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decease, His tender heir might bear his memory:',\n",
       "  'label': 1},\n",
       " {'text': 'such patience, sure, Is not lifes child and mine, but mine and deaths.',\n",
       "  'label': 0},\n",
       " {'text': 'Be wise as thou art cruel;', 'label': 1},\n",
       " {'text': 'Then fold me in your bosom so deep away That memory cannot touch this loveless day.',\n",
       "  'label': 0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually train-test split since we're not using pandas and want to retain class proportions\n",
    "test_ratio = 0.1\n",
    "test_ratio = 1 - test_ratio\n",
    "\n",
    "splits = {'test': list(), 'train': list()}\n",
    "\n",
    "for author, collection in sentences.items():\n",
    "    clen = len(collection)\n",
    "    split_ix = int(clen * test_ratio)\n",
    "    \n",
    "    # not at all elegant, but gets the job done\n",
    "    train = collection[:split_ix]\n",
    "    test = collection[split_ix:]\n",
    "    for sentence in train:\n",
    "        splits['train'].append({'text': sentence, 'label': 1 if author == 'shakespeare' else 0})\n",
    "    for sentence in test:\n",
    "        splits['test'].append({'text': sentence, 'label': 1 if author == 'shakespeare' else 0})\n",
    "        \n",
    "splits['train'][0], splits['train'][-1], splits['test'][0], splits['test'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "503c0ea5-f048-4fe9-86bb-49c23b38f8b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1024\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 114\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# format into DatasetDict format\n",
    "train_data = Dataset.from_list(splits['train'])\n",
    "test_data = Dataset.from_list(splits['test'])\n",
    "dataset = DatasetDict({'train': train_data, 'test': test_data})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9245e6c-477a-4bb5-a10b-e3b4b05283f8",
   "metadata": {},
   "source": [
    "### Tokenization and Prepping Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "472fadec-a28b-4f17-b665-1e07ec31b935",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_TYPE)\n",
    "\n",
    "def tokenizer_func(text):\n",
    "    return tokenizer(text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d23898f7-0c08-4c8d-b5ff-80c4c9222427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ce5e228454491abf0f6eb4b187565a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0ae7c769f84bce99c9d629d82720dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = dataset.map(tokenizer_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "275aaddc-47af-436d-b60c-0e17d3176bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae171d29-8444-4b3f-9285-b31debe2ce98",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02a48aeb-5be3-4b5d-94f4-6dafac68ce7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 23:52:21.663980: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-10-12 23:52:21.664004: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-12 23:52:21.664023: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (archzolam): /proc/driver/nvidia/version does not exist\n",
      "2022-10-12 23:52:21.664220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at ../models/shakespeare-farjeon.distilbert-base-uncased.8.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "if os.path.exists(MODEL_FULL_PATH):\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_FULL_PATH)\n",
    "else:\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29d8c4a8-177c-4d5c-a9a4-783ec5c1a22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# prep train and test sets for model\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_data['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collator\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_data['test'],\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0dfaccd-45fb-4b3c-b78f-821398a7862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up optimizer\n",
    "batches_per_epoch = len(tokenized_data['train']) // BATCH_SIZE\n",
    "total_train_steps = int(batches_per_epoch * N_EPOCHS)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28d60b78-d9ed-4375-840e-c7a113869257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile and fit model\n",
    "if not os.path.exists(MODEL_FULL_PATH):\n",
    "    model.compile(optimizer=optimizer)\n",
    "    model.fit(tf_train_set, validation_data=tf_test_set, epochs = N_EPOCHS)\n",
    "    os.makedirs(MODEL_FULL_PATH)\n",
    "    model.save_pretrained(MODEL_FULL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb56b0d1-9f38-4243-a7ba-d46418973264",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce694fd2-318b-4513-ba1a-050ff4882d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [sentences['farjeon'][random.randint(0,len(sentences['farjeon'])-1)] for _ in range(10)]\n",
    "tests += [sentences['shakespeare'][random.randint(0,len(sentences['shakespeare'])-1)] for _ in range(10)]\n",
    "tests += [\n",
    "    \"I'm just a guy doing what guys do\",\n",
    "    \"Get away from me you mischevious rogue!\",\n",
    "    \"Whither goest thou?\",\n",
    "    \"Romeo, O Romeo! Wherefore art thou Romeo?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc8a5a0e-aca1-4845-b18a-5996f7f45235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Why if we dare not hear make hearing ours?', 0, 0.15327503),\n",
       " ('Lo, this and this and this I did not spend!', 0, 0.14387587),\n",
       " ('And thou wilt to the earth at last, times scorn, Relinquishing a crown thou hast not worn.',\n",
       "  0,\n",
       "  0.1252184),\n",
       " ('Alas, poor fools!', 0, 0.3099175),\n",
       " ('Am I here or there?', 0, 0.1783713),\n",
       " ('But since my walls of ignorance are broken, Though on that desert knowledge builds no towers, I cannot say of life, he has not spoken, I cannot say of love, he has no powers.',\n",
       "  0,\n",
       "  0.12840965),\n",
       " ('O what damnation man would deal himself If meeting her beyond his uttermost dreams He still could face his soul and lie to her.',\n",
       "  0,\n",
       "  0.13320598),\n",
       " ('Hast neither earth nor seed?', 0, 0.14555538),\n",
       " ('A few of us who faltered as we fared Love has returned for.',\n",
       "  0,\n",
       "  0.12886521),\n",
       " ('half-truth hedged with lies!', 0, 0.13973568),\n",
       " ('Against my love shall be as I am now, With Time’s injurious hand crush’d and o’erworn;',\n",
       "  1,\n",
       "  0.9382365),\n",
       " ('My mistress’ eyes are nothing like the sun;', 1, 0.93665224),\n",
       " ('And taught it thus anew to greet;', 1, 0.93643004),\n",
       " ('And wherefore say not I that I am old?', 1, 0.9254157),\n",
       " ('Thine eyes, that taught the dumb on high to sing And heavy ignorance aloft to fly, Have added feathers to the learned’s wing And given grace a double majesty.',\n",
       "  1,\n",
       "  0.9355098),\n",
       " ('Your love and pity doth the impression fill, Which vulgar scandal stamp’d upon my brow;',\n",
       "  1,\n",
       "  0.9374806),\n",
       " ('And so of you, beauteous and lovely youth, When that shall vade, by verse distills your truth.',\n",
       "  1,\n",
       "  0.93581116),\n",
       " ('Yet do thy worst, old Time;', 1, 0.9364999),\n",
       " ('The canker blooms have full as deep a dye As the perfumed tincture of the roses.',\n",
       "  1,\n",
       "  0.8967204),\n",
       " ('But he that writes of you, if he can tell That you are you, so dignifies his story, Let him but copy what in you is writ, Not making worse what nature made so clear, And such a counterpart shall fame his wit, Making his style admired every where.',\n",
       "  1,\n",
       "  0.93248457),\n",
       " (\"I'm just a guy doing what guys do\", 0, 0.37710568),\n",
       " ('Get away from me you mischevious rogue!', 0, 0.42911646),\n",
       " ('Whither goest thou?', 1, 0.92966),\n",
       " ('Romeo, O Romeo! Wherefore art thou Romeo?', 1, 0.8363214)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_class_from_output(output):\n",
    "    return np.argmax(output.logits, axis=1)\n",
    "\n",
    "def get_probs_from_output(output, c=1):\n",
    "    logits = output.logits\n",
    "    return (np.exp(logits) / (1 + np.exp(logits)))[:,c]\n",
    "\n",
    "tests_tokened = tokenizer(tests, return_tensors='tf', padding=True)\n",
    "outputs = model(tests_tokened)\n",
    "classifications = get_class_from_output(outputs)\n",
    "probs = get_probs_from_output(outputs)\n",
    "list(zip(tests, classifications, probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6e2a5-2dba-4199-a865-e13db996c905",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Classification works pretty well! Woohoo!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
