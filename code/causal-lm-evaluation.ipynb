{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6223273-a26f-4b57-9b63-fffcc8461074",
   "metadata": {},
   "source": [
    "# GA Capstone\n",
    "## Causal Model Evaluation\n",
    "\n",
    "The goal here is to evaluate the causal model based on the classification model. To do so, I will generate text, reduced to the first sentence of each generated output, and run each sentence through the classification model. The goal is to have the most possible Shakespearean results (as the causal model is supposed to generate Shakespearean text). Score will be the percentage of Shakespearean results out of total results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c794c7-7577-408a-a480-2227f12bbe4c",
   "metadata": {},
   "source": [
    "### Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ce7c45-baa8-4047-9887-81873e800afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# models\n",
    "from transformers import TFAutoModelForCausalLM, TFAutoModelForSequenceClassification\n",
    "\n",
    "# custom utilities\n",
    "from utilities.utilities import load_config, get_model_path, load_model, load_tokenizer\n",
    "from utilities.utilities import load_text_from_config\n",
    "from utilities.utilities import generate_from\n",
    "from utilities.utilities import classify_from\n",
    "from utilities.utilities import extract_sentences\n",
    "\n",
    "# pandas for csv read and extract\n",
    "import pandas as pd\n",
    "\n",
    "# other\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ead63d-0de7-4739-b641-9fbc670a41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config file\n",
    "CONFIG_FILE = 'config.json'\n",
    "cfgvars = load_config(CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074b343-b97b-4cfd-b71c-9876d9cd4b39",
   "metadata": {},
   "source": [
    "### Load Models and Model Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43df2a50-16e8-4757-b833-d49bc5273bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ../models/shakespeare.distilgpt2.8.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Some layers from the model checkpoint at ../models/shakespeare.distilbert-base-uncased.2 were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ../models/shakespeare.distilbert-base-uncased.2 and are newly initialized: ['dropout_38']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# get model locations, load models, and load tokenizers\n",
    "causal_model_path = get_model_path(CONFIG_FILE, 'causal')\n",
    "class_model_path = get_model_path(CONFIG_FILE, 'class')\n",
    "\n",
    "causal_model = load_model(causal_model_path, 'causal')\n",
    "class_model = load_model(class_model_path, 'class')\n",
    "\n",
    "causal_tokenizer = load_tokenizer(cfgvars['CAUSAL_MODEL'])\n",
    "class_tokenizer = load_tokenizer(cfgvars['CLASS_MODEL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7aa83-080c-4bb0-b0eb-126b130e8843",
   "metadata": {},
   "source": [
    "### Load and Prep Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a012078-6165-489b-8e50-93ed250ad83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aromas include tropical fruit, broom, brimstone and dried herb.', \"The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.\"]\n",
      "['From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decease, His tender heir might bear his memory:', 'But thou, contracted to thine own bright eyes, Feed’st thy light’s flame with self-substantial fuel, Making a famine where abundance lies, Thyself thy foe, to thy sweet self too cruel:']\n",
      "['Lift up your hearts in Gumber, laugh the Weald And you my mother the Valley of Arun sing.', 'Here am I homeward from my wandering Here am I homeward and my heart is healed.']\n"
     ]
    }
   ],
   "source": [
    "# get some test data to fuel generator\n",
    "test_data = dict()\n",
    "\n",
    "wines_test_data_path = os.path.join(cfgvars['DATA_DIR'], 'winemag-data-130k-v2.csv')\n",
    "wines_test_data = pd.read_csv(wines_test_data_path)\n",
    "wines_test_data = ' '.join(list(wines_test_data['description']))\n",
    "wines_test_data = extract_sentences(wines_test_data)\n",
    "test_data['wines'] = wines_test_data\n",
    "\n",
    "s, o = load_text_from_config(cfgvars)\n",
    "test_data['shakespeare'] = extract_sentences(s)\n",
    "test_data['other'] = extract_sentences(o)\n",
    "\n",
    "for v in test_data.values():\n",
    "    print(v[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f748db32-28d0-4b19-9a5d-59f0dfe7d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On the palate, this is light and', 'Aromas of toasted oak, green leaves,']\n",
      "['Swear me, Kate, like a lady as thou art, A', 'O, what a']\n",
      "['If the worlds age, and death be argued well By the Sunnes fall, which', \"drive deep the blow, Jomsburg's sons shall not complain, Never\"]\n"
     ]
    }
   ],
   "source": [
    "# FORMAT TEST DATA\n",
    "\n",
    "# fragment ratio for prompt selection\n",
    "FRAG_RAT = 0.4\n",
    "\n",
    "# sample size\n",
    "SAMPLES = 100\n",
    "\n",
    "# function to extract fragment from sentence\n",
    "def get_frag(text, rat=0.2):\n",
    "    words = text.split()\n",
    "    nwords = len(words)\n",
    "    if not rat:\n",
    "        nout = nwords\n",
    "    elif rat < 1:\n",
    "        nout = int(nwords * rat) or 1\n",
    "    else:\n",
    "        if rat > nwords: rat = nwords\n",
    "        nout = rat\n",
    "        \n",
    "    return ' '.join(words[:nout])\n",
    "\n",
    "def get_samples(data, samples = 10):\n",
    "    sampled = random.sample(data, samples)\n",
    "    return [get_frag(sentence, FRAG_RAT) for sentence in sampled]\n",
    "\n",
    "# create samples\n",
    "samples = {k: get_samples(v, SAMPLES) for k, v in test_data.items()}\n",
    "\n",
    "for v in samples.values():\n",
    "    print(v[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d40cb-ee94-4247-8ccd-356e82acbd7c",
   "metadata": {},
   "source": [
    "### Generate Text from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796e21c2-c467-4ad5-891b-95817e708ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*--- wines ---*\n",
      "Input: On the palate, this is light and\n",
      "> Output: On the palate, this is light and heavy.\n",
      "Input: Aromas of toasted oak, green leaves,\n",
      "> Output: Aromas of toasted oak, green leaves, and chaste palfrey, Hanging like a cannonard on the mare’s side, Spilt with every drop th’ air that may run Under the rosemary tree.\n",
      "Input: Its charred, heavily oaked flavors finish chocolaty, with\n",
      "> Output: Its charred, heavily oaked flavors finish chocolaty, with golden white smoke.\n",
      "Input: Inky in the glass and thick on the nose with deep blueberry and\n",
      "> Output: Inky in the glass and thick on the nose with deep blueberry and apricure, The more sweet than sweet.\n",
      "Input: crisp apples,\n",
      "> Output: crisp apples, and do they eat the apple?\n",
      "Input: Lifted citrus- and\n",
      "> Output: Lifted citrus- and grape-root, With all his labors of love.\n",
      "Input: It's a\n",
      "> Output: It's a sign, but it is not verified.\n",
      "Input: Drink\n",
      "> Output: Drink, you rogue.\n",
      "Input: The palate follows suit offering\n",
      "> Output: The palate follows suit offering as many a drink.\n",
      "Input: Aromas of toasted almond, white\n",
      "> Output: Aromas of toasted almond, white beard;\n",
      "11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, \n",
      "\n",
      "\n",
      "*--- shakespeare ---*\n",
      "Input: Swear me, Kate, like a lady as thou art, A\n",
      "> Output: Swear me, Kate, like a lady as thou art, A creature that’s never touched with sorrow.\n",
      "Input: O, what a\n",
      "> Output: O, what a thing it is!\n",
      "Input: O, these naughty times\n",
      "> Output: O, these naughty times have we left them!\n",
      "Input: I love to hear her speak, yet\n",
      "> Output: I love to hear her speak, yet here is none more;\n",
      "Input: In God’s name and the King’s, say who thou art And\n",
      "> Output: In God’s name and the King’s, say who thou art And what a goodly deed ‘God make thee to act.\n",
      "Input: A dog of that\n",
      "> Output: A dog of that good blood shall outrun the time.\n",
      "Input: A man in hue all ‘hues’\n",
      "> Output: A man in hue all ‘hues’ from heaven and Earth;\n",
      "Input: Away with\n",
      "> Output: Away with ’tis!\n",
      "Input: They will say\n",
      "> Output: They will say good, I prithee.\n",
      "Input: You cannot better be employed,\n",
      "> Output: You cannot better be employed, Since your faults have left you here;\n",
      "11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, \n",
      "\n",
      "\n",
      "*--- other ---*\n",
      "Input: If the worlds age, and death be argued well By the Sunnes fall, which\n",
      "> Output: If the worlds age, and death be argued well By the Sunnes fall, which lives but itself But that it is no danger to die.\n",
      "Input: drive deep the blow, Jomsburg's sons shall not complain, Never\n",
      "> Output: drive deep the blow, Jomsburg's sons shall not complain, Never worse than so loud an English crow.\n",
      "Input: Who dares\n",
      "> Output: Who dares not deny a crown The honor of his son Is that he honors th’ other.\n",
      "Input: fear\n",
      "> Output: fear thyself thou fight’st.\n",
      "Input: Why?\n",
      "> Output: Why?\n",
      "Input: And Opportunity, that empty\n",
      "> Output: And Opportunity, that empty in one’s honor;\n",
      "Input: And yet my\n",
      "> Output: And yet my name is Juliet, And I am as you seem:\n",
      "Input: In\n",
      "> Output: In time and in life shall I behold These unspotted enemies grow.\n",
      "Input: Then weep not, Thor, thy friend's approaching\n",
      "> Output: Then weep not, Thor, thy friend's approaching sorrow.\n",
      "Input: Well, Wisedome's of her children\n",
      "> Output: Well, Wisedome's of her children.\n",
      "11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GENERATE TEXT\n",
    "\n",
    "# maximum lines of ouput per category\n",
    "MAX_OUTPUT_LINES = 10\n",
    "\n",
    "# generate and store\n",
    "generated = {k: list() for k in samples.keys()}\n",
    "for k,v in samples.items():\n",
    "    count = 0\n",
    "    print(f'\\n*--- {k} ---*')\n",
    "    for line in v:\n",
    "        count += 1\n",
    "        if count <= MAX_OUTPUT_LINES: print(f'Input: {line}')\n",
    "        gs = extract_sentences(generate_from(line, causal_model, causal_tokenizer))[0]\n",
    "        if count <= MAX_OUTPUT_LINES: print(f'> Output: {gs}')\n",
    "        if count > MAX_OUTPUT_LINES: print(f'{count},', end=' ')\n",
    "        generated[k].append(gs)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcf2ab-0fec-442d-a5c9-38102c4f4a15",
   "metadata": {},
   "source": [
    "# Classify and Score Generated Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f64f228-3868-425b-8ad0-707e6d29cb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*--- wines ---*\n",
      "Shakespearean: 68.0%\n",
      "Mean score: 0.6242347487248481\n",
      "\n",
      "*--- shakespeare ---*\n",
      "Shakespearean: 96.0%\n",
      "Mean score: 0.8883342948555947\n",
      "\n",
      "*--- other ---*\n",
      "Shakespearean: 49.0%\n",
      "Mean score: 0.4896636430453509\n"
     ]
    }
   ],
   "source": [
    "# CLASSIFY\n",
    "for k, v in generated.items():\n",
    "    class_tokenizer.eos_token = class_tokenizer.unk_token\n",
    "    class_tokenizer.pad_token = class_tokenizer.eos_token\n",
    "    results = classify_from(v, class_model, class_tokenizer)\n",
    "    shakespearean_ratio = sum(results.c) / len(results.c)\n",
    "    score_mean = sum(results.s) / len(results.s)\n",
    "    \n",
    "    print(f'\\n*--- {k} ---*')\n",
    "    print(f'Shakespearean: {shakespearean_ratio * 100}%')\n",
    "    print(f'Mean score: {score_mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc535e10-e995-475a-b6cc-a081c1a2281a",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Maybe it's to be expected, but Shakespearean input generates the most Shakespearean output, as evidenced by the higher positive classifications and mean score for the Shakespeare input text. Overall, all generated text is on average more than 50% Shakespearean.\n",
    "\n",
    "Future tests could try different sized sentence fragments, to see if the causal model generate more Shakespearean text with more or fewer words in the input prompt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
