{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6223273-a26f-4b57-9b63-fffcc8461074",
   "metadata": {},
   "source": [
    "# GA Capstone\n",
    "## Causal Model Evaluation\n",
    "\n",
    "The goal here is to evaluate the causal model based on the classification model. To do so, I will generate text, reduced to the first sentence of each generated output, and run each sentence through the classification model. The goal is to have the most possible Shakespearean results (as the causal model is supposed to generate Shakespearean text). Score will be the percentage of Shakespearean results out of total results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c794c7-7577-408a-a480-2227f12bbe4c",
   "metadata": {},
   "source": [
    "### Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ce7c45-baa8-4047-9887-81873e800afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# models\n",
    "from transformers import TFAutoModelForCausalLM, TFAutoModelForSequenceClassification\n",
    "\n",
    "# custom utilities\n",
    "from utilities.utilities import load_config, get_model_path, load_model, load_tokenizer\n",
    "from utilities.utilities import load_text_from_config\n",
    "from utilities.utilities import generate_from\n",
    "from utilities.utilities import classify_from\n",
    "from utilities.utilities import extract_sentences\n",
    "\n",
    "# pandas for csv read and extract\n",
    "import pandas as pd\n",
    "\n",
    "# other\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8ead63d-0de7-4739-b641-9fbc670a41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model config file\n",
    "CONFIG_FILE = 'config.json'\n",
    "cfgvars = load_config(CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074b343-b97b-4cfd-b71c-9876d9cd4b39",
   "metadata": {},
   "source": [
    "### Load Models and Model Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43df2a50-16e8-4757-b833-d49bc5273bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ../models/shakespeare.distilgpt2.8.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Some layers from the model checkpoint at ../models/shakespeare.distilbert-base-uncased.2 were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at ../models/shakespeare.distilbert-base-uncased.2 and are newly initialized: ['dropout_38']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using eos_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "# get model locations, load models, and load tokenizers\n",
    "causal_model_path = get_model_path(CONFIG_FILE, 'causal')\n",
    "class_model_path = get_model_path(CONFIG_FILE, 'class')\n",
    "\n",
    "causal_model = load_model(causal_model_path, 'causal')\n",
    "class_model = load_model(class_model_path, 'class')\n",
    "\n",
    "causal_tokenizer = load_tokenizer(cfgvars['CAUSAL_MODEL'])\n",
    "class_tokenizer = load_tokenizer(cfgvars['CLASS_MODEL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f7aa83-080c-4bb0-b0eb-126b130e8843",
   "metadata": {},
   "source": [
    "### Load and Prep Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a012078-6165-489b-8e50-93ed250ad83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Aromas include tropical fruit, broom, brimstone and dried herb.', \"The palate isn't overly expressive, offering unripened apple, citrus and dried sage alongside brisk acidity.\"]\n",
      "['From fairest creatures we desire increase, That thereby beauty’s rose might never die, But as the riper should by time decease, His tender heir might bear his memory:', 'But thou, contracted to thine own bright eyes, Feed’st thy light’s flame with self-substantial fuel, Making a famine where abundance lies, Thyself thy foe, to thy sweet self too cruel:']\n",
      "['Lift up your hearts in Gumber, laugh the Weald And you my mother the Valley of Arun sing.', 'Here am I homeward from my wandering Here am I homeward and my heart is healed.']\n"
     ]
    }
   ],
   "source": [
    "# get some test data to fuel generator\n",
    "test_data = dict()\n",
    "\n",
    "wines_test_data_path = os.path.join(cfgvars['DATA_DIR'], 'winemag-data-130k-v2.csv')\n",
    "wines_test_data = pd.read_csv(wines_test_data_path)\n",
    "wines_test_data = ' '.join(list(wines_test_data['description']))\n",
    "wines_test_data = extract_sentences(wines_test_data)\n",
    "test_data['wines'] = wines_test_data\n",
    "\n",
    "s, o = load_text_from_config(cfgvars)\n",
    "test_data['shakespeare'] = extract_sentences(s)\n",
    "test_data['other'] = extract_sentences(o)\n",
    "\n",
    "for v in test_data.values():\n",
    "    print(v[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f748db32-28d0-4b19-9a5d-59f0dfe7d17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"At four years old, it's still young, its\", 'Drink']\n",
      "['And yet not so,', 'Stay, Humphrey,']\n",
      "['It gave them both', 'Thou shalt, when he hath runne the']\n"
     ]
    }
   ],
   "source": [
    "# FORMAT TEST DATA\n",
    "\n",
    "# fragment ratio for prompt selection\n",
    "FRAG_RAT = 0.4\n",
    "\n",
    "# sample size\n",
    "SAMPLES = 100\n",
    "\n",
    "# function to extract fragment from sentence\n",
    "def get_frag(text, rat=0.2):\n",
    "    words = text.split()\n",
    "    nwords = len(words)\n",
    "    if not rat:\n",
    "        nout = nwords\n",
    "    elif rat < 1:\n",
    "        nout = int(nwords * rat) or 1\n",
    "    else:\n",
    "        if rat > nwords: rat = nwords\n",
    "        nout = rat\n",
    "        \n",
    "    return ' '.join(words[:nout])\n",
    "\n",
    "def get_samples(data, samples = 10):\n",
    "    sampled = random.sample(data, samples)\n",
    "    return [get_frag(sentence, FRAG_RAT) for sentence in sampled]\n",
    "\n",
    "# create samples\n",
    "samples = {k: get_samples(v, SAMPLES) for k, v in test_data.items()}\n",
    "\n",
    "for v in samples.values():\n",
    "    print(v[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d40cb-ee94-4247-8ccd-356e82acbd7c",
   "metadata": {},
   "source": [
    "### Generate Text from Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "796e21c2-c467-4ad5-891b-95817e708ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*--- wines ---*\n",
      "Input: At four years old, it's still young, its\n",
      "> Output: At four years old, it's still young, its very full.\n",
      "Input: Drink\n",
      "> Output: Drink, and make haste.\n",
      "Input: On the velvety palate, firm, polished tannins\n",
      "> Output: On the velvety palate, firm, polished tannins.\n",
      "Input: This is open and and very\n",
      "> Output: This is open and and very little in my life.\n",
      "Input: Drink\n",
      "> Output: Drink but a drink and bring the lads out.\n",
      "Input: It's an opulent, delicious wine, immediately likeable now\n",
      "> Output: It's an opulent, delicious wine, immediately likeable now than a year in Italy.\n",
      "Input: Would pair with\n",
      "> Output: Would pair with her so to say I was a monster?\n",
      "Input: Oak provides fatness, and the right\n",
      "> Output: Oak provides fatness, and the right wing makes it.\n",
      "Input: drink\n",
      "> Output: drink and eat.\n",
      "Input: A hand-stitched, red vinyl body suit, tailored to the bottle’s seductive\n",
      "> Output: A hand-stitched, red vinyl body suit, tailored to the bottle’s seductive breath?\n",
      "11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, \n",
      "\n",
      "\n",
      "*--- shakespeare ---*\n",
      "Input: And yet not so,\n",
      "> Output: And yet not so, but she is well;\n",
      "Input: Stay, Humphrey,\n",
      "> Output: Stay, Humphrey, and to you I am made captain.\n",
      "Input: Ay, so\n",
      "> Output: Ay, so I take my pains.\n",
      "Input: Good\n",
      "> Output: Good morrow, good my lord.\n",
      "Input: Thou whoreson mandrake, thou art fitter to\n",
      "> Output: Thou whoreson mandrake, thou art fitter to have a tongue than my teeth.\n",
      "Input: The King hath on him such a countenance\n",
      "> Output: The King hath on him such a countenance as would prove to be a good housewife at once in the place.\n",
      "Input: For, sure, my thoughts do\n",
      "> Output: For, sure, my thoughts do lie there.\n",
      "Input: No.\n",
      "> Output: No.\n",
      "Input: I have, and I will\n",
      "> Output: I have, and I will.\n",
      "Input: Wash they\n",
      "> Output: Wash they not, I pray you?\n",
      "11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, \n",
      "\n",
      "\n",
      "*--- other ---*\n",
      "Input: It gave them both\n",
      "> Output: It gave them both the joy of my life And in my thoughts, which must be given you.\n",
      "Input: Thou shalt, when he hath runne the\n",
      "> Output: Thou shalt, when he hath runne the ground?\n",
      "Input: Show her sweet face,\n",
      "> Output: Show her sweet face, and set it so you cannot hide the mouth of fear.\n",
      "Input: leave me but This unprovided body for\n",
      "> Output: leave me but This unprovided body for the crown.\n",
      "Input: Such were these\n",
      "> Output: Such were these to be, all those that have eyes To tell good tidings at his back.\n",
      "Input: The\n",
      "> Output: The old guard, when I am alive, may outlive And never tire his heart.\n",
      "Input: So, I\n",
      "> Output: So, I warrant you:\n",
      "Input: That All, which alwayes is All every where, Which cannot sinne, and yet all\n",
      "> Output: That All, which alwayes is All every where, Which cannot sinne, and yet all be good, Will die ere we do repent.\n",
      "Input: The little laugh of water falling down Is not so musical, the clammy gold Close hoarded in the\n",
      "> Output: The little laugh of water falling down Is not so musical, the clammy gold Close hoarded in the air Like a huge flood.\n",
      "Input: and an\n",
      "> Output: and an if ’t come down into my mouth, and I am very much drunk.\n",
      "11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GENERATE TEXT\n",
    "\n",
    "# maximum lines of ouput per category\n",
    "MAX_OUTPUT_LINES = 10\n",
    "\n",
    "# generate and store\n",
    "generated = {k: list() for k in samples.keys()}\n",
    "for k,v in samples.items():\n",
    "    count = 0\n",
    "    print(f'\\n*--- {k} ---*')\n",
    "    for line in v:\n",
    "        count += 1\n",
    "        if count <= MAX_OUTPUT_LINES: print(f'Input: {line}')\n",
    "        gs = extract_sentences(generate_from(line, causal_model, causal_tokenizer))[0]\n",
    "        if count <= MAX_OUTPUT_LINES: print(f'> Output: {gs}')\n",
    "        if count > MAX_OUTPUT_LINES: print(f'{count},', end=' ')\n",
    "        generated[k].append(gs)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcf2ab-0fec-442d-a5c9-38102c4f4a15",
   "metadata": {},
   "source": [
    "# Classify and Score Generated Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f64f228-3868-425b-8ad0-707e6d29cb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*--- wines ---*\n",
      "Shakespearean: 63.0%\n",
      "Mean score: 0.5876119114458561\n",
      "\n",
      "*--- shakespeare ---*\n",
      "Shakespearean: 99.0%\n",
      "Mean score: 0.8906413942575455\n",
      "\n",
      "*--- other ---*\n",
      "Shakespearean: 61.0%\n",
      "Mean score: 0.5830802789190784\n"
     ]
    }
   ],
   "source": [
    "# CLASSIFY\n",
    "for k, v in generated.items():\n",
    "    class_tokenizer.eos_token = class_tokenizer.unk_token\n",
    "    class_tokenizer.pad_token = class_tokenizer.eos_token\n",
    "    results = classify_from(v, class_model, class_tokenizer)\n",
    "    shakespearean_ratio = sum(results.c) / len(results.c)\n",
    "    score_mean = sum(results.s) / len(results.s)\n",
    "    \n",
    "    print(f'\\n*--- {k} ---*')\n",
    "    print(f'Shakespearean: {shakespearean_ratio * 100}%')\n",
    "    print(f'Mean score: {score_mean}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc535e10-e995-475a-b6cc-a081c1a2281a",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Maybe it's to be expected, but Shakespearean input generates the most Shakespearean output, as evidenced by the higher positive classifications and mean score for the Shakespeare input text. Overall, all generated text is on average more than 50% Shakespearean.\n",
    "\n",
    "Future tests could try different sized sentence fragments, to see if the causal model generate more Shakespearean text with more or fewer words in the input prompt."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
