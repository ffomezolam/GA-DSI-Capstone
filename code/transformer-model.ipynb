{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e247aac4-af16-4b6e-be7c-8a32403263d8",
   "metadata": {},
   "source": [
    "# GA Capstone\n",
    "## Transformer Modeling with GPT2\n",
    "\n",
    "Creating a model to generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2444a-4ed5-4266-b23f-6e4cafcfd25d",
   "metadata": {},
   "source": [
    "A lot of the below is adapted from the gpt2 tutorial at https://huggingface.co/docs/transformers/v4.22.2/en/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ebac-93be-488f-b5b8-3fc324c36fe7",
   "metadata": {},
   "source": [
    "### Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c7b112-6339-422c-8b13-6beae6a1ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting for model\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# lm collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# model and support\n",
    "from transformers import TFAutoModelForCausalLM, create_optimizer, AdamWeightDecay\n",
    "\n",
    "# other utilities\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40b0184-61c8-4b8c-9be3-bca0f00cac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model we are using\n",
    "MODELS = [\n",
    "    'gpt', # original GPT\n",
    "    'distilgpt2', # 84M features\n",
    "    'gpt2', # 117M features\n",
    "    'gpt2-medium', # 355M features\n",
    "    'gpt2-large', # 744M features\n",
    "    'ctrl',\n",
    "    'transformerxl',\n",
    "    'reformer',\n",
    "    'xlnet'\n",
    "]\n",
    "    \n",
    "model_type = 'distilgpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3af3b72-26ba-46b1-9e03-40a695527906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "MODEL_NAME = 'shakespeare'\n",
    "N_EPOCHS = 20\n",
    "DIR_MODEL = '../models/'\n",
    "DIR_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f8435d-28be-4c30-868d-0e5b7e1a3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regexes\n",
    "RE_SENTENCE = re.compile(r'\\w.*?[.?!:;]', re.S)\n",
    "RE_WHITESPACE = re.compile(r'\\s+')\n",
    "RE_BLANKLINE = re.compile(r'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959970d-dfcd-4339-bdac-9877ac3f3607",
   "metadata": {},
   "source": [
    "### Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f247c9d-a576-4d1a-83a7-508ce3252b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "paths = [\n",
    "    os.path.join(DIR_DATA, 'shakespeare-sonnets.clean.txt'),\n",
    "    os.path.join(DIR_DATA, 'shakespeareplays.txt')\n",
    "]\n",
    "\n",
    "text = list()\n",
    "\n",
    "for path in paths:\n",
    "    with open(path, 'r') as f:\n",
    "        text.append([line.strip() for line in f.readlines() if line.strip()])\n",
    "\n",
    "text = ' '.join(chain(*text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4235a565-342d-4972-9ab0-f7672de305ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Sentences: 76578\n"
     ]
    }
   ],
   "source": [
    "# Split into relevant subsets\n",
    "# POEMS\n",
    "#poems = RE_BLANKLINE.split(text)\n",
    "\n",
    "# LINES\n",
    "#lines = [line.strip() for line in text.split('\\n')]\n",
    "\n",
    "# SENTENCES\n",
    "sentences = RE_SENTENCE.findall(text)\n",
    "sentences = [RE_WHITESPACE.sub(' ', sentence) for sentence in sentences]\n",
    "\n",
    "print(f'# Sentences: {len(sentences)}')\n",
    "#print(f'# Poems: {len(poems)}\\n# Sentences: {len(sentences)}\\n# Lines: {len(lines)}\\n# Chars: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e49f659d-ca55-4ab6-ad50-15e178ab0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test code - small sample of lines\n",
    "nlines = 2500\n",
    "sentences = sentences[500:500 + nlines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c01b72ce-07e3-4575-8b73-750b339c6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2375, 125)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test\n",
    "lines_train, lines_test = train_test_split(sentences, test_size=0.05, shuffle=False)\n",
    "len(lines_train), len(lines_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437b48-0310-4f4e-ba9f-fc222c3f84e4",
   "metadata": {},
   "source": [
    "### Cleaning and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69bc6d46-e09d-4927-ae28-d28e8bdaa767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2375\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset and DatasetDict instances - I think this is needed for model\n",
    "train_dataset = Dataset.from_dict({'text': lines_train})\n",
    "test_dataset = Dataset.from_dict({'text': lines_test})\n",
    "datasets = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3853c6fe-9d1c-43fb-b91b-7c019b812597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54bbcd4-81ee-42c6-83d3-428829fa00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for tokenizer to use with map() method of datasetdict\n",
    "def token_preproc(data):\n",
    "    return tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f538e0b6-4460-4ba0-94a9-00d3001aa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eabc2aec4f6540f7bffb503a271dae7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74c4620a2af4f06b84ccd67bf5ec911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c0a9014c7049bcafed9ee89b50294e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908d6cb4b9bf404d90f8336cd4a74c37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb6453d20fe4d638c7710d352e64c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45ea54332d74d26b7dcde072e22a575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "357c12e49fda4043b1bb2b06abbd5e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e913b8ac1b9452ab11ce7e8a3a1406d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 2375\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokened_data = datasets.map(token_preproc, batched=True, num_proc=4, remove_columns=['text'])\n",
    "tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c7c5c8-c106-4913-bebc-806d69deb88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973 ['And', 'ĠI', 'Ġwill', 'Ġdo', 'Ġso', '.'] And I will do so.\n",
      "1419 ['Ay', ',', 'Ġsir', ',', 'Ġhe', ',', 'Ġsir', ',', 'ĠâĢ', 'Ļ', 's', 'Ġa', 'Ġgood', 'Ġwork', 'man', ',', 'Ġa', 'Ġvery', 'Ġgood', 'Ġtailor', '.'] Ay, sir, he, sir, ’s a good workman, a very good tailor.\n",
      "961 ['It', 'Ġwas', 'Ġthis', 'Ġvery', 'Ġsword', 'Ġentrenched', 'Ġit', '.'] It was this very sword entrenched it.\n",
      "2157 ['Therefore', 'Ġyou', 'Ġmust', 'Ġdie', '.'] Therefore you must die.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(4):\n",
    "    n = random.randint(0, len(tokened_data['train']))\n",
    "    print(n, tokenizer.convert_ids_to_tokens(tokened_data['train'][n]['input_ids']), lines_train[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a746d71-3b09-40b8-95ae-b5f3a71d44d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='distilgpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='tf')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad encodings and prep for modeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors='tf')\n",
    "collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10ee63-a50a-42da-a49f-59f015e2814e",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f45202c2-393d-4e35-9ffd-fea2adf70329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at distilgpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "model_path = os.path.join(DIR_MODEL, f'{model_type}.{MODEL_NAME}.{str(N_EPOCHS)}')\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_type, pad_token_id = tokenizer.eos_token_id)\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_path)\n",
    "    \n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec648afe-a7b2-464e-b805-ca0ffce0e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(32, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(32, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(32, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to special format for tf model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tf_train_set = model.prepare_tf_dataset(tokened_data['train'], shuffle=True, batch_size=32, collate_fn=collator)\n",
    "tf_test_set = model.prepare_tf_dataset(tokened_data['test'], shuffle=False, batch_size=32, collate_fn=collator)\n",
    "tf_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6abd4074-b0d6-4058-b017-67d196514c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6d0663a-d3bc-4c8d-b713-bb892ec5dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "74/74 [==============================] - 241s 3s/step - loss: 5.2836 - val_loss: 5.0645\n",
      "Epoch 2/20\n",
      "74/74 [==============================] - 236s 3s/step - loss: 4.9199 - val_loss: 4.9189\n",
      "Epoch 3/20\n",
      "74/74 [==============================] - 238s 3s/step - loss: 4.7417 - val_loss: 4.8571\n",
      "Epoch 4/20\n",
      "74/74 [==============================] - 238s 3s/step - loss: 4.6070 - val_loss: 4.8218\n",
      "Epoch 5/20\n",
      "74/74 [==============================] - 233s 3s/step - loss: 4.4948 - val_loss: 4.7930\n",
      "Epoch 6/20\n",
      "74/74 [==============================] - 237s 3s/step - loss: 4.4116 - val_loss: 4.8031\n",
      "Epoch 7/20\n",
      "74/74 [==============================] - 240s 3s/step - loss: 4.3250 - val_loss: 4.7957\n",
      "Epoch 8/20\n",
      "74/74 [==============================] - 236s 3s/step - loss: 4.2389 - val_loss: 4.8097\n",
      "Epoch 9/20\n",
      "74/74 [==============================] - 236s 3s/step - loss: 4.1812 - val_loss: 4.8193\n",
      "Epoch 10/20\n",
      "74/74 [==============================] - 233s 3s/step - loss: 4.1071 - val_loss: 4.8322\n",
      "Epoch 11/20\n",
      "74/74 [==============================] - 237s 3s/step - loss: 4.0442 - val_loss: 4.8575\n",
      "Epoch 12/20\n",
      "74/74 [==============================] - 241s 3s/step - loss: 3.9736 - val_loss: 4.8773\n",
      "Epoch 13/20\n",
      "74/74 [==============================] - 236s 3s/step - loss: 3.9034 - val_loss: 4.9007\n",
      "Epoch 14/20\n",
      "74/74 [==============================] - 240s 3s/step - loss: 3.8393 - val_loss: 4.9441\n",
      "Epoch 15/20\n",
      "74/74 [==============================] - 232s 3s/step - loss: 3.7730 - val_loss: 4.9679\n",
      "Epoch 16/20\n",
      "74/74 [==============================] - 228s 3s/step - loss: 3.7121 - val_loss: 4.9819\n",
      "Epoch 17/20\n",
      "74/74 [==============================] - 233s 3s/step - loss: 3.6359 - val_loss: 5.0295\n",
      "Epoch 18/20\n",
      "74/74 [==============================] - 231s 3s/step - loss: 3.5853 - val_loss: 5.0500\n",
      "Epoch 19/20\n",
      "74/74 [==============================] - 235s 3s/step - loss: 3.5219 - val_loss: 5.1114\n",
      "Epoch 20/20\n",
      "74/74 [==============================] - 235s 3s/step - loss: 3.4515 - val_loss: 5.1460\n"
     ]
    }
   ],
   "source": [
    "# fit model (if pretrained does not exist)\n",
    "if not os.path.exists(model_path):\n",
    "    model.fit(tf_train_set, validation_data=tf_test_set, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b5ee69e-b6b2-4f6d-822d-e42a0a75eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9205b01-beaf-4603-9836-01cfcefe172e",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7018e21-169f-4268-bf96-20b11eae6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get predicted text\n",
    "def test(text, max_new=50, temp=1, top_k=50, rep_penalty=1.5, len_penalty=0.75, n_seq=1):\n",
    "    tokened = tokenizer(text, return_tensors='tf')\n",
    "    output = model.generate(**tokened,\n",
    "                            do_sample=True,\n",
    "                            max_new_tokens=max_new, \n",
    "                            temperature=temp, \n",
    "                            top_k=top_k, \n",
    "                            repetition_penalty=rep_penalty,\n",
    "                            length_penalty=len_penalty,\n",
    "                            num_return_sequences=n_seq)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc98e32b-111c-4dc0-843c-252cab0de139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Fair, kind, and true,’ is all\n",
      "Output: Fair, kind, and true,’ is all the King does. And yet it shall be no more than a pastime for his lordship to come here tonight: ‘I will not steal my riches nor give them away; but I am free now To do what you please! But in your hand are they bound? The sword from me comes when thou dost hold thee captive— That which so holds thy heart When thine own tongue flies out of balance with mine eye For that whose motion makes love fair as if she were both proud\n",
      "\n",
      "Original: You’ll be gone, sir\n",
      "Output: You’ll be gone, sir. But no more for your sake: I shall stay here where you live till death is found! The rest of the world will die if they do not repent Of their sinning and virtue-saving deeds which were done Before God himself; when nature approves them with a single hand And in this hope to show mercy upon him For goodness does well help but fails To give it grace by all that doth make It possible without constraint That heaven knows what may come first Upon us who have abandoned our hopes\n",
      "\n",
      "Original: Make\n",
      "Output: Make that ring. That rose, my lord? Who’s it! And the other jewel of thy love: The most precious to me; and a crown Of all worth— A wondrous gift I will ever give again To so many more worthy heads Than mine own would have but now gone Unthankful for them still being there For whom they were once But forgotten forever in our world As we know how their quirks are fixed Today In beauty's correction where none could say We never saw ourselves see\n",
      "\n",
      "Original: is very\n",
      "Output: is very well, and I hope not to be too late for him. But he will come here at once; after all his lordship’s death my best wishes are now gone: For that which is so dear To the past day it still doth lie The present hour when our most holy days have ended A time of great mourning where we do beseech Heaven save us from ourselves forever! And yet there comes a single man left who knows what this hell shall happen next—and every one\n",
      "\n",
      "Original: boys are\n",
      "Output: boys are not to be blamed for their sinful behaviour. But rather they were due, and deserved it straight: To make a man sin is like making an egg of your own stomach; For the guilty act will never have any effect except that by being whipped—they’re all but worthless if you eat them up! And look no further than this at home when those in heaven serve thee well? When men themselves do fail so with wrongs then blame comes first upon itself as disgrace forever afterwards.— So\n",
      "\n",
      "Original: You have\n",
      "Output: You have undone the great honor of a nobleman, and now you must do so. And I think it is your duty to be here tonight that will bring him home for debate: To make sure he has some business in his absence; or else thereabouts go too far with us! We shall take heed Of all our warnings which we hear from France Which may well not come true But then such caution does dissuade thee From speaking much more confidently about what might happen When no one knows how many deaths are\n",
      "\n",
      "Original: I would not tell\n",
      "Output: I would not tell you the truth, but I know it was a lie. But that my life in your favor now is to die of so: As soon as thy sweet love doth thirst; My dear lord will suffer forever— And yet thou art mine! The first and greatest fire which burns thee out shall burn away like an old flame That’s still hotter than black heaven does Today when all hell hath set down All those whose praises have flown To me from afar With their gentle showers Which they kiss at\n",
      "\n",
      "Original: Well, I\n",
      "Output: Well, I’ll give you a letter to your Majesty. As soon as we hear of it and have no further comment yet on the matter until then! And what is my pleasure? What sovereign will do me in this case till he knows that his son-in law shall be whipped for not paying him some respect when they begin their business To make sure our young lordship pays them all such respects When none but one maid comes with us or gives her any honor Of course she may take advantage if there\n",
      "\n",
      "Original: In things of great receipt with\n",
      "Output: In things of great receipt with ’t, I will not have to say much. But it is a good convenience that receives the credit in one thing: Since you know what we are doing now—see this very hand! That which stands still when our action becomes too full for all but your finger; and so does my heart whereof mine eyes doth see nothing yet? And then there comes an instant place wherein no other man can bring himself out Of ever seeing anything except his own eye or sight nor hear any word\n",
      "\n",
      "Original: A maid of Dian’s this advantage found, And\n",
      "Output: A maid of Dian’s this advantage found, And in the bath did I have pleasure; and my delight grew so great that it hath worn out. But then there were no more salutation for me Than one to say: ‘No! That was a disgrace indeed—that you must be ashamed Of your own pride which is not worthy To deserve praise from others? For what honor does yours owe her esteem Which owes herself as well As honors whose dignity now lies at stake In mine eyes where she loses sight Yet touches his beauty with such an ease\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lines = [sentences[random.randint(0,len(sentences) - 1)] for _ in range(10)]\n",
    "fragment_ratio = 0.4\n",
    "\n",
    "for line in test_lines:\n",
    "    words = line.split()\n",
    "    stop_ix = int(len(words) * fragment_ratio) or 1\n",
    "    fragment = ' '.join(words[0:stop_ix])\n",
    "    print(f'Original: {fragment}')\n",
    "    output = test(fragment,\n",
    "                  temp=0.5,\n",
    "                  max_new=100,\n",
    "                  top_k=200,\n",
    "                  rep_penalty=1.5,\n",
    "                  len_penalty=0.75,\n",
    "                  n_seq=1)\n",
    "    print(f'Output: {output}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf742070-67be-4913-8246-0eacf1f8bc47",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Text is generated that at many times sounds reasonably Shakespearian. Soon to be combined with classification model to test this out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
