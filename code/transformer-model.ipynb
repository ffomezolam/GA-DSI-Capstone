{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e247aac4-af16-4b6e-be7c-8a32403263d8",
   "metadata": {},
   "source": [
    "# Tranformer Modeling with GPT2\n",
    "\n",
    "Creating a model based on various criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2444a-4ed5-4266-b23f-6e4cafcfd25d",
   "metadata": {},
   "source": [
    "A lot of the below is adapted from the gpt2 tutorial at https://huggingface.co/docs/transformers/v4.22.2/en/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ebac-93be-488f-b5b8-3fc324c36fe7",
   "metadata": {},
   "source": [
    "## Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c7b112-6339-422c-8b13-6beae6a1ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting for model\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# lm collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# model and support\n",
    "from transformers import TFAutoModelForCausalLM, create_optimizer, AdamWeightDecay\n",
    "\n",
    "# other utilities\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40b0184-61c8-4b8c-9be3-bca0f00cac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model we are using\n",
    "MODELS = [\n",
    "    'gpt', # original GPT\n",
    "    'distilgpt2', # 84M features\n",
    "    'gpt2', # 117M features\n",
    "    'gpt2-medium', # 355M features\n",
    "    'gpt2-large', # 744M features\n",
    "    'ctrl',\n",
    "    'transformerxl',\n",
    "    'reformer',\n",
    "    'xlnet'\n",
    "]\n",
    "    \n",
    "model_type = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3af3b72-26ba-46b1-9e03-40a695527906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "MODEL_NAME = 'multi'\n",
    "N_EPOCHS = 8\n",
    "DIR_MODEL = '../models/'\n",
    "DIR_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f8435d-28be-4c30-868d-0e5b7e1a3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regexes\n",
    "RE_SENTENCE = re.compile(r'\\w.*?[.?!:;]', re.S)\n",
    "RE_WHITESPACE = re.compile(r'\\s+')\n",
    "RE_BLANKLINE = re.compile(r'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aaf62d9-a97e-4944-a5aa-e1c8eee3f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other special constants\n",
    "EOLINE_TOKEN = '<|eol|>'\n",
    "EOSENTENCE_TOKEN = '<|eos|>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959970d-dfcd-4339-bdac-9877ac3f3607",
   "metadata": {},
   "source": [
    "## Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f247c9d-a576-4d1a-83a7-508ce3252b18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "paths = [\n",
    "    os.path.join(DIR_DATA, 'shakespeare-sonnets.clean.txt'),\n",
    "    os.path.join(DIR_DATA, 'browning-sonnets.clean.txt'),\n",
    "    os.path.join(DIR_DATA, 'daniel-constable-sonnets.clean.txt'),\n",
    "    os.path.join(DIR_DATA, 'drayton-griffin-smith-sonnet-cycles.clean.txt'),\n",
    "    os.path.join(DIR_DATA, 'farjeon-sonnets.clean.txt'),\n",
    "    os.path.join(DIR_DATA, 'lovell-southey-sonnets.clean.txt')\n",
    "]\n",
    "\n",
    "text = list()\n",
    "\n",
    "for path in paths:\n",
    "    with open(path, 'r') as f:\n",
    "        text.append([line.strip() for line in f.readlines()])\n",
    "\n",
    "text = '\\n'.join(chain(*text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4235a565-342d-4972-9ab0-f7672de305ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Poems: 585\n",
      "# Sentences: 3836\n",
      "# Lines: 8419\n",
      "# Chars: 338875\n"
     ]
    }
   ],
   "source": [
    "# Split into relevant subsets\n",
    "# POEMS\n",
    "poems = RE_BLANKLINE.split(text)\n",
    "\n",
    "# LINES\n",
    "lines = [line.strip() for line in text.split('\\n')]\n",
    "\n",
    "# SENTENCES\n",
    "sentences = RE_SENTENCE.findall(text)\n",
    "sentences = [RE_WHITESPACE.sub(' ', sentence) for sentence in sentences]\n",
    "\n",
    "print(f'# Poems: {len(poems)}\\n# Sentences: {len(sentences)}\\n# Lines: {len(lines)}\\n# Chars: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c01b72ce-07e3-4575-8b73-750b339c6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3644, 192)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test\n",
    "lines_train, lines_test = train_test_split(sentences, test_size=0.05)\n",
    "len(lines_train), len(lines_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437b48-0310-4f4e-ba9f-fc222c3f84e4",
   "metadata": {},
   "source": [
    "## Cleaning and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69bc6d46-e09d-4927-ae28-d28e8bdaa767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3644\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 192\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset and DatasetDict instances - I think this is needed for model\n",
    "train_dataset = Dataset.from_dict({'text': lines_train})\n",
    "test_dataset = Dataset.from_dict({'text': lines_test})\n",
    "datasets = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3853c6fe-9d1c-43fb-b91b-7c019b812597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54bbcd4-81ee-42c6-83d3-428829fa00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for tokenizer to use with map() method of datasetdict\n",
    "def token_preproc(data):\n",
    "    return tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f538e0b6-4460-4ba0-94a9-00d3001aa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d44632f3c1d441508eb884a74c44d58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7884d7b2bc1848b096a831652d1b55c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa248c2a1ff643caac85f14e8102aa22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577e39eac5cd43278ca13aa4ca28446d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4deb179d1c9a4a3a85bb014b597c5c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e08ee43a6d45aea2bb8f9a69f1ceae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8c79da9ab4413b961a9420120dd6e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad73d0bef6ea43fdb59bd811c5c6670e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 3644\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 192\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokened_data = datasets.map(token_preproc, batched=True, num_proc=4, remove_columns=['text'])\n",
    "tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c7c5c8-c106-4913-bebc-806d69deb88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3478 ['And', 'Ġrather', 'Ġmake', 'Ġthem', 'Ġborn', 'Ġto', 'Ġour', 'Ġdesire', 'ĠThan', 'Ġthink', 'Ġthat', 'Ġwe', 'Ġbefore', 'Ġhave', 'Ġheard', 'Ġthem', 'Ġtold', '.'] And rather make them born to our desire Than think that we before have heard them told.\n",
      "1480 ['All', 'Ġthis', 'Ġmy', 'Ġheart', 'Ġfrom', 'Ġlove', 'Ġcan', 'Ġnever', 'Ġmove', '.'] All this my heart from love can never move.\n",
      "693 ['O', '!'] O!\n",
      "1482 ['Oh', ',', 'Ġyes', '!'] Oh, yes!\n"
     ]
    }
   ],
   "source": [
    "for _ in range(4):\n",
    "    n = random.randint(0, len(tokened_data['train']))\n",
    "    print(n, tokenizer.convert_ids_to_tokens(tokened_data['train'][n]['input_ids']), lines_train[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a746d71-3b09-40b8-95ae-b5f3a71d44d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='tf')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad encodings and prep for modeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors='tf')\n",
    "collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10ee63-a50a-42da-a49f-59f015e2814e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f45202c2-393d-4e35-9ffd-fea2adf70329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "model_path = os.path.join(DIR_MODEL, f'{model_type}.{MODEL_NAME}.{str(N_EPOCHS)}')\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_type, pad_token_id = tokenizer.eos_token_id)\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_path)\n",
    "    \n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec648afe-a7b2-464e-b805-ca0ffce0e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(32, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(32, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(32, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to special format for tf model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tf_train_set = model.prepare_tf_dataset(tokened_data['train'], shuffle=True, batch_size=32, collate_fn=collator)\n",
    "tf_test_set = model.prepare_tf_dataset(tokened_data['test'], shuffle=False, batch_size=32, collate_fn=collator)\n",
    "tf_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6abd4074-b0d6-4058-b017-67d196514c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6d0663a-d3bc-4c8d-b713-bb892ec5dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "113/113 [==============================] - 706s 6s/step - loss: 5.2926 - val_loss: 4.9951\n",
      "Epoch 2/8\n",
      "113/113 [==============================] - 721s 6s/step - loss: 4.9740 - val_loss: 4.9152\n",
      "Epoch 3/8\n",
      "113/113 [==============================] - 2001s 18s/step - loss: 4.8207 - val_loss: 4.8770\n",
      "Epoch 4/8\n",
      "113/113 [==============================] - 3080s 27s/step - loss: 4.7014 - val_loss: 4.8686\n",
      "Epoch 5/8\n",
      "113/113 [==============================] - 1649s 15s/step - loss: 4.6157 - val_loss: 4.8592\n",
      "Epoch 6/8\n",
      "113/113 [==============================] - 1245s 11s/step - loss: 4.5235 - val_loss: 4.8577\n",
      "Epoch 7/8\n",
      "113/113 [==============================] - 668s 6s/step - loss: 4.4450 - val_loss: 4.8667\n",
      "Epoch 8/8\n",
      "113/113 [==============================] - 629s 6s/step - loss: 4.3624 - val_loss: 4.8705\n"
     ]
    }
   ],
   "source": [
    "# fit model (if pretrained does not exist)\n",
    "if not os.path.exists(model_path):\n",
    "    model.fit(tf_train_set, validation_data=tf_test_set, epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b5ee69e-b6b2-4f6d-822d-e42a0a75eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9205b01-beaf-4603-9836-01cfcefe172e",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7018e21-169f-4268-bf96-20b11eae6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get predicted text\n",
    "def test(text, max_new=50, temp=1, top_k=50, rep_penalty=1.5, len_penalty=0.75, n_seq=1):\n",
    "    tokened = tokenizer(text, return_tensors='tf')\n",
    "    output = model.generate(**tokened,\n",
    "                            do_sample=True,\n",
    "                            max_new_tokens=max_new, \n",
    "                            temperature=temp, \n",
    "                            top_k=top_k, \n",
    "                            repetition_penalty=rep_penalty,\n",
    "                            length_penalty=len_penalty,\n",
    "                            num_return_sequences=n_seq)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc98e32b-111c-4dc0-843c-252cab0de139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Whilst we both make the world admire at us,\n",
      "Output: Whilst we both make the world admire at us, Which is of such worth as never was made before. For thy beauty thou must be remembered; for if it should remain so long In a dull state then grace would not suffice To show our beautys youth to maturity: And thus my verse doth tell The most perfect truth that ever I saw! So in this way shall you find your true love express—I call on thee now and speak unto me again When these two truths come together That no one knows better than myself do Know how they are\n",
      "\n",
      "Original: By granting me thy favour to obtain.\n",
      "Output: By granting me thy favour to obtain. Thou art my friend, and I am his guest; And when thou hast loved him so much the world can see My love in this way is not only made stronger by thee but more! When these two are reconciled they will be like unto one another: For it doth seem that truth gives true strength To prove what all men say about us? But how should we know if he was born of a lie or from some other torment Than as well she did before her eyes were cast forth In\n",
      "\n",
      "Original: Bewray unto the world how fair thou art;\n",
      "Output: Bewray unto the world how fair thou art; and what a wretch hath done of thee, Wherefore must I weep? And why dost he not pity me so much as when all his cares have been spent: For in thy favour do they dwell. So that my heart is but her breast,—and this she bewrays to bear My sorrows with disdain! Her beauty may never exceed those beautys which we call love In these three parts alone are wrought by you— That though your eyes appear white or pale They still remain bright\n",
      "\n",
      "Original: \n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "cannot compute ConcatV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:ConcatV2] name: concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m test_lines:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOriginal: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m test(line,\n\u001b[1;32m      6\u001b[0m                   temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m,\n\u001b[1;32m      7\u001b[0m                   max_new\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      8\u001b[0m                   top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      9\u001b[0m                   rep_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m,\n\u001b[1;32m     10\u001b[0m                   len_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m,\n\u001b[1;32m     11\u001b[0m                   n_seq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [20], line 4\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(text, max_new, temp, top_k, rep_penalty, len_penalty, n_seq)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest\u001b[39m(text, max_new\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, rep_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, len_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m, n_seq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      3\u001b[0m     tokened \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokened\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrep_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlen_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/transformers/lib/python3.10/site-packages/transformers/generation_tf_utils.py:583\u001b[0m, in \u001b[0;36mTFGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, max_new_tokens, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, output_scores, output_attentions, output_hidden_states, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, **model_kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_sample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_beams \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    582\u001b[0m     seed \u001b[38;5;241m=\u001b[39m model_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepetition_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbad_words_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbad_words_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlength_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_return_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_start_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforced_eos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforced_eos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;66;03m# We cannot generate if the model does not have a LM head\u001b[39;00m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_output_embeddings() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/transformers/lib/python3.10/site-packages/transformers/generation_tf_utils.py:1678\u001b[0m, in \u001b[0;36mTFGenerationMixin._generate\u001b[0;34m(self, input_ids, max_length, max_new_tokens, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, num_return_sequences, attention_mask, decoder_start_token_id, use_cache, seed, output_scores, output_attentions, output_hidden_states, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, **model_kwargs)\u001b[0m\n\u001b[1;32m   1670\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1671\u001b[0m         input_ids,\n\u001b[1;32m   1672\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mnum_return_sequences,\n\u001b[1;32m   1673\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1674\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1675\u001b[0m     )\n\u001b[1;32m   1677\u001b[0m     \u001b[38;5;66;03m# 11. run sample\u001b[39;00m\n\u001b[0;32m-> 1678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1682\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_beams \u001b[38;5;241m<\u001b[39m num_return_sequences:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/transformers/lib/python3.10/site-packages/transformers/generation_tf_utils.py:2462\u001b[0m, in \u001b[0;36mTFGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, logits_warper, max_length, pad_token_id, eos_token_id, seed, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   2460\u001b[0m \u001b[38;5;66;03m# initialize `generated` (pre-populated with `pad_token_id`), `finished_sequences`\u001b[39;00m\n\u001b[1;32m   2461\u001b[0m input_ids_padding \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mones((batch_size, max_length \u001b[38;5;241m-\u001b[39m cur_len), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32) \u001b[38;5;241m*\u001b[39m (pad_token_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 2462\u001b[0m generated \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids_padding\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2463\u001b[0m finished_sequences \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros((batch_size,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mbool)\n\u001b[1;32m   2465\u001b[0m \u001b[38;5;66;03m# 4. define \"xla-compile-able\" stop-condition and auto-regressive function\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/transformers/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.4/envs/transformers/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7208\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7209\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: cannot compute ConcatV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:ConcatV2] name: concat"
     ]
    }
   ],
   "source": [
    "test_lines = [lines[random.randint(0,len(lines) - 1)] for _ in range(10)]\n",
    "\n",
    "for line in test_lines:\n",
    "    print(f'Original: {line}')\n",
    "    output = test(line,\n",
    "                  temp=0.5,\n",
    "                  max_new=100,\n",
    "                  top_k=200,\n",
    "                  rep_penalty=1.5,\n",
    "                  len_penalty=0.75,\n",
    "                  n_seq=1)\n",
    "    print(f'Output: {output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b278a-3993-4ec1-a3fb-6e2fc33e768b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
