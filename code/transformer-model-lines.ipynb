{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e247aac4-af16-4b6e-be7c-8a32403263d8",
   "metadata": {},
   "source": [
    "# Modeling on Lines\n",
    "Creating a transformers model based on poem lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2444a-4ed5-4266-b23f-6e4cafcfd25d",
   "metadata": {},
   "source": [
    "A lot of the below is adapted from the gpt2 tutorial at https://huggingface.co/docs/transformers/v4.22.2/en/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ebac-93be-488f-b5b8-3fc324c36fe7",
   "metadata": {},
   "source": [
    "## Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c7b112-6339-422c-8b13-6beae6a1ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting for model\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# lm collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# model and support\n",
    "from transformers import TFAutoModelForCausalLM, create_optimizer, AdamWeightDecay\n",
    "\n",
    "# other utilities\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40b0184-61c8-4b8c-9be3-bca0f00cac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model we are using\n",
    "MODELS = [\n",
    "    'gpt', # original GPT\n",
    "    'distilgpt2', # 84M features\n",
    "    'gpt2', # 117M features\n",
    "    'gpt2-medium', # 355M features\n",
    "    'gpt2-large', # 744M features\n",
    "    'ctrl',\n",
    "    'transformerxl',\n",
    "    'reformer',\n",
    "    'xlnet'\n",
    "]\n",
    "    \n",
    "model_type = 'gpt2-medium'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3af3b72-26ba-46b1-9e03-40a695527906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "MODEL_FORMAT = 'lines'\n",
    "DIR_MODEL = '../models/'\n",
    "DIR_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959970d-dfcd-4339-bdac-9877ac3f3607",
   "metadata": {},
   "source": [
    "## Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1c7d8c-8aa3-4227-ab44-40d079392282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of lines: 2155\n",
      "First 5: ['From fairest creatures we desire increase,', 'That thereby beautys rose might never die,', 'But as the riper should by time decease,', 'His tender heir might bear his memory:', 'But thou, contracted to thine own bright eyes,']\n",
      "# of lines: 617\n",
      "First 5: ['\\ufeff', 'I thought once how Theocritus had sung', 'Of the sweet years, the dear and wished-for years,', 'Who each one in a gracious hand appears', 'To bear a gift for mortals, old or young:']\n",
      "# of lines: 2046\n",
      "First 5: ['\\ufeff    Wonder of these, glory of other times,', 'O thou whom envy evn is forced tadmire!', 'Great Patroness of these my humble rhymes,', 'Which thou from out thy greatness dost inspire!', 'Since only thou has deigned to raise them higher,']\n",
      "# of lines: 2524\n",
      "First 5: ['\\ufeff', 'Into these loves who but for passion looks,', 'At this first sight here let him lay them by,', 'And seek elsewhere in turning other books,', 'Which better may his labour satisfy.']\n",
      "TOTAL LINES: 7342\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "paths = [\n",
    "    os.path.join(DIR_DATA, 'shakespeare-sonnets.clean.txt'),\n",
    "    os.path.join(DIR_DATA, 'browning-sonnets.clean.txt'),\n",
    "    os.path.join(DIR_DATA, 'daniel-constable-sonnets.clean.txt'),\n",
    "    os.path.join(DIR_DATA, 'drayton-griffin-smith-sonnet-cycles.clean.txt')\n",
    "]\n",
    "                 \n",
    "data = []\n",
    "for path in paths:\n",
    "    with open(path, 'r') as f:\n",
    "        data.append([line.strip() for line in f.readlines() if line.strip()])\n",
    "        print(f'# of lines: {len(data[-1])}\\nFirst 5: {data[-1][:5]}')\n",
    "\n",
    "lines = list(chain(*data))\n",
    "print(f'TOTAL LINES: {len(lines)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c01b72ce-07e3-4575-8b73-750b339c6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6974, 368)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test\n",
    "lines_train, lines_test = train_test_split(lines, test_size=0.05)\n",
    "len(lines_train), len(lines_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437b48-0310-4f4e-ba9f-fc222c3f84e4",
   "metadata": {},
   "source": [
    "## Cleaning and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bc6d46-e09d-4927-ae28-d28e8bdaa767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6974\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 368\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset and DatasetDict instances - I think this is needed for model\n",
    "train_dataset = Dataset.from_dict({'text': lines_train})\n",
    "test_dataset = Dataset.from_dict({'text': lines_test})\n",
    "datasets = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3853c6fe-9d1c-43fb-b91b-7c019b812597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54bbcd4-81ee-42c6-83d3-428829fa00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for tokenizer to use with map() method of datasetdict\n",
    "def token_preproc(data):\n",
    "    return tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f538e0b6-4460-4ba0-94a9-00d3001aa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc90e5ee4a1b4a89802e08f92612456c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e46ca49092234712979544cc1286e6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc5e42cd4f94ca5b9e32c5953ec82ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb5ed59d07f24ff89776405808737171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6e9a823e184240b7ee6c12d9836ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f06247b59e347f3831ca6388b959fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "519b17e5297d4bb5a3a4d4f139df1a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249666a278474e3e81267ce8cc6f9d97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 6974\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 368\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokened_data = datasets.map(token_preproc, batched=True, num_proc=4, remove_columns=['text'])\n",
    "tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70c7c5c8-c106-4913-bebc-806d69deb88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 ['Whe', 'ret', 'o', 'Ġth', 'Ġinviting', 'Ġtime', 'Ġour', 'Ġfashion', 'Ġcalls', ':'] Whereto th inviting time our fashion calls:\n",
      "1580 ['Life', 'Ġto', 'Ġthy', 'Ġfame', ',', 'Ġthou', 'Ġright', 'Ġa', 'Ġph', 'oenix', 'Ġart', ','] Life to thy fame, thou right a phoenix art,\n",
      "3275 ['For', 'Ġmen', 'Ġdise', 'ased', ';', 'Ġbut', 'ĠI', ',', 'Ġmy', 'Ġmistress', 'Ġthr', 'all', ','] For men diseased; but I, my mistress thrall,\n",
      "4208 ['For', 'Ġtruth', 'Ġproves', 'Ġth', 'iev', 'ish', 'Ġfor', 'Ġa', 'Ġprize', 'Ġso', 'Ġdear', '.'] For truth proves thievish for a prize so dear.\n",
      "6596 ['And', 'Ġyet', 'ĠI', 'Ġcannot', 'Ġrep', 'rehend', 'Ġthe', 'Ġflight', ','] And yet I cannot reprehend the flight,\n",
      "4498 ['But', 'Ġwere', 'ĠI', 'Ġdead', ',', 'Ġshe', 'Ġwould', 'Ġnot', 'Ġbe', 'Ġbetrayed', ';'] But were I dead, she would not be betrayed;\n",
      "3730 ['I', 'Ġcannot', 'Ġsend', 'Ġfor', 'Ġaid', 'Ġunto', 'Ġthe', 'Ġtown', ','] I cannot send for aid unto the town,\n",
      "1147 ['That', 'Ġfortune', 'Ġfalse', 'Ġmy', 'Ġfaith', 'Ġshall', 'Ġnot', 'Ġremove', '.'] That fortune false my faith shall not remove.\n",
      "6602 ['One', 'Ġthing', 'Ġexpressing', ',', 'Ġleaves', 'Ġout', 'Ġdifference', '.'] One thing expressing, leaves out difference.\n",
      "3470 ['My', 'Ġconst', 'ancy', 'Ġthat', 'Ġyou', 'Ġfull', 'Ġwell', 'Ġhave', 'Ġproved', ','] My constancy that you full well have proved,\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    n = random.randint(0, len(tokened_data['train']))\n",
    "    print(n, tokenizer.convert_ids_to_tokens(tokened_data['train'][n]['input_ids']), lines_train[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a746d71-3b09-40b8-95ae-b5f3a71d44d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='gpt2-medium', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='tf')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad encodings and prep for modeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors='tf')\n",
    "collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10ee63-a50a-42da-a49f-59f015e2814e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f45202c2-393d-4e35-9ffd-fea2adf70329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at ../models/gpt2-medium.lines.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# instantiate model\n",
    "model_path = os.path.join(DIR_MODEL, f'{model_type}.{MODEL_FORMAT}')\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_type, pad_token_id = tokenizer.eos_token_id)\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec648afe-a7b2-464e-b805-ca0ffce0e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(16, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(16, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(16, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to special format for tf model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tf_train_set = model.prepare_tf_dataset(tokened_data['train'], shuffle=True, batch_size=16, collate_fn=collator)\n",
    "tf_test_set = model.prepare_tf_dataset(tokened_data['test'], shuffle=False, batch_size=16, collate_fn=collator)\n",
    "tf_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6abd4074-b0d6-4058-b017-67d196514c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6d0663a-d3bc-4c8d-b713-bb892ec5dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model (if pretrained does not exist)\n",
    "if not os.path.exists(model_path):\n",
    "    model.fit(tf_train_set, validation_data=tf_test_set, epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b5ee69e-b6b2-4f6d-822d-e42a0a75eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9205b01-beaf-4603-9836-01cfcefe172e",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7018e21-169f-4268-bf96-20b11eae6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get predicted text\n",
    "def test(text, max_length=50, temp=0.5, top_k=50, rep_penalty=1.5, len_penalty=0.75):\n",
    "    tokened = tokenizer(text, return_tensors='np')\n",
    "    output = model.generate(**tokened, \n",
    "                            max_length=max_length, \n",
    "                            temperature=temp, \n",
    "                            top_k=top_k, \n",
    "                            repetition_penalty=rep_penalty,\n",
    "                            length_penalty=len_penalty)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc98e32b-111c-4dc0-843c-252cab0de139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temp: 1.5\n",
      "Original: Tomorrow I will\n",
      "Output: Tomorrow I will not sleep, but must die.  And yet my love is so strong;--and still it is! O well then: thou art a saint!—O beauteous one!--I sing,—Ah me! how oft, and why —why? Ah, ah yes, what to say, when asked... Dear friend of mine, tell us more than ever you know, or can think (or see) ; for this we both need, dear boy, speak truth\n",
      "\n",
      "Temp: 1.5\n",
      "Original: Yesterday we were\n",
      "Output: Yesterday we were both young, and yet so old.  What a difference! I am thine; you are my love:--and then the story begins!--I was born in Fidessa's landscape Gardens Conservatory Gymnasium Building, where you live today (now) Sitting by me on grassy hillside Plain Backgrounded with green, I stand gazing, You look away from us Both looking at each other Looking back to one another, now gone Now past, never seen Before\n",
      "\n",
      "Temp: 1.5\n",
      "Original: For naught I may\n",
      "Output: For naught I may love thee, but that thou mightst know.  For this reason my heart is in strife; and yet I am not free:--and thus it lies. O! how much more doth she please me than you? What a fair thing do we see here—what art thou? And what are your faces!? Then let us hear her speak, for the truth of it. Speak then to me truly, if thou wilt. Dear friend!--if ever thy\n",
      "\n",
      "Temp: 1.5\n",
      "Original: My love has been\n",
      "Output: My love has been a fire, and my flame hath not died.  And now I know it is cold; yea colder than snow!  Then did I feel the chill of night: then was mine eye dry—now doth seem white.—Now do see that day brighteneth, or wane? Now are you fair in hue,—or ill-bred!?” “Ah! what joys have I found here!--what wonders wept thereabouts?!† †Ye\n",
      "\n",
      "Temp: 1.5\n",
      "Original: I am\n",
      "Output: I am not a man, but an angel.  And yet I love thee with such zeal;--and so do you: and thus we both!—so it is said,—but thou art the true one!--O Love me, my Belovëd!, say'st she (she) aloud? Dear Friend (!), what dost thy heart desire!? Then let us kiss each other, as lovers are wont to do.—Ah well then farewell! Let our parting be sweet! Be\n",
      "\n",
      "Temp: 1.5\n",
      "Original: Thou art\n",
      "Output: Thou art the true and only god, who dost know all.  For thou hast no name but that, which is his;--god!—and I am not: so be it known to thee alone!--O love me with a fair heart,—for ever! Amen? O Love's sweet radiance shineeth! Dear Friend! bless my soul in this hour of grief » Then sighs are shed on thy brow-beaten song ; tears flow upon thine eyes... then\n",
      "\n",
      "Temp: 1.5\n",
      "Original: The little love-god lying once asleep\n",
      "Output: The little love-god lying once asleep, doth wake;--and the sun shines bright.  The world is not so dark, nor yet as it used be: nay! fairer days are come!--more fair than now?—better shine.--Ah well then I will tell my tale,—tell me of thee and me...   \n",
      "Posted by _Buckley_ at 17 April 1990 on his way to work in vain...\n",
      "\n",
      "Temp: 1.5\n",
      "Original: In loving thee thou\n",
      "Output: In loving thee thou hast no right to complain.  For I am not a woman, nor born in the time;--nor can be so!—but you are. You have my heart and soul: mine alone is your name? Then tell me then what it means!--what makes us twain,—and why we love one another!? Let him hear his part now.--Ah yes indeed he hears well enough, though poor wretch that i' this case was made (see note)!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lines = [\n",
    "    'Tomorrow I will',\n",
    "    'Yesterday we were',\n",
    "    'For naught I may',\n",
    "    'My love has been',\n",
    "    'I am',\n",
    "    'Thou art',\n",
    "    'The little love-god lying once asleep',\n",
    "    'In loving thee thou'\n",
    "]\n",
    "\n",
    "\n",
    "for line in test_lines:\n",
    "    print(f'Original: {line}\\nOutput: {test(line, temp=temp, top_k=1000, max_length=100)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b278a-3993-4ec1-a3fb-6e2fc33e768b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
