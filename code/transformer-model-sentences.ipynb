{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e247aac4-af16-4b6e-be7c-8a32403263d8",
   "metadata": {},
   "source": [
    "# Modeling on Sentences\n",
    "\n",
    "Creating a model based on sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd2444a-4ed5-4266-b23f-6e4cafcfd25d",
   "metadata": {},
   "source": [
    "A lot of the below is adapted from the gpt2 tutorial at https://huggingface.co/docs/transformers/v4.22.2/en/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae9ebac-93be-488f-b5b8-3fc324c36fe7",
   "metadata": {},
   "source": [
    "## Imports and Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3c7b112-6339-422c-8b13-6beae6a1ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data formatting for model\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# lm collator\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# model and support\n",
    "from transformers import TFAutoModelForCausalLM, create_optimizer, AdamWeightDecay\n",
    "\n",
    "# other utilities\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40b0184-61c8-4b8c-9be3-bca0f00cac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model we are using\n",
    "MODELS = [\n",
    "    'gpt', # original GPT\n",
    "    'distilgpt2', # 84M features\n",
    "    'gpt2', # 117M features\n",
    "    'gpt2-medium', # 355M features\n",
    "    'gpt2-large', # 744M features\n",
    "    'ctrl',\n",
    "    'transformerxl',\n",
    "    'reformer',\n",
    "    'xlnet'\n",
    "]\n",
    "    \n",
    "model_type = 'gpt2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3af3b72-26ba-46b1-9e03-40a695527906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "MODEL_FORMAT = 'sentences-2'\n",
    "DIR_MODEL = '../models/'\n",
    "DIR_DATA = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9f8435d-28be-4c30-868d-0e5b7e1a3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regexes\n",
    "RE_SENTENCE = re.compile(r'\\w.*?[.?!]', re.S)\n",
    "RE_WHITESPACE = re.compile(r'\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6aaf62d9-a97e-4944-a5aa-e1c8eee3f612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# other special constants\n",
    "EOL_TOKEN = '<|eol|>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959970d-dfcd-4339-bdac-9877ac3f3607",
   "metadata": {},
   "source": [
    "## Load and Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a1c7d8c-8aa3-4227-ab44-40d079392282",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108475,\n",
       " 557,\n",
       " 'From fairest creatures we desire increase,<|eol|> That thereby beautys rose might never die,<|eol|> But as the riper should by time decease,<|eol|> His tender heir might bear his memory:<|eol|> But thou, contracted to thine own bright eyes,<|eol|> Feedst thy lights flame with self-substantial fuel,<|eol|> Making a famine where abundance lies,<|eol|> Thyself thy foe, to thy sweet self too cruel:<|eol|> Thou that art now the worlds fresh ornament,<|eol|> And only herald to the gaudy spring,<|eol|> Within thine own bud buriest thy content,<|eol|> And tender churl makst waste in niggarding:<|eol|> Pity the world, or else this glutton be,<|eol|> To eat the worlds due, by the grave and thee.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "paths = [\n",
    "    os.path.join(DIR_DATA, 'shakespeare-sonnets.clean.txt'),\n",
    "    #os.path.join(DIR_DATA, 'browning-sonnets.clean.txt'),\n",
    "    #os.path.join(DIR_DATA, 'daniel-constable-sonnets.clean.txt'),\n",
    "    #os.path.join(DIR_DATA, 'drayton-griffin-smith-sonnet-cycles.clean.txt'),\n",
    "    #os.path.join(DIR_DATA, 'farjeon-sonnets.clean.txt'),\n",
    "    #os.path.join(DIR_DATA, 'lovell-southey-sonnets.clean.txt')\n",
    "]\n",
    "\n",
    "text = list()\n",
    "\n",
    "for path in paths:\n",
    "    with open(path, 'r') as f:\n",
    "        text.append([line.strip() + EOL_TOKEN for line in f.readlines() if line.strip()])\n",
    "\n",
    "text = ' '.join(chain(*text))\n",
    "lines = RE_SENTENCE.findall(text)\n",
    "lines = [RE_WHITESPACE.sub(' ', line) for line in lines]\n",
    "len(text), len(lines), lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c01b72ce-07e3-4575-8b73-750b339c6e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 28)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split train and test\n",
    "lines_train, lines_test = train_test_split(lines, test_size=0.05)\n",
    "len(lines_train), len(lines_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3437b48-0310-4f4e-ba9f-fc222c3f84e4",
   "metadata": {},
   "source": [
    "## Cleaning and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69bc6d46-e09d-4927-ae28-d28e8bdaa767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 529\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create Dataset and DatasetDict instances - I think this is needed for model\n",
    "train_dataset = Dataset.from_dict({'text': lines_train})\n",
    "test_dataset = Dataset.from_dict({'text': lines_test})\n",
    "datasets = DatasetDict({'train': train_dataset, 'test': test_dataset})\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3853c6fe-9d1c-43fb-b91b-7c019b812597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type, additional_special_tokens=[EOL_TOKEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c54bbcd4-81ee-42c6-83d3-428829fa00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for tokenizer to use with map() method of datasetdict\n",
    "def token_preproc(data):\n",
    "    return tokenizer(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f538e0b6-4460-4ba0-94a9-00d3001aa69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a42aa80fe0649c3abafac90294a0e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95db0583b53347eea9afd31c7ae52f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cfbc20cc675451b9f9872f00b7a68f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64323d4f54bf47da9f501ae9f14b7b89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6769129918426faec7428b0bbb6019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5fcf12c4cd49cf91be1012cfe83821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11d54c870f9148da854d8cd3627c474e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2d61e9568e547468c70ff623c73605d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 529\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 28\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize data\n",
    "tokened_data = datasets.map(token_preproc, batched=True, num_proc=4, remove_columns=['text'])\n",
    "tokened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70c7c5c8-c106-4913-bebc-806d69deb88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 ['that', 'Ġour', 'Ġnight', 'Ġof', 'Ġw', 'oe', 'Ġmight', 'Ġhave', 'Ġremembered', '<|eol|>', 'ĠMy', 'Ġdeepest', 'Ġsense', ',', 'Ġhow', 'Ġhard', 'Ġtrue', 'Ġsorrow', 'Ġhits', ',', '<|eol|>', 'ĠAnd', 'Ġsoon', 'Ġto', 'Ġyou', ',', 'Ġas', 'Ġyou', 'Ġto', 'Ġme', ',', 'Ġthen', 'Ġtend', 'ered', '<|eol|>', 'ĠThe', 'Ġhumble', 'Ġsal', 've', ',', 'Ġwhich', 'Ġwounded', 'Ġbos', 'oms', 'Ġfits', '!'] that our night of woe might have remembered<|eol|> My deepest sense, how hard true sorrow hits,<|eol|> And soon to you, as you to me, then tendered<|eol|> The humble salve, which wounded bosoms fits!\n",
      "61 ['e', 'ol', '|', '>', 'ĠWhen', 'Ġin', 'Ġthe', 'Ġchron', 'icle', 'Ġof', 'Ġwasted', 'Ġtime', '<|eol|>', 'ĠI', 'Ġsee', 'Ġdescriptions', 'Ġof', 'Ġthe', 'Ġfaire', 'st', 'Ġw', 'ights', ',', '<|eol|>', 'ĠAnd', 'Ġbeauty', 'Ġmaking', 'Ġbeautiful', 'Ġold', 'Ġr', 'ime', ',', '<|eol|>', 'ĠIn', 'Ġpraise', 'Ġof', 'Ġladies', 'Ġdead', 'Ġand', 'Ġlovely', 'Ġknights', ',', '<|eol|>', 'ĠThen', ',', 'Ġin', 'Ġthe', 'Ġbl', 'azon', 'Ġof', 'Ġsweet', 'Ġbeaut', 'ys', 'Ġbest', ',', '<|eol|>', 'ĠOf', 'Ġhand', ',', 'Ġof', 'Ġfoot', ',', 'Ġof', 'Ġlip', ',', 'Ġof', 'Ġeye', ',', 'Ġof', 'Ġbrow', ',', '<|eol|>', 'ĠI', 'Ġsee', 'Ġtheir', 'Ġantique', 'Ġpen', 'Ġwould', 'Ġhave', 'Ġexpressed', '<|eol|>', 'ĠEven', 'Ġsuch', 'Ġa', 'Ġbeauty', 'Ġas', 'Ġyou', 'Ġmaster', 'Ġnow', '.'] eol|> When in the chronicle of wasted time<|eol|> I see descriptions of the fairest wights,<|eol|> And beauty making beautiful old rime,<|eol|> In praise of ladies dead and lovely knights,<|eol|> Then, in the blazon of sweet beautys best,<|eol|> Of hand, of foot, of lip, of eye, of brow,<|eol|> I see their antique pen would have expressed<|eol|> Even such a beauty as you master now.\n",
      "490 ['e', 'ol', '|', '>', 'ĠIf', 'Ġmy', 'Ġdear', 'Ġlove', 'Ġwere', 'Ġbut', 'Ġthe', 'Ġchild', 'Ġof', 'Ġstate', ',', '<|eol|>', 'ĠIt', 'Ġmight', 'Ġfor', 'ĠF', 'ortun', 'es', 'Ġbastard', 'Ġbe', 'Ġunf', 'athered', ',', '<|eol|>', 'ĠAs', 'Ġsubject', 'Ġto', 'ĠTimes', 'Ġlove', 'Ġor', 'Ġto', 'ĠTimes', 'Ġhate', ',', '<|eol|>', 'ĠWe', 'eds', 'Ġamong', 'Ġweeds', ',', 'Ġor', 'Ġflowers', 'Ġwith', 'Ġflowers', 'Ġgathered', '.'] eol|> If my dear love were but the child of state,<|eol|> It might for Fortunes bastard be unfathered,<|eol|> As subject to Times love or to Times hate,<|eol|> Weeds among weeds, or flowers with flowers gathered.\n",
      "214 ['e', 'ol', '|', '>', 'ĠSo', 'Ġis', 'Ġthe', 'Ġtime', 'Ġthat', 'Ġkeeps', 'Ġyou', 'Ġas', 'Ġmy', 'Ġchest', ',', '<|eol|>', 'ĠOr', 'Ġas', 'Ġthe', 'Ġwardrobe', 'Ġwhich', 'Ġthe', 'Ġrobe', 'Ġd', 'oth', 'Ġhide', ',', '<|eol|>', 'ĠTo', 'Ġmake', 'Ġsome', 'Ġspecial', 'Ġinstant', 'Ġspecial', '-', 'bl', 'est', ',', '<|eol|>', 'ĠBy', 'Ġnew', 'Ġunfolding', 'Ġhis', 'Ġimp', 'ris', 'ond', 'Ġpride', '.'] eol|> So is the time that keeps you as my chest,<|eol|> Or as the wardrobe which the robe doth hide,<|eol|> To make some special instant special-blest,<|eol|> By new unfolding his imprisond pride.\n",
      "502 ['call', 'Ġnot', 'Ġme', 'Ġto', 'Ġjustify', 'Ġthe', 'Ġwrong', '<|eol|>', 'ĠThat', 'Ġthy', 'Ġun', 'kind', 'ness', 'Ġlays', 'Ġupon', 'Ġmy', 'Ġheart', ';', '<|eol|>', 'ĠW', 'ound', 'Ġme', 'Ġnot', 'Ġwith', 'Ġth', 'ine', 'Ġeye', ',', 'Ġbut', 'Ġwith', 'Ġthy', 'Ġtongue', ':', '<|eol|>', 'ĠUse', 'Ġpower', 'Ġwith', 'Ġpower', ',', 'Ġand', 'Ġslay', 'Ġme', 'Ġnot', 'Ġby', 'Ġart', ',', '<|eol|>', 'ĠTell', 'Ġme', 'Ġthou', 'Ġlov', 'est', 'Ġelsewhere', ';', 'Ġbut', 'Ġin', 'Ġmy', 'Ġsight', ',', '<|eol|>', 'ĠDear', 'Ġheart', ',', 'Ġforb', 'ear', 'Ġto', 'Ġglance', 'Ġth', 'ine', 'Ġeye', 'Ġaside', ':', '<|eol|>', 'ĠWhat', 'Ġneed', 'st', 'Ġthou', 'Ġwound', 'Ġwith', 'Ġcunning', ',', 'Ġwhen', 'Ġthy', 'Ġmight', '<|eol|>', 'ĠIs', 'Ġmore', 'Ġthan', 'Ġmy', 'Ġover', 'pressed', 'Ġdefence', 'Ġcan', 'Ġb', 'ide', '?'] call not me to justify the wrong<|eol|> That thy unkindness lays upon my heart;<|eol|> Wound me not with thine eye, but with thy tongue:<|eol|> Use power with power, and slay me not by art,<|eol|> Tell me thou lovest elsewhere; but in my sight,<|eol|> Dear heart, forbear to glance thine eye aside:<|eol|> What needst thou wound with cunning, when thy might<|eol|> Is more than my overpressed defence can bide?\n",
      "215 ['e', 'ol', '|', '>', 'ĠWhere', 'Ġart', 'Ġthou', 'ĠMuse', 'Ġthat', 'Ġthou', 'Ġforget', 'st', 'Ġso', 'Ġlong', ',', '<|eol|>', 'ĠTo', 'Ġspeak', 'Ġof', 'Ġthat', 'Ġwhich', 'Ġgives', 'Ġthee', 'Ġall', 'Ġthy', 'Ġmight', '?'] eol|> Where art thou Muse that thou forgetst so long,<|eol|> To speak of that which gives thee all thy might?\n",
      "522 ['e', 'ol', '|', '>', 'ĠO', '!'] eol|> O!\n",
      "54 ['e', 'ol', '|', '>', 'ĠO', '!'] eol|> O!\n",
      "272 ['e', 'ol', '|', '>', 'ĠWhen', 'ĠI', 'Ġdo', 'Ġcount', 'Ġthe', 'Ġclock', 'Ġthat', 'Ġtells', 'Ġthe', 'Ġtime', ',', '<|eol|>', 'ĠAnd', 'Ġsee', 'Ġthe', 'Ġbrave', 'Ġday', 'Ġsunk', 'Ġin', 'Ġhideous', 'Ġnight', ';', '<|eol|>', 'ĠWhen', 'ĠI', 'Ġbehold', 'Ġthe', 'Ġviolet', 'Ġpast', 'Ġprime', ',', '<|eol|>', 'ĠAnd', 'Ġs', 'able', 'Ġcurls', ',', 'Ġall', 'Ġsil', 'vered', 'Ġover', 'Ġwith', 'Ġwhite', ';', '<|eol|>', 'ĠWhen', 'Ġlofty', 'Ġtrees', 'ĠI', 'Ġsee', 'Ġbarren', 'Ġof', 'Ġleaves', ',', '<|eol|>', 'ĠWhich', 'Ġer', 'st', 'Ġfrom', 'Ġheat', 'Ġdid', 'Ġcanopy', 'Ġthe', 'Ġherd', ',', '<|eol|>', 'ĠAnd', 'Ġsummers', 'Ġgreen', 'Ġall', 'Ġg', 'ird', 'ed', 'Ġup', 'Ġin', 'Ġshe', 'aves', ',', '<|eol|>', 'ĠB', 'orne', 'Ġon', 'Ġthe', 'Ġb', 'ier', 'Ġwith', 'Ġwhite', 'Ġand', 'Ġbrist', 'ly', 'Ġbeard', ',', '<|eol|>', 'ĠThen', 'Ġof', 'Ġthy', 'Ġbeauty', 'Ġdo', 'ĠI', 'Ġquestion', 'Ġmake', ',', '<|eol|>', 'ĠThat', 'Ġthou', 'Ġamong', 'Ġthe', 'Ġwastes', 'Ġof', 'Ġtime', 'Ġmust', 'Ġgo', ',', '<|eol|>', 'ĠSince', 'Ġsweets', 'Ġand', 'Ġbeaut', 'ies', 'Ġdo', 'Ġthemselves', 'Ġfors', 'ake', '<|eol|>', 'ĠAnd', 'Ġdie', 'Ġas', 'Ġfast', 'Ġas', 'Ġthey', 'Ġsee', 'Ġothers', 'Ġgrow', ';', '<|eol|>', 'ĠAnd', 'Ġnothing', 'Ġgain', 'st', 'ĠTimes', 'Ġsc', 'y', 'the', 'Ġcan', 'Ġmake', 'Ġdefence', '<|eol|>', 'ĠSave', 'Ġbreed', ',', 'Ġto', 'Ġbrave', 'Ġhim', 'Ġwhen', 'Ġhe', 'Ġtakes', 'Ġthee', 'Ġhence', '.'] eol|> When I do count the clock that tells the time,<|eol|> And see the brave day sunk in hideous night;<|eol|> When I behold the violet past prime,<|eol|> And sable curls, all silvered over with white;<|eol|> When lofty trees I see barren of leaves,<|eol|> Which erst from heat did canopy the herd,<|eol|> And summers green all girded up in sheaves,<|eol|> Borne on the bier with white and bristly beard,<|eol|> Then of thy beauty do I question make,<|eol|> That thou among the wastes of time must go,<|eol|> Since sweets and beauties do themselves forsake<|eol|> And die as fast as they see others grow;<|eol|> And nothing gainst Times scythe can make defence<|eol|> Save breed, to brave him when he takes thee hence.\n",
      "31 ['e', 'ol', '|', '>', 'ĠThou', 'Ġart', 'Ġmore', 'Ġlovely', 'Ġand', 'Ġmore', 'Ġtemper', 'ate', ':', '<|eol|>', 'ĠRough', 'Ġwinds', 'Ġdo', 'Ġshake', 'Ġthe', 'Ġdarling', 'Ġbuds', 'Ġof', 'ĠMay', ',', '<|eol|>', 'ĠAnd', 'Ġsummers', 'Ġlease', 'Ġhath', 'Ġall', 'Ġtoo', 'Ġshort', 'Ġa', 'Ġdate', ':', '<|eol|>', 'ĠS', 'ometime', 'Ġtoo', 'Ġhot', 'Ġthe', 'Ġeye', 'Ġof', 'Ġheaven', 'Ġshines', ',', '<|eol|>', 'ĠAnd', 'Ġoften', 'Ġis', 'Ġhis', 'Ġgold', 'Ġcomplexion', 'Ġdim', 'med', ',', '<|eol|>', 'ĠAnd', 'Ġevery', 'Ġfair', 'Ġfrom', 'Ġfair', 'Ġsometime', 'Ġdeclines', ',', '<|eol|>', 'ĠBy', 'Ġchance', ',', 'Ġor', 'Ġn', 'atures', 'Ġchanging', 'Ġcourse', 'Ġunt', 'rim', 'med', ':', '<|eol|>', 'ĠBut', 'Ġthy', 'Ġeternal', 'Ġsummer', 'Ġshall', 'Ġnot', 'Ġfade', ',', '<|eol|>', 'ĠNor', 'Ġlose', 'Ġpossession', 'Ġof', 'Ġthat', 'Ġfair', 'Ġthou', 'Ġo', 'west', ',', '<|eol|>', 'ĠNor', 'Ġshall', 'Ġdeath', 'Ġbr', 'ag', 'Ġthou', 'Ġwand', 'erest', 'Ġin', 'Ġhis', 'Ġshade', ',', '<|eol|>', 'ĠWhen', 'Ġin', 'Ġeternal', 'Ġlines', 'Ġto', 'Ġtime', 'Ġthou', 'Ġgrow', 'est', ',', '<|eol|>', 'ĠSo', 'Ġlong', 'Ġas', 'Ġmen', 'Ġcan', 'Ġbreathe', ',', 'Ġor', 'Ġeyes', 'Ġcan', 'Ġsee', ',', '<|eol|>', 'ĠSo', 'Ġlong', 'Ġlives', 'Ġthis', ',', 'Ġand', 'Ġthis', 'Ġgives', 'Ġlife', 'Ġto', 'Ġthee', '.'] eol|> Thou art more lovely and more temperate:<|eol|> Rough winds do shake the darling buds of May,<|eol|> And summers lease hath all too short a date:<|eol|> Sometime too hot the eye of heaven shines,<|eol|> And often is his gold complexion dimmed,<|eol|> And every fair from fair sometime declines,<|eol|> By chance, or natures changing course untrimmed:<|eol|> But thy eternal summer shall not fade,<|eol|> Nor lose possession of that fair thou owest,<|eol|> Nor shall death brag thou wanderest in his shade,<|eol|> When in eternal lines to time thou growest,<|eol|> So long as men can breathe, or eyes can see,<|eol|> So long lives this, and this gives life to thee.\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    n = random.randint(0, len(tokened_data['train']))\n",
    "    print(n, tokenizer.convert_ids_to_tokens(tokened_data['train'][n]['input_ids']), lines_train[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a746d71-3b09-40b8-95ae-b5f3a71d44d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'additional_special_tokens': ['<|eol|>']}), mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='tf')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad encodings and prep for modeling\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors='tf')\n",
    "collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10ee63-a50a-42da-a49f-59f015e2814e",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f45202c2-393d-4e35-9ffd-fea2adf70329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<transformers.modeling_tf_utils.TFSharedEmbeddings at 0x172613e20>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model\n",
    "model_path = os.path.join(DIR_MODEL, f'{model_type}.{MODEL_FORMAT}')\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_type, pad_token_id = tokenizer.eos_token_id)\n",
    "else:\n",
    "    model = TFAutoModelForCausalLM.from_pretrained(model_path)\n",
    "    \n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec648afe-a7b2-464e-b805-ca0ffce0e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=({'input_ids': TensorSpec(shape=(16, None), dtype=tf.int64, name=None), 'attention_mask': TensorSpec(shape=(16, None), dtype=tf.int64, name=None)}, TensorSpec(shape=(16, None), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert data to special format for tf model\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tf_train_set = model.prepare_tf_dataset(tokened_data['train'], shuffle=True, batch_size=16, collate_fn=collator)\n",
    "tf_test_set = model.prepare_tf_dataset(tokened_data['test'], shuffle=False, batch_size=16, collate_fn=collator)\n",
    "tf_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6abd4074-b0d6-4058-b017-67d196514c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d0663a-d3bc-4c8d-b713-bb892ec5dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "31/33 [===========================>..] - ETA: 11s - loss: 21.0287"
     ]
    }
   ],
   "source": [
    "# fit model (if pretrained does not exist)\n",
    "if not os.path.exists(model_path):\n",
    "    model.fit(tf_train_set, validation_data=tf_test_set, epochs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2b5ee69e-b6b2-4f6d-822d-e42a0a75eb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "    model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9205b01-beaf-4603-9836-01cfcefe172e",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a7018e21-169f-4268-bf96-20b11eae6982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get predicted text\n",
    "def test(text, max_new=50, temp=1, top_k=50, rep_penalty=1.5, len_penalty=0.75, n_seq=1):\n",
    "    tokened = tokenizer(text, return_tensors='tf')\n",
    "    output = model.generate(**tokened,\n",
    "                            do_sample=True,\n",
    "                            max_new_tokens=max_new, \n",
    "                            temperature=temp, \n",
    "                            top_k=top_k, \n",
    "                            repetition_penalty=rep_penalty,\n",
    "                            length_penalty=len_penalty,\n",
    "                            num_return_sequences=n_seq)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "cc98e32b-111c-4dc0-843c-252cab0de139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[49488   314   481   407   307  1498 48920  1675  1064   616  1842   287\n",
      "    262   995    11  4249  1683   284  2107    13  5896   318   329   502\n",
      "    257  1517   286   645   779    26  1865   340   373   288   849   787\n",
      "    523    25   960 23205  3607 17903   428  1204   290   326   543 14210\n",
      "    266  2326  4425     0  4718 12311 11906   944   788   611   345   743\n",
      "    475  3520   612   890  1576 34976   392   766   703  1290   534 18522\n",
      "     82  2121   783  1165  2739    30   440  1309   514   467   319   588\n",
      "   1450   355   356   423  3750   878 13402  1532   777  6066   466  3387\n",
      "    294   500  4151   393  2612   484  1276]], shape=(1, 103), dtype=int32)\n",
      "Original: Tomorrow I will\n",
      "Output: Tomorrow I will not be able LINE To find my love in the world, nor ever to live. Love is for me a thing of no use; yet it was doth make so:—love gives thee this life and that which thou wilt lose! Relieve thyself then if you may but remain there long enough,—and see how far your griefs fall now too late? O let us go on like men as we have gone before.—If these thoughts do please thine eye or heart they must\n",
      "\n",
      "tf.Tensor(\n",
      "[[49488    11   616  1842     0 48920  1320   543   314   466   892   318\n",
      "    262   749  3991    26   438  6981  1865   340   288   849   307  2081\n",
      "     13  1114   477  1243   326   389  1760   287   502   428   835    25\n",
      "    523  5879 14210  1242   826   290   922   284  4150   356   815 42926\n",
      "     30   440   788   416 11906 11542  3436 36258   777  6066 15058 34976\n",
      "     83   359   484  2121   656  6164  2951  1165  2739   960   754   332\n",
      "   2236  1239   760 17903  1365   621   618   345   547   717  4642 13402\n",
      "     40   716   407   257   582   286  1807   475   783   804   379  3589\n",
      "    355   530   351  1854   764]], shape=(1, 101), dtype=int32)\n",
      "Original: Tomorrow\n",
      "Output: Tomorrow, my love! LINE That which I do think is the most false;--AND yet it doth be true. For all things that are done in me this way: so prove thou art right and good to whom we should aspire? O then by thy grace alone shalt these thoughts arise,—till they fall into mine eyes too late—forever shall never know thee better than when you were first born.—I am not a man of thought but now look at myself as one with others.\n",
      "\n",
      "tf.Tensor(\n",
      "[[49488   314   481  1842 17903    11   290  1865 14210  1242   616 27666\n",
      "     13 48920   632   318   355   611   262  5417   550   645   886   475\n",
      "    284 14595    26   523   340 22027  1760   287   428  1339    25   960\n",
      "     39   594   339   326   288   849  6227   502   407  2236  1064   503\n",
      "    465  2081  3450     0  1114   644  2911   460   301 34048   423   286\n",
      "    683   508 10408    30  1471  2073   561 11906 25622  2951   307 30859\n",
      "    351 10953   618   484   766   294   500  4151 34976    36   803  4145\n",
      "    319   257 22999    12   929  8837   438    40   716   497   263  2695\n",
      "    276   379   477  2845   329  3252 15370]], shape=(1, 103), dtype=int32)\n",
      "Original: Tomorrow I will\n",
      "Output: Tomorrow I will love thee, and yet thou art my doom. LINE It is as if the sea had no end but to sink; so it hath done in this case:—Hence he that doth desire me not shall find out his true nature! For what hope canst Thou have of him who loves? Or else would thy lifes eyes be drowned with tears when they see thine eye,—Eating thus on a burnt-up vessel--I am neer contented at all except for fear thereof\n",
      "\n",
      "tf.Tensor(\n",
      "[[28065   356   547  1111  3595    26 48920   843  1865    11   991   355\n",
      "    996   339   550   407   587   465  1545     0  1406   783   314   716\n",
      "   5527   290   423   645   517   284  1577    13 35205  4053    82   466\n",
      "    502   922   287   326   810  1659   484  1650    25   329   788   616\n",
      "   2612   318  2695   276   351 17903   960   198   464  1198  1363   757\n",
      "  10597  2739   262  4252 22027   900 34976 22850   815   301 14210 32814\n",
      "    523    30  6910  1114  6164  4151   288   849   766   257  3148 15061\n",
      "   3957   612   319   340   357  2016   673  1239  3568     8 11906   944\n",
      "    852  3750 13402    44   419   501   477]], shape=(1, 103), dtype=int32)\n",
      "Original: Yesterday we were\n",
      "Output: Yesterday we were both poor; LINE And yet, still as though he had not been his friend! So now I am rich and have no more to give. Farewells do me good in that whereof they sit: for then my heart is contented with thee—\n",
      "Thence home again till late the sun hath set,—why shouldst thou shun so? Line For mine eye doth see a fair flower growing there on it (though she never appears) thyself being gone.—Mortice all\n",
      "\n",
      "tf.Tensor(\n",
      "[[ 1890   299  3413   314   743   475 26246 48920   383  3595    11   290\n",
      "    262 14720    26   329   616  1842 14210  1242   749 43210    13  5896\n",
      "    318   407   284   307 44990     0  4091   644  8737   460  1577 17903\n",
      "    523   881 10974    25   326 11906   944   287   502  1244   301   766\n",
      "    340  1365   621  1854   466 13402    44   684   494   333   390   609\n",
      "   4685   271   960    32  5948   788 34976  5460   703   345  3566   893\n",
      "   2951   389   925   416   534  6504    12 18351  2337 14250  5145  1374\n",
      "   4361   674 11954   900   511 36605   319  2046   286   257  7319   812\n",
      "     30  9365   395   428   649  4562   288   849   787]], shape=(1, 105), dtype=int32)\n",
      "Original: For naught I may\n",
      "Output: For naught I may but pity LINE The poor, and the hungry; for my love thou art most gracious. Love is not to be despised! See what beauty can give thee so much delight: that thyself in me mightst see it better than others do.—Monsieur de Chloris—Aye then,—look how you beautys eyes are made by your sight-wrights invention! How therefore our hearts set their vows on fire of a thousand years? Knowest this new faith doth make\n",
      "\n",
      "tf.Tensor(\n",
      "[[ 3666  1842   468   587   523   890  1201   616   938  8033    11 48920\n",
      "   1320   287   340   314  2314  3551    26   290  1865   379  4129   466\n",
      "  44567    13   843   351   326 19680    82   284  3993   319   502  2236\n",
      "   1057     0   960    40 44434 17903   329  1683  3549    25   475 14210\n",
      "    288   455   407  1282  1474  1576 10597   783 34976 22850   815   301\n",
      "  34048 46931   539 11906   944    30  1406   881   262  4785  1276   428\n",
      "  23146   307   618  1204   318   625    12    86  2909 13402  1532   772\n",
      "    530  1517   460   423  3421   884   257  1181   355  6164 22027  1760\n",
      "    416  1755   393  1110   357  4758   991   389]], shape=(1, 104), dtype=int32)\n",
      "Original: My love has been\n",
      "Output: My love has been so long since my last breath, LINE That in it I cannot write; and yet at length do moan. And with that sighs to sleep on me shall run!—I crave thee for evermore: but thou dost not come near enough till now,—why shouldst Thou forsake thyself? So much the worse must this lament be when life is over-wrought.—If even one thing can have changed such a state as mine hath done by night or day (which still are\n",
      "\n",
      "tf.Tensor(\n",
      "[[   40   716    11   290   314   481   307    13 48920   887   788   616\n",
      "   1842  2236 42531   287   502    26   329   996   339  4656   717   286\n",
      "    477   465  2861   284  2107    11 48920  2011 18522    82   389   523\n",
      "   1049   326   484  1276 15000   511  5252    25  1865 14210  1242  6164\n",
      "      0   440 13674  1545   960 22850   288   455 11906 26246   407  3105\n",
      "  17903    30  5896   318   991   351   514   379  4129 34976  4360   355\n",
      "   2582   423   356  9258   764   317  5948   517  7634   673 22027   607\n",
      "   2612  1028   294   500  4151   621  1683   878 13402    39  4948  5907\n",
      "    852 23568   416   262 42644    12]], shape=(1, 102), dtype=int32)\n",
      "Original: I am\n",
      "Output: I am, and I will be. LINE But then my love shall perish in me; for though he die first of all his worth to live, LINE My griefs are so great that they must consume their fuel: yet thou art mine! O dear friend—why dost thy pity not slow thee? Love is still with us at length,—but as soon have we begun. Aye more strongly she hath her heart against thine eye than ever before.—Hymns being sung by the choir-\n",
      "\n",
      "tf.Tensor(\n",
      "[[  817   280  1242   262   691   530   284  1577    11 48920   843   314\n",
      "    716 11906 49519    13 34048 19338   645   584  3572    26   996 14210\n",
      "    266  2326   307  6164    25   329   326   543   318  1266   481   407\n",
      "  34302 17903     0   960  1114   618   616  2612  2236  5587   502   351\n",
      "    257  6611   340   743  2270 34976   392   788  1842   288   849   299\n",
      "   2917   475  8138   611   339   815  1064  8046 13315    36   683    30\n",
      "    887   428   465 18522    82   466  2728 12722   514   477  2356   290\n",
      "  33378 13402  1406 28639 22027   673   587   607 25920    67  2933    12\n",
      "  15219  1760  1201   717   356  2497  1123]], shape=(1, 103), dtype=int32)\n",
      "Original: Thou art\n",
      "Output: Thou art the only one to give, LINE And I am thy debtor. Thou hast no other choice; though thou wilt be mine: for that which is best will not suffice thee!— For when my heart shall strike me with a blow it may break,—and then love doth nought but blame if he should find fault WITHE him? But this his griefs do cause unto us all pain and torment.— So oft hath she been her maidd boy-girl done since first we saw each\n",
      "\n",
      "tf.Tensor(\n",
      "[[  464  1310  1842    12 25344  9105  1752 16039    11 48920 23420 13674\n",
      "   1545   286   616  5848     0  5896   290  1918   389   734  1243    26\n",
      "    475   287 17903   356  1111  4656    13 34048   266  2326   466   517\n",
      "    621   428    25 14210 36258   307   262   976   960  5562   318   284\n",
      "    910   314   716   257  2877   530   357  2016 11906  1204  1239   750\n",
      "    886     8   764   843  1865   326 28989  2236   407  2666   502  3436\n",
      "  34976  1640   788   294   500  2951  1276   766   757   644   373  1760\n",
      "    878   606   523   881  3421  2162   329   484   481 23700   703  6164\n",
      "   4151   288   849  1487   351   340   477  1755   890  1058  1309   514]], shape=(1, 108), dtype=int32)\n",
      "Original: The little love-god lying once asleep\n",
      "Output: The little love-god lying once asleep, LINE Dear dear friend of my soul! Love and death are two things; but in thee we both die. Thou wilt do more than this: thou shalt be the same—that is to say I am a living one (though thy life never did end). And yet that eternity shall not leave me alone,—for then thine eyes must see again what was done before them so much changed ; for they will behold how mine eye doth change with it all night long : let us\n",
      "\n",
      "tf.Tensor(\n",
      "[[  818 14442 17903 14210 19338   925   616  1204 13626    11 48920   843\n",
      "    314   423   407  1813   284  1854   262  3872    13  5896   318  1842\n",
      "    290   645   584   621   340    25   438  1114   644  2116    12 31642\n",
      "  22027   326   826    30  1400   517   460   301 11906  8737   307  1900\n",
      "     26   329  6164   898 11060  1276   477  1243 42531     0 40802   788\n",
      "    286   502   611   345   481  2453   428 18527   960   440  8716   516\n",
      "  27636 23700   395 34048  1242   523 13674   355   880 34976   198    62\n",
      "  33795  4808   464   402  8344   666 32887   287   663   555 11031  1108\n",
      "    905  2788   703   881  1365   356   389   416]], shape=(1, 104), dtype=int32)\n",
      "Original: In loving thee thou\n",
      "Output: In loving thee thou hast made my life sacred, LINE And I have not given to others the truth. Love is love and no other than it:-- For what self-same hath that right? No more canst thy beauty be known; for mine own sake must all things perish! Speak then of me if you will accept this verse— O joyous heavens beholdest Thou art so dear as well,—\n",
      "_Footnote _The Grecian Muse in its unkindness showeth how much better we are by\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lines = [\n",
    "    'Tomorrow I will',\n",
    "    'Tomorrow',\n",
    "    'Tomorrow I will',\n",
    "    'Yesterday we were',\n",
    "    'For naught I may',\n",
    "    'My love has been',\n",
    "    'I am',\n",
    "    'Thou art',\n",
    "    'The little love-god lying once asleep',\n",
    "    'In loving thee thou'\n",
    "]\n",
    "\n",
    "for line in test_lines:\n",
    "    output = test(line,\n",
    "                  temp=0.5,\n",
    "                  max_new=100,\n",
    "                  top_k=200,\n",
    "                  rep_penalty=1.5,\n",
    "                  len_penalty=0.75,\n",
    "                  n_seq=1)\n",
    "    print(f'Original: {line}\\nOutput: {output}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63b278a-3993-4ec1-a3fb-6e2fc33e768b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
